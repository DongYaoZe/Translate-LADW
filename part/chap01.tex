
%%%%%%%%%%%%%%%%%%%% 引言 %%%%%%%%%%%%%%%%%%%%%

\chapter{基本概念}\label{chap:Intro}

\section{向量空间}\label{sec:background}


向量空间 $V$ 是由称为向量（本书中用小写粗体字母表示，如 $\mathbf{v}$）的对象组成，以及两个运算：向量加法和数（标量）乘法
\footnote{为了在向量和其他对象之间做出视觉区分，本书使用\textbf{加粗的小写字母}来表示向量，而使用\textbf{普通小写字母}来表示数字（标量）。在一些（更高级的）书中，拉丁字母保留给向量，而希腊字母用于标量；在更高级的文本中，任何字母都可以用于任何目的，读者必须根据上下文理解每个符号的含义。我认为，尤其对于初学者来说，在不同对象之间有一定的视觉区分是有帮助的，所以加粗的小写字母将始终表示一个向量。而在黑板上，通常会使用箭头（如 $\vec{v}$）来标识一个向量。
}
，使得以下 8 个性质（所谓的向量空间\textbf{公理}）成立：

前 4 个性质涉及加法：
\footnote{
这时会引出一个问题：“我们该如何记住上述性质呢？” 而答案是，根本不需要记住，请看下文！
}

1. 交换律：$\mathbf{v} + \mathbf{w} = \mathbf{w} + \mathbf{v}$ 对所有 $\mathbf{v}, \mathbf{w} \in V$；

2. 结合律：$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ 对所有 $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$；

3. 零向量：存在一个特殊的向量，记作 $\mathbf{0}$，使得 $\mathbf{v} + \mathbf{0} = \mathbf{v}$ 对所有 $\mathbf{v} \in V$；

4. 加法逆元：对于每个向量 $\mathbf{v} \in V$ 都存在一个向量 $\mathbf{w} \in V$ 使得 $\mathbf{v} + \mathbf{w} = \mathbf{0}$。 这样的加法逆元通常记作 $-\mathbf{v}$；

接下来的两个性质涉及乘法：

5. 乘法单位元：$1 \mathbf{v} = \mathbf{v}$ 对所有 $\mathbf{v} \in V$；

6. 乘法结合律：$(\alpha\beta) \mathbf{v} = \alpha(\beta \mathbf{v})$ 对所有 $\mathbf{v} \in V$ 和所有标量 $\alpha, \beta$；

最后是两个分配律，它们连接了乘法和加法：

7. $\alpha (\mathbf{u} + \mathbf{v}) = \alpha \mathbf{u} + \alpha \mathbf{v}$ 对所有 $\mathbf{u}, \mathbf{v} \in V$ 和所有标量 $\alpha$；

8. $(\alpha + \beta) \mathbf{v} = \alpha \mathbf{v} + \beta \mathbf{v}$ 对所有 $\mathbf{v} \in V$ 和所有标量 $\alpha, \beta$。

 \textbf{注释}~上述性质似乎很难记忆，但没有必要。它们只是我们从高中学到的关于数字的代数运算的熟悉规则。这里唯一的陌生之处在于，你必须理解你可以在什么对象上应用什么运算。你可以相加向量，也可以用数字（标量）乘以向量。当然，你可以对数字进行所有你以前学过的运算。但是，你不能将两个向量相乘，也不能将一个数字加到一个向量上。

\textbf{注释} ~可以很容易地证明零向量 $\mathbf{0}$ 是唯一的，并且给定 $\mathbf{v} \in V$ 其加法逆元 $-\mathbf{v}$ 也是唯一的。

通过利用向量空间的性质5，6和8，也不难证明 $\mathbf{0} = 0 \mathbf{v}$ 对任何 $\mathbf{v} \in V$，并且 $-\mathbf{v} = (-1)\mathbf{v}$。注意，要做到这一点，仍然需要使用向量空间的其它性质的证明，特别是性质 3 和 4。


如果标量是通常的实数，我们称空间 $V$ 为\textbf{实}(real)向量空间。如果标量是复数，即如果我们能用复数乘以向量，我们称空间 $V$ 为\textbf{复}(complex)向量空间。

注意，任何复向量空间也都是实向量空间（如果我们能用复数乘以向量，那么我们也能用实数乘以向量），但反之则不然。

也有可能考虑标量是任意域 $\mathbb{F}$ 的元素的情况。在这种情况下，我们说 $V$ 是域 $\mathbb{F}$ 上的向量空间。虽然本书中的许多构造（特别是第一至第三章中的所有内容）适用于一般域，但本书仅考虑实数和复数向量空间。

如果我们不指定标量集，或者使用字母 $\mathbb{F}$ 来表示它，那么结果对实数和复数空间都成立。如果我们想区分实数和复数情况，我们会明确说明我们正在考虑哪种情况。

请注意，在定义域 $\mathbb{F}$ 上的向量空间定义中，我们\textbf{要求}标量集是一个域，因此我们可以始终进行除法（无余数），尽管不能进行整数除法。因此，可以考虑有理数域上的向量空间，但不能考虑整数环上的向量空间。





\textbf{1.1. 例子}

\textbf{示例}~ 空间 $\mathbb{R}^n$ 由所有大小为 $n$ 的列向量组成：
$$
\mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
$$
其元素是实数。加法和乘法是逐个元素定义的，即
$$
\alpha \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} = \begin{pmatrix} \alpha v_1 \\ \alpha v_2 \\ \vdots \\ \alpha v_n \end{pmatrix}, \quad \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} + \begin{pmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{pmatrix} = \begin{pmatrix} v_1 + w_1 \\ v_2 + w_2 \\ \vdots \\ v_n + w_n \end{pmatrix}
$$

\textbf{示例}~ 空间 $\mathbb{C}^n$ 也由大小为 $n$ 的列向量组成，只是元素现在是复数。加法和乘法与 $\mathbb{R}^n$ 中的定义完全相同，唯一的区别是我们现在可以乘以\textbf{复}数，即 $\mathbb{C}^n$ 是一个\textbf{复}向量空间。

本书中的许多结果对于 $\mathbb{R}^n$ 和 $\mathbb{C}^n$ 都成立。在这种情况我们使用符号 $\mathbb{F}^n$。

\textbf{示例}~ 空间 $M_{m \times n}$（也记作 $M_{m,n}$）是 $m \times n$ 矩阵的集合：加法和标量乘法是逐个元素定义的。如果我们只允许实数项（因此只允许实数乘法），那么我们得到一个实向量空间；如果我们允许复数项和复数乘法，那么我们得到一个复向量空间。

形式上，我们必须区分实数情况和复数情况，即写成 $M^{\mathbb{R}}_{m,n}$ 或 $M^{\mathbb{C}}_{m,n}$。然而，在大多数情况下，实数和复数情况之间没有区别，也无需指明我们正在考虑哪种情况。如果有区别，我们会明确说明正在考虑哪种情况。

\textbf{注释}~ 正如我们上面提到的，向量空间的公理仅仅是（实数或复数）数字的代数运算的熟悉规则，所以如果我们把标量（数字）当作向量，所有公理都会被满足！因此，实数集 $\mathbb{R}$ 是一个实向量空间，复数集 $\mathbb{C}$ 是一个复向量空间。

更重要的是，由于在上面的例子中，所有向量运算（加法和标量乘法）都是逐个元素执行的，因此对于这些例子，向量空间的公理自动满足，因为它们对于标量是满足的（你能看出为什么吗？）。所以，我们不必检验公理，而是自动获得了这些例子确实是向量空间的事实！

同样的情况也适用于下一个例子，即多项式，其中多项式的系数起着条目的作用。

\textbf{示例}~ 空间 $\mathbb{P}_n$ 是最多 $n$ 次的多项式，包含所有形式为
$$p(t) = a_0 + a_1 t + a_2 t^2 + \dots + a_n t^n$$
的多项式，其中 $t$ 是自变量。注意，一些或甚至所有系数 $a_k$ 可以是 0。

在实系数 $a_k$ 的情况下，我们得到一个实向量空间，复数系数则构成一个复向量空间。同样，我们只在情况至关重要时才明确说明我们正在处理实数或复数情况；否则，一切都适用于这两种情况。

\textbf{问题}~ 在以上每个例子中，零向量是什么？

\textbf{1.2. 矩阵表示}

一个 $m \times n$ 矩阵是具有 $m$ 行和 $n$ 列的矩形数组。数组的元素称为矩阵的\textbf{项}(entry)。

通常我们可以方便地用带下标的字母来表示矩阵的项：第一个下标表示项所在的行号，第二个下标表示列号。例如，
\begin{equation}
A = (a_{j,k})_{m \times n, j=1, k=1}^n = \begin{pmatrix}
a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
a_{2,1} & a_{2,2} & \dots & a_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \dots & a_{m,n}
\end{pmatrix}
\end{equation}
是一种写 $m \times n$ 矩阵的一般方式。

非常频繁地，对于矩阵 $A$，位于第 $j$ 行和第 $k$ 列的项表示为 $A_{j,k}$ 或 $(A)_{j,k}$，有时像上面 (1.1) 那个例子一样，用小写字母表示相同的字母，也用于表示矩阵的项。

给定矩阵 $A$，它的\textbf{转置}(transpose)（或转置矩阵）$A^T$ 是通过将 $A$ 的行变为列来定义的。例如
$$
\begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix}^T = \begin{pmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{pmatrix}.
$$
所以，$A^T$ 的列是 $A$ 的行，反之亦然，$A^T$ 的行是 $A$ 的列。

正式定义如下：$(A^T)_{j,k} = (A)_{k,j}$ 意思是 $A^T$ 中第 $j$ 行第 $k$ 列的项等于 $A$ 中第 $k$ 行第 $j$ 列的项。

转置矩阵在线性变换方面有一个很好的解释，即它给出了所谓的\textbf{伴随}(adjoint)变换。我们将在后面详细讨论这一点，但现在转置只是一个有用的形式运算。

转置的一个早期用途是我们可以将列向量 $\mathbf{x} \in \mathbb{F}^n$（回想一下 $\mathbb{F}$ 是 $\mathbb{R}$ 或 $\mathbb{C}$）写成 $\mathbf{x} = (x_1, x_2, \dots, x_n)^T$。如果我们将列向量垂直放置，它将占用更多的空间。

\textbf{练习}~
\footnote{
按照问卷调查结果，译者不会为本书制作答案。所有练习请读者自行完成。
}

1.1. 令 $\mathbf{x} = (1, 2, 3)^T$, $\mathbf{y} = (y_1, y_2, y_3)^T$, $\mathbf{z} = (4, 2, 1)^T$。计算 $2\mathbf{x}$, $3\mathbf{y}$, $\mathbf{x} + 2\mathbf{y} - 3\mathbf{z}$。

1.2. 下列集合（在自然的加法和标量乘法下）哪些是向量空间？请给出你的理由。

a) $[0, 1]$ 区间上所有连续函数的集合；

b) $[0, 1]$ 区间上所有非负函数的集合；

c) \textbf{恰好} $n$ 次多项式的集合；

d) 所有对称 $n \times n$ 矩阵的集合，即满足 $A^T = A$ 的矩阵 $A = \{a_{j,k}\}_{j,k=1}^n $。


1.3. 对错题：

a) 每个向量空间都包含一个零向量；

b) 一个向量空间可以有多个零向量；

c) 一个 $m \times n$ 矩阵有 $m$ 行和 $n$ 列；

d) 如果 $f$ 和 $g$ 是 $n$ 次多项式，那么 $f+g$ 也是 $n$ 次多项式；

e) 如果 $f$ 和 $g$ 是最高为 $n$ 次的多项式，那么 $f+g$ 也是最高为 $n$ 次的多项式。

1.4. 证明向量空间 $V$ 的零向量 $\mathbf{0}$ 是唯一的。

1.5. 空间 $M_{2 \times 3}$ 的零向量是什么矩阵？

1.6. 证明向量空间公理 4 中定义的加法逆元是唯一的。

1.7. 证明 $0 \mathbf{v} = \mathbf{0}$ 对任何向量 $\mathbf{v} \in V$。

1.8. 证明对于任何向量 $\mathbf{v}$，其加法逆元 $-\mathbf{v}$ 由 $(-1)\mathbf{v}$ 给出。



\section{线性组合，基}

设 $V$ 为向量空间，又设 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p \in V$ 为一组向量。向量 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p$ 的\textbf{线性组合}(linear combination)是形式为
$$
\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_p \mathbf{v}_p = \sum_{k=1}^p \alpha_k \mathbf{v}_k
$$
的和。

\textbf{定义}~ 向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \in V$ 称为 $V$ 的\textbf{基}(base)（或\textbf{基底}），如果任何向量 $\mathbf{v} \in V$ 都可以唯一地表示为线性组合
$$
\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_n \mathbf{v}_n = \sum_{k=1}^n \alpha_k \mathbf{v}_k.
$$
系数 $\alpha_1, \alpha_2, \dots, \alpha_n$ 称为向量 $\mathbf{v}$ 的\textbf{坐标}(coordinates)（在基 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 下，或相对于基 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$）。

另一种说 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是基的方式是说，对于任何可能的右侧 $\mathbf{v}$ 的选择，方程 $x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \dots + x_m \mathbf{v}_n = \mathbf{v}$（未知数为 $x_k$）有唯一解。

在讨论基的任何性质之前
\footnote{
"basis" 的复数是 "bases"，与 "base" 的复数相同。
}
，让我们给出几个例子，说明这些对象确实存在，并且研究它们是有意义的。


\textbf{示例2.2.}~ 
在第一个例子中，空间 ${V}$ 是 $\mathbb{F}^n$，其中 $\mathbb{F}$ 是实数 $\mathbb{R}$ 或复数 $\mathbb{C}$。考虑向量
$$ \mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \quad \mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \quad \mathbf{e}_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \\ \vdots \\ 0 \end{pmatrix}, \quad \dots, \quad \mathbf{e}_n = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix} ,$$
（向量 $\mathbf{e}_k$ 除第 $k$ 个分量为 1 外，其余分量均为 0）。向量组 $\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n$ 是 $\mathbb{F}^n$ 的一个基。事实上，任意向量 $\mathbf{v} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \in \mathbb{F}^n$ 都可以表示为线性组合
$$ \mathbf{v} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + \dots + x_n \mathbf{e}_n = \sum_{k=1}^n x_k \mathbf{e}_k $$
并且这种表示是唯一的。向量组 $\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \in \mathbb{F}^n$ 被称为 $\mathbb{F}^n$ 中的\textbf{标准基}(standard basis)。


\textbf{示例2.3.}~ 
在这个例子中，空间是至多 $n$ 次多项式构成的空间 $\mathbb{P}_n$。考虑向量（多项式） $\mathbf{e}_0, \mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \in \mathbb{P}_n$ 定义为
$$ \mathbf{e}_0 := 1, \quad \mathbf{e}_1 := t, \quad \mathbf{e}_2 := t^2, \quad \mathbf{e}_3 := t^3, \quad \dots, \quad \mathbf{e}_n := t^n $$
显然，任意多项式$p$， $p(t) = a_0 + a_1 t + a_2 t^2 + \dots + a_n t^n$ 都存在唯一的表示
$$ p = a_0 \mathbf{e}_0 + a_1 \mathbf{e}_1 + \dots + a_n \mathbf{e}_n $$
因此，向量组 $\mathbf{e}_0, \mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \in \mathbb{P}_n$ 是 $\mathbb{P}_n$ 中的一个基。我们将它称为 $\mathbb{P}_n$ 中的标准基。

\textbf{注释}~
如果一个向量空间 $V$ 拥有基 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$，那么任何向量 $\mathbf{v}$ 都可以由其在分解 $\mathbf{v} = \sum_{k=1}^n \alpha_k \mathbf{v}_k$ 中的系数唯一确定
\footnote{这是一个非常重要的注记，将在本书中贯穿使用。它允许我们将任何关于标准列空间 $\mathbb{F}^n$ 的陈述转化为关于具有基 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 的向量空间 $V$ 的陈述。}。
因此，如果我们把系数 $\alpha_k$ 堆叠成一个列向量，我们可以像处理列向量一样处理它们，即像处理 $\mathbb{F}^n$ 的元素一样（同样，这里的 $\mathbb{F}$ 是 $\mathbb{R}$ 或 $\mathbb{C}$，但所有内容也都适用于抽象域 $\mathbb{F}$）。

具体来说，如果 $\mathbf{v} = \sum_{k=1}^n \alpha_k \mathbf{v}_k$ 且 $\mathbf{w} = \sum_{k=1}^n \beta_k \mathbf{v}_k$，那么
$$ \mathbf{v} + \mathbf{w} = \sum_{k=1}^n \alpha_k \mathbf{v}_k + \sum_{k=1}^n \beta_k \mathbf{v}_k = \sum_{k=1}^n (\alpha_k + \beta_k) \mathbf{v}_k $$
也就是说，要得到和的坐标列，只需将各个向量的坐标列相加。类似地，要得到 $\alpha \mathbf{v}$ 的坐标，只需将 $\mathbf{v}$ 的坐标列乘以 $\alpha$。



\textbf{2.1. 生成系统与线性无关系统}

基的定义是任何向量都可以表示为线性组合。这句话实际上是两个陈述，即表示存在和表示唯一。让我们分别分析这两个陈述。

如果我们只考虑存在性，我们就得到以下概念：

\textbf{定义}~  向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p \in V$ 称为 $V$ 中的\textbf{生成系统}(generating system)（也称为\textbf{张成系统}(spanning system)或\textbf{完备系统}(complete system)），如果任何向量 $\mathbf{v} \in V$ 都可以表示为线性组合
$$
\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_p \mathbf{v}_p = \sum_{k=1}^p \alpha_k \mathbf{v}_k
$$
与基的定义不同之处在于，我们不假定上面的表示是唯一的。


这里，“生成”、“张成”和“完备”是同义词。我个人更喜欢“完备”这个词，因为我的算子理论背景。生成和张成在更常见的线性代数教材中使用。

显然，任何基都是生成（完备）系统。此外，如果我们有一个基，例如 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$，并且我们向其中添加几个向量，例如 $\mathbf{v}_{n+1}, \dots, \mathbf{v}_p$，那么新的系统将是生成（完备）系统。实际上，我们可以将任何向量表示为向量 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 的线性组合，并将新向量（通过将相应的系数 $\alpha_k = 0$）忽略掉。

现在，让我们关注唯一性。我们不想担心存在性，所以让我们考虑零向量 $\mathbf{0}$，它总是可以表示为线性组合。

\textbf{定义}~  线性组合 $\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_p \mathbf{v}_p$ 称为\textbf{平凡}的(trivial)，如果 $\alpha_k = 0 \ \forall k$。

平凡线性组合总是（对于所有选择的向量 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p$）等于 $\mathbf{0}$，这也许就是这个名字的原因。


\textbf{定义}~ 向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p \in V$ 称为\textbf{线性无关}(linearly independent)的，如果只有平凡线性组合（$\sum_{k=1}^p \alpha_k \mathbf{v}_k$ 其中 $\alpha_k = 0 \ \forall k$）等于 $\mathbf{0}$。

换句话说，系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p$ 是线性无关的，当且仅当方程 $x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \dots + x_p \mathbf{v}_p = \mathbf{0}$（未知数为 $x_k$）只有一个平凡解 $x_1 = x_2 = \dots = x_p = 0$。


如果系统不是线性无关的，则称为\textbf{线性相关}(linearly dependent)。通过否定线性无关的定义，我们得到以下定义：



\textbf{定义}~ 向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p$ 称为\textbf{线性相关}的，如果 $\mathbf{0}$ 可以表示为非平凡的线性组合，即 $0 = \sum_{k=1}^p \alpha_k \mathbf{v}_k$。非平凡意味着至少有一个系数 $\alpha_k$ 非零。这可以（并且通常）写成 $\sum_{k=1}^p |\alpha_k| \neq 0$。


因此，重申定义，我们可以说，一个系统是线性相关的，当且仅当存在不全为零的标量 $\alpha_1, \alpha_2, \dots, \alpha_p$，使得 
$$\sum_{k=1}^p \alpha_k \mathbf{v}_k = \mathbf{0}.$$

另一个定义（关于方程）是，系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p$ 是线性相关的，当且仅当方程 
$$x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \dots + x_p \mathbf{v}_p = \mathbf{0}$$
（未知数为 $x_k$）有一个非平凡解。非平凡，再次意味着至少有一个 $x_k$ 不为零，并且可以写成 $\sum_{k=1}^p |x_k| \neq 0$。

以下命题提供了线性相关系统的一个替代描述。

\textbf{命题 2.6.}~ 向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p \in V$ 是线性相关的，当且仅当其中一个向量 $\mathbf{v}_k$ 可以表示为其他向量的线性组合，
\begin{equation}\nonumber
 (2.1)\quad \mathbf{v}_k = \sum_{j=1, j \neq k}^p \beta_j \mathbf{v}_j
\end{equation}


\textbf{证明}~ 
 假设系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p$ 是线性相关的。那么存在不全为零的标量 $\alpha_k$（$\sum_{k=1}^p |\alpha_k| \neq 0$），使得 
$$\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_p \mathbf{v}_p = \mathbf{0}.$$
设 $k$ 是 $\alpha_k \neq 0$ 的下标。那么，将除 $\alpha_k \mathbf{v}_k$ 之外的所有项移到右侧，我们得到 $$\alpha_k \mathbf{v}_k = -\sum_{j=1, j \neq k}^p \alpha_j \mathbf{v}_j.$$
将两边除以 $\alpha_k$，我们得到 (2.1) 式，其中 $\beta_j = -\alpha_j / \alpha_k$。

另一方面，如果 (2.1) 式成立，则 $\mathbf{0}$ 可以表示为非平凡线性组合 
$$\mathbf{v}_k - \sum_{j=1, j \neq k}^p \beta_j \mathbf{v}_j = \mathbf{0}.$$


显然，任何基都是线性无关的系统。实际上，如果一个系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是基，则 $\mathbf{0}$ 允许唯一表示 
$$\mathbf{0} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_n \mathbf{v}_n = \sum_{k=1}^n \alpha_k \mathbf{v}_k.$$
因为平凡线性组合总是给出 $\mathbf{0}$，所以平凡组合必须是给出 $\mathbf{0}$ 的\textbf{唯一}组合。

因此，正如我们已经讨论过的，如果一个系统是基，那么它就是完备（生成）的并且线性无关的系统。以下命题表明其反向蕴含也成立。

\textbf{命题 2.7.} 向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \in V$ 是基，当且仅当它线性无关且完备（生成）。

\textbf{证明}~ 
我们已经知道基总是线性无关且完备的，所以命题的一个方向已经证明。

让我们证明另一个方向。假设系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是线性和完备的。取任意向量 $\mathbf{v} \in V$。由于系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是线性完备的（生成的），$\mathbf{v}$ 可以表示为 
$$\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_n \mathbf{v}_n = \sum_{k=1}^n \alpha_k \mathbf{v}_k.$$
我们只需要证明这个表示是唯一的。

假设 $\mathbf{v}$ 还有另一个表示 $$\mathbf{v} = \sum_{k=1}^n \tilde{\alpha}_k \mathbf{v}_k.$$
那么 
$$\sum_{k=1}^n (\alpha_k - \tilde{\alpha}_k) \mathbf{v}_k = \sum_{k=1}^n \alpha_k \mathbf{v}_k - \sum_{k=1}^n \tilde{\alpha}_k \mathbf{v}_k = \mathbf{v} - \mathbf{v} = \mathbf{0}.$$
由于系统是线性无关的，$\alpha_k - \tilde{\alpha}_k = 0 \ \forall k$，因此表示 $\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_n \mathbf{v}_n$ 是唯一的。

\textbf{注释}~ 在许多教材中，基被定义为完备且线性无关的系统（根据命题 2.7，这个定义等价于我们的定义）。虽然这个定义比本书提出的定义更常见，但我更喜欢后者。它强调了基的主要性质，即任何向量都可以唯一地表示为一个线性组合。

\textbf{命题 2.8.} 任何（有限）生成系统都包含一个基。

\textbf{证明}~ 
假设 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p \in V$ 是生成（完备）集。如果它是线性无关的，那么它就是基，我们就完成了证明。

假设它不是线性无关的，即它是线性相关的。那么存在一个向量 $\mathbf{v}_k$ 可以表示为向量 $\mathbf{v}_j$ ($j \neq k$) 的线性组合。

由于 $\mathbf{v}_k$ 可以表示为向量 $\mathbf{v}_j$ ($j \neq k$) 的线性组合，因此任何向量 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p$ 的线性组合都可以表示为相同的向量（即 $\mathbf{v}_j$, $1 \le j \le p$, $j \neq k$）的线性组合（即删掉 $\mathbf{v}_k$ 之后的向量）。因此，如果我们删除向量 $\mathbf{v}_k$，新的系统仍然是完备的。

如果新的系统是线性无关的，我们就完成了证明。如果不是，我们重复这个过程。

重复这个过程有限次后，我们将得到一个线性无关且完备的系统，否则我们将删除所有向量，最后得到一个空集。

因此，任何有限的完备（生成）集都包含一个完备的线性无关子集，即一个基。



\textbf{练习}~

2.1. 在 $3 \times 2$ 矩阵空间 $M_{3 \times 2}$ 中找到一个基。

2.2. 对错题：

a) 包含零向量的任何集合都是线性相关的；

b) 基必须包含 $\mathbf{0}$；

c) 线性相关集的子集是线性相关的；

d) 线性无关集的子集是线性无关的；

e) 如果 $\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_n \mathbf{v}_n = \mathbf{0}$，那么所有标量 $\alpha_k$ 都为零。

2.3. 回忆一下，如果 $A^T = A$，则矩阵称为\textbf{对称}矩阵。写下一个 $2 \times 2$ 对称矩阵空间的基（有许多可能的答案）。基中有多少个元素？

2.4. 写出以下空间的基：

a) $3 \times 3$ 对称矩阵；

b) $n \times n$ 对称矩阵；

c) $n \times n$ 反对称矩阵 ($A^T = -A$)。

2.5. 设向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r$ 是线性无关的，但不是生成的。证明可以找到向量 $\mathbf{v}_{r+1}$ 使得系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r, \mathbf{v}_{r+1}$ 是线性无关的。

提示：选择任何不能表示为 $\sum_{k=1}^r \alpha_k \mathbf{v}_k$ 的向量作为 $\mathbf{v}_{r+1}$，并证明系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r, \mathbf{v}_{r+1}$ 是线性无关的。

2.6. 向量 $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ 是否可能是线性相关的，而向量 $\mathbf{w}_1 = \mathbf{v}_1 + \mathbf{v}_2$, $\mathbf{w}_2 = \mathbf{v}_2 + \mathbf{v}_3$ 和 $\mathbf{w}_3 = \mathbf{v}_3 + \mathbf{v}_1$ 是线性、\textbf{独立}的？



\section{线性变换~矩阵-向量乘法}

从集合 $X$ 到集合 $Y$ 的\textbf{变换}
\footnote{
单词“变换”（transformation）、“映射”（map, mapping）、“算子”（operator）、“函数”（function）这些词都表示同一个概念。
} (transformation)$T$ 是一个规则，它为每个自变量（输入）$x \in X$ 分配一个值（输出）$y = T(x) \in Y$。

集合 $X$ 称为 $T$ 的\textbf{定义域}(domain)，集合 $Y$ 称为 $T$ 的\textbf{目标空间}(target space)或\textbf{上域}(codomain)。

我们写 $T: X \to Y$ 来表示 $T$ 是一个定义域为 $X$、目标空间为 $Y$ 的变换。

\textbf{定义}~  设 $V$，$W$ 为向量空间（在同一域 $\mathbb{F}$ 上）。变换 $T: V \to W$ 称为\textbf{线性}的，如果

1. $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ $\quad \forall \mathbf{u}, \mathbf{v} \in V$；

2. $T(\alpha \mathbf{v}) = \alpha T(\mathbf{v})$ 对所有 $\mathbf{v} \in V$ 和所有标量 $\alpha \in \mathbb{F}$.


性质 1 和 2 一起等价于以下一个性质：
$T(\alpha \mathbf{u} + \beta \mathbf{v}) = \alpha T(\mathbf{u}) + \beta T(\mathbf{v})$ ~~对所有 $\mathbf{u}, \mathbf{v} \in V$ ~~和所有标量 $\alpha, \beta$。



\textbf{一些例子}~
您以前接触过线性变换，可能甚至没有意识到，如下例所示。

\textbf{示例}~ 
\textbf{微分}: 设 $V = \mathbb{P}_n$（至多 $n$ 次多项式的集合），$W = \mathbb{P}_{n-1}$，令 $T : \mathbb{P}_n \to \mathbb{P}_{n-1}$ 为微分算子，
$$ T(p) := p' \quad \forall p \in \mathbb{P}_n. $$
因为 $(f+g)' = f' + g'$ 且 $(\alpha f)' = \alpha f'$，这是一个线性变换。


\textbf{示例}~
\textbf{旋转}: 在这个例子中，$V = W = \mathbb{R}^2$（通常的坐标平面），一个变换 $T_\gamma : \mathbb{R}^2 \to \mathbb{R}^2$ 将 $\mathbb{R}^2$ 中的一个向量逆时针旋转 $\gamma$ 弧度。由于 $T_\gamma$ 整体将平面旋转，它也整体旋转了用于定义两个向量之和的平行四边形（平行四边形法则）。因此，线性变换的性质 1 成立。也很容易看出性质 2 也为真。



\red{（在此处应插入rotation figure）}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{rotation_figure.png} % 假设有一个旋转图形文件
%     \caption{旋转}
%     \label{fig:rotation}
% \end{figure}

\textbf{示例}~ 
\textbf{反射}: 在这个例子中，同样 $V = W = \mathbb{R}^2$，变换 $T : \mathbb{R}^2 \to \mathbb{R}^2$ 是关于第一坐标轴的反射，参见图 \ref{fig:reflection}。
也可以几何地证明这个变换是线性的，但我们将使用另一种方法来证明。

即，很容易写出 $T$ 的公式：
$$ T\left(\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\right) = \begin{pmatrix} x_1 \\ -x_2 \end{pmatrix} $$
并且从这个公式可以看出，这个变换是线性的。



\textbf{示例}~ 
让我们来研究线性变换 $T : \mathbb{R} \to \mathbb{R}$。任何这样的变换都由公式 $T(x) = ax$ 给出，其中 $a = T(1)$。
事实上，$$T(x) = T(x \times 1) = x T(1) = xa = ax.$$
因此，$\mathbb{R}$ 的任何线性变换仅仅是乘以一个常数。





\textbf{3.2. 线性变换 $\mathbb{F}^n \to \mathbb{F}^m$。矩阵-向量乘法}

事实证明，从 $\mathbb{F}^n$ 到 $\mathbb{F}^m$ 的线性变换 $T$ 也表示为乘法，不是乘标量，而是乘矩阵。我们来看看怎么做。

设 $T: \mathbb{F}^n \to \mathbb{F}^m$ 是一个线性变换。计算 $T(\mathbf{x})$ 对所有向量 $\mathbf{x} \in \mathbb{F}^n$ 需要哪些信息？我的主张是，知道 $T$ 如何作用于 $\mathbb{F}^n$ 的标准基 $\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n$ 就足够了。也就是说，知道 $n$ 个 $\mathbb{F}^m$ 中的向量（即大小为 $m$ 的向量）， $$\mathbf{a}_1 = T(\mathbf{e}_1), \mathbf{a}_2 := T(\mathbf{e}_2), \dots, \mathbf{a}_n := T(\mathbf{e}_n)$$ 就足够了。

实际上，设 $$\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}.$$那么 $\mathbf{x} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + \dots + x_n \mathbf{e}_n = \sum_{k=1}^n x_k \mathbf{e}_k$ 并且 $$T(\mathbf{x}) = T(\sum_{k=1}^n x_k \mathbf{e}_k) = \sum_{k=1}^n T(x_k \mathbf{e}_k) = \sum_{k=1}^n x_k T(\mathbf{e}_k) = \sum_{k=1}^n x_k \mathbf{a}_k.$$

因此，如果我们把向量（列）$\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_n$ 组合成一个矩阵 $A = [\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_n]$（$\mathbf{a}_k$ 是 $A$ 的第 $k$ 列，$k = 1, 2, \dots, n$），这个矩阵就包含了关于 $T$ 的所有信息。让我们看看如何定义矩阵与向量（列）的乘积来将变换 $T$ 表示为乘积，$T(\mathbf{x}) = A \mathbf{x}$。设
$$
A = \begin{pmatrix}
a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
a_{2,1} & a_{2,2} & \dots & a_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \dots & a_{m,n}
\end{pmatrix}
$$
回忆一下，$A$ 的第 $k$ 列是向量 $\mathbf{a}_k$，即 $$\mathbf{a}_k = \begin{pmatrix} a_{1,k} \\ a_{2,k} \\ \vdots \\ a_{m,k} \end{pmatrix}.$$
那么，如果我们希望 $A \mathbf{x} = T(\mathbf{x})$，我们得到
$$
A \mathbf{x} = \sum_{k=1}^n x_k \mathbf{a}_k = x_1 \begin{pmatrix} a_{1,1} \\ a_{2,1} \\ \vdots \\ a_{m,1} \end{pmatrix} + x_2 \begin{pmatrix} a_{1,2} \\ a_{2,2} \\ \vdots \\ a_{m,2} \end{pmatrix} + \dots + x_n \begin{pmatrix} a_{1,n} \\ a_{2,n} \\ \vdots \\ a_{m,n} \end{pmatrix}
$$
所以，矩阵-向量乘法应该通过以下\textbf{按列坐标规则}(column by coordinating rule)执行：
$$\fbox{将矩阵的每一列乘以向量的相应坐标。}$$

\textbf{示例}~
$$
\begin{pmatrix} 1 & 2 & 3 \\ 3 & 2 & 1 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} = 1 \begin{pmatrix} 1 \\ 3 \end{pmatrix} + 2 \begin{pmatrix} 2 \\ 2 \end{pmatrix} + 3 \begin{pmatrix} 3 \\ 1 \end{pmatrix} = \begin{pmatrix} 1+4+9 \\ 3+4+3 \end{pmatrix} = \begin{pmatrix} 14 \\ 10 \end{pmatrix}.
$$

“按列坐标规则”对于表示变换为乘积非常适用。它在后面不同的理论构造中也将会非常重要。

然而，在手动计算时，逐个条目计算结果更方便。这可以表示为以下\textbf{按行列规则}(row by column rule)：

\fbox{要得到结果的第 $k$ 个条目，需要将矩阵的第 $k$ 行乘以向量，即，如果 $A \mathbf{x} = \mathbf{y}$，}

\fbox{那么
$y_k = \sum_{j=1}^n a_{k,j} x_j, \quad k = 1, 2, \dots, m;$}

这里 $x_j$ 和 $y_k$ 分别是向量 $\mathbf{x}$ 和 $\mathbf{y}$ 的坐标，而 $a_{j,k}$ 是矩阵 $A$ 的项。

\textbf{示例}~
$$
\begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} = \begin{pmatrix} 1 \cdot 1 + 2 \cdot 2 + 3 \cdot 3 \\ 4 \cdot 1 + 5 \cdot 2 + 6 \cdot 3 \end{pmatrix} = \begin{pmatrix} 1+4+9 \\ 4+10+18 \end{pmatrix} = \begin{pmatrix} 14 \\ 32 \end{pmatrix}
$$


\textbf{3.3. 线性变换与生成集}

正如我们在上面讨论的，作用于 $\mathbb{F}^n$ 到 $\mathbb{F}^m$ 的线性变换 $T$ 完全由其在 $\mathbb{F}^n$ 标准基上的值定义。

我们考虑标准基的事实并非关键，可以考虑任何基，甚至任何生成（张成）集。也就是说，

\fbox{线性变换 $T: V \to W$ 完全由其在生成集上的值定义（特别是由其在基上的值定义）。}
因此，如果 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是 $V$ 中的生成集（特别地，如果它是基），并且 $T$ 和 $T_1$ 是(具有相同定义域和目标空间的)两个线性变换 ($T, T_1: V \to W$) 并且 $$T \mathbf{v}_k = T_1 \mathbf{v}_k, k = 1, 2, \dots, n,$$ 
则 $T = T_1$。

这个命题的证明是显然的，留作练习。

\textbf{3.4. 结论}
\begin{itemize}
\item 要获得 $T: \mathbb{F}^n \to \mathbb{F}^m$ 的线性变换的矩阵，只需将向量 $\mathbf{a}_k = T \mathbf{e}_k$（其中 $\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n$ 是 $\mathbb{F}^n$ 的标准基）组合成一个矩阵：矩阵的第 $k$ 列是 $\mathbf{a}_k$, $k = 1, 2, \dots, n$。
\item 如果已知线性变换 $T$ 的矩阵 $A$，则 $T(\mathbf{x})$ 可以通过矩阵-向量乘法找到，$T(\mathbf{x}) = A \mathbf{x}$。要执行矩阵-向量乘法，可以使用“按列坐标规则”或“按行列规则”。
\end{itemize}
后者似乎更适合手动计算。前者非常适合并行计算，并且将在后面不同的理论构造中使用。

对于线性变换 $T: \mathbb{F}^n \to \mathbb{F}^m$，其矩阵通常记作 $[T]$。然而，人们常常不区分线性变换和它的矩阵，并使用相同的符号表示两者。当它不引起混淆时，我们也将使用相同的符号表示变换和它的矩阵。

由于线性变换本质上是乘法，因此 $T \mathbf{v}$ 这个表示通常会被采用，而不是 $T(\mathbf{v})$
\footnote{
$T \mathbf{v}$ 的表示比 $T(\mathbf{v})$更常用。
}
。我们将使用这种表示法。注意，通常的代数运算顺序适用，即 $T \mathbf{v} + \mathbf{u}$ 表示 $T(\mathbf{v}) + \mathbf{u}$，而不是 $T(\mathbf{v} + \mathbf{u})$。

\textbf{注释}~ 在矩阵-向量乘法 $A \mathbf{x}$ 中，矩阵 $A$ 的列数必须与向量 $\mathbf{x}$ 的大小一致
\footnote{
在使用“行乘以列”规则进行矩阵向量乘法时，请确保行中的元素(项)数量与列中的元素数量相同。行和列的元素应该同时结束：如果不满足，则乘法未定义。
}
，即 $\mathbb{F}^n$ 中的向量只能被 $m \times n$ 矩阵相乘。

这是有意义的，因为 $m \times n$ 矩阵定义了一个从 $\mathbb{F}^n$ 到 $\mathbb{F}^m$ 的线性变换，所以向量 $\mathbf{x}$ 必须属于 $\mathbb{F}^n$。

最简单的记住这个事实的方法是，如果在进行乘法时，只要你先用完了某个元素，这时还有一些元素未利用，那么乘法就是未定义的。

\textbf{注释}~ 不需要将自己局限于标准基的 $\mathbb{F}^n$ 的情况：当存在定义域和目标空间中的基时，本节中描述的所有内容都适用于任意向量空间。当然，如果改变了基，线性变换的矩阵也会不同。这将在后面的第 8 节中讨论。


\textbf{练习}~

3.1. 乘法：

a) $\begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix} \begin{pmatrix} 1 \\ 3 \\ 2 \end{pmatrix}$;

b) $\begin{pmatrix} 1 & 2 \\ 0 & 1 \\ 2 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 3 \end{pmatrix}$;

c) $\begin{pmatrix} 1 & 2 & 0 & 0 \\ 0 & 1 & 2 & 0 \\ 0 & 0 & 1 & 2 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}$;

d) $\begin{pmatrix} 1 & 2 & 0 \\ 0 & 1 & 2 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}$.

3.2. 找到 $\mathbb{R}^2$ 中关于直线 $x_1 = 3x_2$ 的反射的线性变换的矩阵。

3.3. 对于以下每个线性变换，找到它的矩阵：

a) $T: \mathbb{R}^2 \to \mathbb{R}^3$ 定义为 $T(\begin{pmatrix} x \\ y \end{pmatrix}) = \begin{pmatrix} x + 2y \\ 2x - 5y \\ 7y \end{pmatrix}$;

b) $T: \mathbb{R}^4 \to \mathbb{R}^3$ 定义为 $T(x_1, x_2, x_3, x_4)^T = (x_1 + x_2 + x_3 + x_4, x_2 - x_4, x_1 + 3x_2 + 6x_4)^T$;

c) $T: \mathbb{P}_n \to \mathbb{P}_n$, $T f(t) = f'(t)$（在标准基 $1, t, t^2, \dots, t^n$ 下找到矩阵）;

d) $T: \mathbb{P}_n \to \mathbb{P}_n$, $T f(t) = 2 f(t) + 3 f'(t) - 4 f''(t)$（同样在标准基 $1, t, t^2, \dots, t^n$ 下找到矩阵）.

3.4. 找到表示 $\mathbb{R}^3$ 中变换的 $3 \times 3$ 矩阵，这些变换：

a) 将每个向量投影到 $x-y$ 平面；

b) 将每个向量反射到 $x-y$ 平面；

c) 将 $x-y$ 平面绕 $z$ 轴旋转 $30^\circ$，同时保持 $z$ 轴不变。

3.5. 设 $A$ 是一个线性变换。如果 $\mathbf{z}$ 是线段 $[\mathbf{x}, \mathbf{y}]$ 的中点，证明 $A \mathbf{z}$ 是线段 $[A \mathbf{x}, A \mathbf{y}]$ 的中点。

\textbf{提示}：$\mathbf{z}$ 是线段 $[\mathbf{x}, \mathbf{y}]$ 的中点意味着什么？

3.6. 复数集 $\mathbb{C}$ 可以通过将 $z = x + {\rm i}  y \in \mathbb{C}$ 视为列向量 $(x, y)^T \in \mathbb{R}^2$ 来进行规范识别。

a) 将 $\mathbb{C}$ 视为复向量空间，证明通过 $\alpha = a + {\rm i} b \in \mathbb{C}$ 的乘法是在 $\mathbb{C}$ 中的线性变换。它的矩阵是什么？

b) 将 $\mathbb{C}$ 视为实向量空间 $\mathbb{R}^2$，证明通过 $\alpha = a + {\rm i} b \in \mathbb{C}$ 的乘法在那里定义了一个线性变换。它的矩阵是什么？

c) 定义 $T(x + {\rm i} y) = 2x - y + {\rm i} (x - 3y)$。证明这个变换不是 $\mathbb{C}$ 复向量空间中的线性变换，但如果我们把 $\mathbb{C}$ 视为实向量空间 $\mathbb{R}^2$，那么它在那里是一个线性变换（即 $T$ 是一个\textbf{实线性}但不是\textbf{复线性}变换）。找到这个实线性变换的矩阵。

3.7. 证明 $\mathbb{C}$ 中的任何线性变换（视为复向量空间）都是通过乘以 $\alpha \in \mathbb{C}$ 来实现的。


\section{线性变换，作为向量空间}

我们可以对线性变换进行哪些运算？我们总是可以在一个线性变换上乘上一个标量，也即，如果我们有一个线性变换 $T: V \to W$ 和一个标量 $\alpha$，我们可以定义一个新的变换 $\alpha T$ 为 $$(\alpha T) \mathbf{v} = \alpha (T \mathbf{v})~~\forall \mathbf{v} \in V.$$
可以很容易地检验， $\alpha T$ 也是一个线性变换：
%fill



$(\alpha T )(\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2)= \alpha( T (\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2))$（根据$\alpha T$的定义）

$= \alpha (\alpha_1 T \mathbf{v}_1 + \alpha_2 T \mathbf{v}_2)$（根据$T$的线性）

$= \alpha_1 \alpha T \mathbf{v}_1 +\alpha_2  \alpha T \mathbf{v}_2  = \alpha_1( \alpha T) \mathbf{v}_1 +\alpha_2( \alpha T) \mathbf{v}_2   $

如果 $T_1$ 和 $T_2$ 是具有相同定义域和目标空间的线性变换 ($T_1: V \to W$ 和 $T_2: V \to W$，或简写为 $T_1, T_2: V \to W$)，那么我们可以将这些变换相加，即定义一个新的变换 $T = (T_1 + T_2): V \to W$ 为 $$(T_1 + T_2) \mathbf{v} = T_1 \mathbf{v} + T_2 \mathbf{v}~~\forall \mathbf{v} \in V.$$
可以很容易地检验出变换 $T_1 + T_2$ 是线性的，只需重复上面关于 $\alpha T$ 线性的推理即可。

因此，如果我们固定向量空间 $V$ 和 $W$ 并考虑从 $V$ 到 $W$ 的所有线性变换的集合（我们将其表示为 $\mathcal{L}(V, W)$），我们可以定义 $\mathcal{L}(V, W)$ 上的 2 个运算：标量乘法和加法。可以很容易地证明这些运算满足向量空间公理，该公理在第 1 节中定义。

这里，读者不应该感到惊讶，因为向量空间的公理基本上意味着向量上的运算遵循代数运算的熟悉规则。而线性变换上的运算定义就是为了满足这些规则！

作为说明，让我们为向量空间公理的第一个分配律（公理 7）写下正式证明。我们想证明 $\alpha (T_1 + T_2) = \alpha T_1 + \alpha T_2$。对于 $V$ 中的任何 $\mathbf{v}$，

$\alpha (T_1 + T_2) \mathbf{v} = \alpha ((T_1 + T_2) \mathbf{v})$ （根据乘法的定义）

$= \alpha (T_1 \mathbf{v} + T_2 \mathbf{v})$ （根据和的定义）

$= \alpha T_1 \mathbf{v} + \alpha T_2 \mathbf{v}$ （根据 $W$ 的公理 7）

$= (\alpha T_1 + \alpha T_2) \mathbf{v}$ （根据和的定义）

因此，确实 $\alpha (T_1 + T_2) = \alpha T_1 + \alpha T_2$。

\textbf{注释}~ 线性运算（加法和标量乘法）在 $T: \mathbb{F}^n \to \mathbb{F}^m$ 的线性变换上对应于它们矩阵上的相应运算。由于我们知道 $m \times n$ 矩阵的集合是一个向量空间，这立即意味着 $\mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ 是一个向量空间。

我们首先提出了抽象证明，首先是因为它适用于一般的空间，例如，适用于没有基的向量空间，在那里我们不能使用坐标。其次，类似于这里展示的抽象推理，在许多地方都会使用，所以读者将受益于理解它。

并且随着读者获得一些数学上的成熟，他/她将看到这种抽象推理确实是非常简单的，几乎可以自动完成。



\section{线性变换的复合与矩阵乘法}

\textbf{5.1. 矩阵乘法的定义}

知道了矩阵-向量乘法，人们很容易猜出两个矩阵乘积 $AB$ 的自然定义：让我们用 $A$ 乘以 $B$ 的每个列（矩阵-向量乘法），并将得到的列向量连接成一个矩阵。形式上，

\fbox{如果 $\mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_r$ 是 $B$ 的列，那么 $A \mathbf{b}_1, A \mathbf{b}_2, \dots, A \mathbf{b}_r$ 就是矩阵 $AB$ 的列。}

回忆矩阵-向量乘法的“按行列规则”，我们得到矩阵的\textbf{按行列规则}：

\fbox{$AB$ 的项 $(AB)_{j,k}$（第 $j$ 行第 $k$ 列的项）定义为}

\fbox{~~~~~~~~~$(AB)_{j,k} = $ ( $A$ 的第 $j$ 行 ) $\cdot$ ( $B$ 的第 $k$ 列 ~~)}\\
形式上可以写成 
$$(AB)_{j,k} = \sum_{l} a_{j,l} b_{l,k},$$
如果 $a_{j,k}$ 和 $b_{j,k}$ 分别是矩阵 $A$ 和 $B$ 的项。

我特意没有提及矩阵 $A$ 和 $B$ 的大小，但如果我们回忆矩阵-向量乘法的按行列规则，我们可以看到，为了使乘法有定义，$A$ 的行的大小应等于 $B$ 的列的大小。

换句话说，乘积 $AB$ 有定义当且仅当 $A$ 是 $m \times n$ 的矩阵，$B$ 是 $n \times r$ 的矩阵。（这与从按行列规则获得的条件相同。）

\textbf{5.2. 动机：线性变换的复合}

这里，可以问问自己：为什么我们要使用如此复杂的乘法规则？为什么我们不直接逐个元素地相乘矩阵？

答案是，如上定义的乘法自然地源于线性变换的复合。

假设我们有两个线性变换，$T_1: \mathbb{F}^n \to \mathbb{F}^m$ 和 $T_2: \mathbb{F}^r \to \mathbb{F}^n$。定义变换的\textbf{复合} $T = T_1 \circ T_2$ 为
$$T(\mathbf{x}) = T_1(T_2(\mathbf{x})) ~~\ \forall \mathbf{x} \in \mathbb{F}^r.$$
请注意，$T_2(\mathbf{x}) \in \mathbb{F}^n$。由于 $T_1: \mathbb{F}^n \to \mathbb{F}^m$，表达式 $T_1(T_2(\mathbf{x}))$ 是有定义的，并且结果属于 $\mathbb{F}^m$。所以，$T: \mathbb{F}^r \to \mathbb{F}^m.$
\footnote{我们通常将线性变换与其矩阵等同，但在接下来的几个段落中，我们将区分它们。}

可以很容易地证明 $T$ 是一个线性变换（练习），所以它由一个 $m \times r$ 矩阵定义。已知 $T_1$ 和 $T_2$ 的矩阵，如何找到 $T$ 的矩阵？

令 $A$ 为 $T_1$ 的矩阵，令 $B$ 为 $T_2$ 的矩阵。正如我们在上一节中所讨论的，$T$ 的列是向量 $T(\mathbf{e}_1), T(\mathbf{e}_2), \dots, T(\mathbf{e}_r)$，其中 $\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_r$ 是 $\mathbb{F}^r$ 中的标准基。对于 $k = 1, 2, \dots, r$，我们有
$$T(\mathbf{e}_k) = T_1(T_2(\mathbf{e}_k)) = T_1(B \mathbf{e}_k) = T_1(\mathbf{b}_k) = A \mathbf{b}_k$$
（变换 $T_2$ 和 $T_1$ 分别就是乘 $B$ 和乘 $A$）。

所以，$T$ 的矩阵的列是 $A \mathbf{b}_1, A \mathbf{b}_2, \dots, A \mathbf{b}_r$，这正是矩阵 $AB$ 的定义方式！

让我们回到等同的观点。由于矩阵乘法与复合一致，我们可以（并且将）写作 $T_1 T_2$ 而不是 $T_1 \circ T_2$，以及 $T_1 T_2 \mathbf{x}$ 而不是 $T_1(T_2(\mathbf{x}))$。
\footnote{\textbf{注意}：变换的顺序！ }

注意在组合$T_1 T_2$中，先作变换$T_2$！记住这种方法的方法是看 $T_1 T_2 \mathbf{x}$ 中，变换 $T_2$ 首先与 $\mathbf{x}$ 相遇。

\textbf{注释}~ 除了“按行列规则”的矩阵乘法之外，还有另一种检查矩阵乘积维数的方法：对于复合 $T_1 T_2$ 的定义，必须使得 $T_2 \mathbf{x}$ 属于 $T_1$ 的定义域。如果 $T_2$ 作用于某个空间，比如 $\mathbb{F}^r$ 到 $\mathbb{F}^n$，那么 $T_1$ 必须作用于 $\mathbb{F}^n$ 到某个空间，比如 $\mathbb{F}^m$。因此，为了使 $T_1 T_2$ 有定义，$T_1$ 和 $T_2$ 的矩阵应该分别是 $m \times n$ 和 $n \times r$ 的大小——这与从“按行列规则”获得的条件相同。

\textbf{示例}~ 设 $T: \mathbb{R}^2 \to \mathbb{R}^2$ 是关于直线 $x_1 = 3x_2$ 的反射。这是一个线性变换，所以让我们找到它的矩阵。为了找到矩阵，我们需要计算 $T \mathbf{e}_1$ 和 $T \mathbf{e}_2$。然而，直接计算 $T \mathbf{e}_1$ 和 $T \mathbf{e}_2$ 需要比一个理智的人愿意记住的三角学更多的知识。

找到 $T$ 的矩阵的一个更简单的方法是将其表示为简单线性变换的复合。也就是说，设 $\gamma$ 是 $x_1$ 轴和直线 $x_1 = 3x_2$ 之间的夹角，设 $T_0$ 是关于 $x_1$ 轴的反射。那么要得到反射 $T$，我们可以先将平面旋转 $-\gamma$ 角，将直线 $x_1 = 3x_2$ 移动到 $x_1$ 轴，然后将所有内容在 $x_1$ 轴上反射，然后将平面旋转 $\gamma$ 角，将所有东西移回原位。形式上可以写成 
$$T = R_\gamma T_0 R_{-\gamma}$$
（注意项的顺序！），其中 $R_\gamma$ 是绕 $\gamma$ 角的旋转矩阵。$T_0$ 的矩阵很容易计算，是 
$$T_0 = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix},$$
旋转矩阵是已知的 
$$R_\gamma = \begin{pmatrix} \cos \gamma & -\sin \gamma \\ \sin \gamma & \cos \gamma \end{pmatrix}$$

$$R_{-\gamma} = \begin{pmatrix} \cos(-\gamma) & -\sin(-\gamma) \\ \sin(-\gamma) & \cos(-\gamma) \end{pmatrix} = \begin{pmatrix} \cos \gamma & \sin \gamma \\ -\sin \gamma & \cos \gamma \end{pmatrix}$$
为了计算 $\sin \gamma$ 和 $\cos \gamma$，取直线 $x_1 = 3x_2$ 上的一个向量，例如向量 $(3, 1)^T$。那么 
$$\cos \gamma = \frac{\text{第一个坐标}}{\text{长度}} = \frac{3}{\sqrt{3^2 + 1^2}} = \frac{3}{\sqrt{10}},$$
类似地 
$$\sin \gamma = \frac{\text{第二个坐标}}{\text{长度}} = \frac{1}{\sqrt{3^2 + 1^2}} = \frac{1}{\sqrt{10}}.$$

将所有内容收集起来，我们得到
$$T = R_\gamma T_0 R_{-\gamma} = \frac{1}{\sqrt{10}} \begin{pmatrix} 3 & -1 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} \frac{1}{\sqrt{10}} \begin{pmatrix} 3 & 1 \\ -1 & 3 \end{pmatrix} $$
$$= \frac{1}{10} \begin{pmatrix} 3 & -1 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} \begin{pmatrix} 3 & 1 \\ -1 & 3 \end{pmatrix}$$
最后一步是进行矩阵乘法以得到最终结果。


\textbf{5.3. 矩阵乘法的性质}

矩阵乘法享有许多我们从高中代数中熟悉的性质：

1. 结合律：$A(BC) = (AB)C$，只要等式的一侧或另一侧有定义；因此我们可以（并且将）简单地写成 $ABC$。

2. 分配律：$A(B + C) = AB + AC$, $(A + B)C = AC + BC$，只要每个等式的任一侧有定义。

3. 可以提取标量系数：$A(\alpha B) = (\alpha A) B = \alpha (AB) = \alpha AB$。

这些性质很容易证明。可以证明对应于线性变换的性质，然后它们几乎自然地从定义中得出。线性变换的性质然后蕴含了矩阵乘法的性质。

这里新的特点是交换律失败了：

~~~~~~~矩阵乘法通常是非交换的，即通常 $AB \neq BA$。

容易看出，期望矩阵乘法满足交换律是不合理的。确实，如果 $A$ 和 $B$ 是 $m \times n$ 和 $n \times r$ 大小的矩阵，那么乘积 $AB$ 有定义，但如果 $m \neq r$，则 $BA$ 未定义。

即使两个乘积都有定义，例如，当 $A$ 和 $B$ 是 $n \times n$（方阵）时，乘法仍然是非交换的。如果我们随机选择矩阵 $A$ 和 $B$，那么 $AB \neq BA$ 的概率很高：我们非常幸运时才能得到 $AB = BA$。

\textbf{5.4. 转置矩阵与乘法}

给定矩阵 $A$，它的\textbf{转置}(transpose)（或转置矩阵）$A^T$ 通过将 $A$ 的行变为列来定义。例如
$$
\begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix}^T = \begin{pmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{pmatrix}.
$$
所以，$A^T$ 的列是 $A$ 的行，反之亦然，$A^T$ 的行是 $A$ 的列。

形式定义如下：$(A^T)_{j,k} = (A)_{k,j}$ 意思是 $A^T$ 中第 $j$ 行第 $k$ 列的项等于 $A$ 中第 $k$ 行第 $j$ 列的项。

转置矩阵在线性变换方面有一个很好的解释，即它给出了所谓的\textbf{伴随}(adjoint)变换。我们将在后面详细研究这一点，但现在转置只是一个有用的形式运算。

转置的一个早期用途是我们可以将列向量 $\mathbf{x} \in \mathbb{F}^n$ 写成 $\mathbf{x} = (x_1, x_2, \dots, x_n)^T$。如果我们将列向量垂直放置，它将占用更多的空间。

一个简单的分析表明 $$(AB)^T = B^T A^T,$$
即当你取乘积的转置时，你需要改变项的顺序。

\textbf{5.5. 迹与矩阵乘法}

对于一个方阵（$n \times n$）$A = (a_{j,k})$，它的\textbf{迹}(trace)（记作 $\text{trace } A$）是其对角线项的和：
$$
\text{trace } A = \sum_{k=1}^n a_{k,k}
$$

\textbf{定理 5.1} ~设 $A$ 和 $B$ 是大小分别为 $m \times n$ 和 $n \times m$ 的矩阵（因此两个乘积 $AB$ 和 $BA$ 都有定义）。那么
$$
\text{trace}(AB) = \text{trace}(BA).
$$
我们将此定理的证明留作练习，见下文问题 5.6。证明此定理基本上有两种方法。一种方法是计算 $AB$ 和 $BA$ 的对角线项并比较它们的和。这种方法需要一些处理 $\sum$ 符号中求和的熟练技巧。

如果你不熟悉代数运算，还有另一种方法。我们可以考虑两个线性变换$T$和$T_1$，它们作用于 $M_{n \times m}$ 到 $\mathbb{F} = \mathbb{F}^1$，由 
$T(X) = \text{trace}(AX)$ 和 $T_1(X) = \text{trace}(XA)$
定义。
为了证明该定理，只需检验 $T=T_1$ 即可；当 $X=B$ 时，等式即给出该定理。
%fill

\textbf{练习}~

5.1. 设 $A = \begin{pmatrix} 1 & 2 \\ 3 & 1 \end{pmatrix}$, $B = \begin{pmatrix} 1 & 0 & 2 \\ 3 & 1 & -2 \end{pmatrix}$, $C = \begin{pmatrix} 1 & -2 & 3 \\ -2 & 1 & -1 \end{pmatrix}$, $D = \begin{pmatrix} -2 \\ 2 \\ 1 \end{pmatrix}.$

a) 标记所有有定义的乘积，并给出结果的维数：$AB, BA, ABC, ABD, BC, BC^T$, $B^T C, DC, D^T C^T$。

b) 计算 $AB$, $A(3B + C)$, $B^T A$, $A(BD)$, $(AB)D$。

5.2. 设 $T_\gamma$ 是 $\mathbb{R}^2$ 中绕 $\gamma$ 角旋转的矩阵。通过矩阵乘法验证 $T_\gamma T_{-\gamma} = T_{-\gamma} T_\gamma = I$。

5.3. 乘以两个旋转矩阵 $T_\alpha$ 和 $T_\beta$（这是乘法是交换的罕见情况，即 $T_\alpha T_\beta = T_\beta T_\alpha$，所以顺序不重要）。从中推导出 $\sin(\alpha + \beta)$ 和 $\cos(\alpha + \beta)$ 的公式。

5.4. 找到 $\mathbb{R}^2$ 中关于直线 $x_1 = -2x_2$ 的正交投影矩阵。

提示：$x_1$ 轴上的投影矩阵是什么？

5.5. 找到 $A, B: \mathbb{R}^2 \to \mathbb{R}^2$ 的线性变换，使得 $AB = 0$ 但 $BA \neq 0$。

5.6. 证明定理 5.1，即证明 $\text{trace}(AB) = \text{trace}(BA)$。

5.7. 构建一个非零矩阵 $A$ 使得 $A^2 = 0$。

5.8. 找到直线 $y = -2x/3$ 的反射矩阵。执行所有乘法。


\section{可逆变换与矩阵~同构}

\textbf{6.1. 恒等变换与单位矩阵}

在所有线性变换中，有一个特殊的变换，即\textbf{恒等变换}（算子）$I$, $I \mathbf{x} = \mathbf{x}$, $\forall \mathbf{x}$。

准确地说，存在无数个恒等变换：对于任何向量空间 $V$，存在恒等变换 $I = I_V: V \to V$, $I_V \mathbf{x} = \mathbf{x}$, $\forall \mathbf{x} \in V$。但是，当它不引起混淆时，我们也将使用相同的符号 $I$ 来表示所有恒等操作（变换）。只有当我们想强调变换在哪一个空间中作用时，我们才会使用符号 $I_V$。
\footnote{
符号$E$经常在线性代数教科书中用来表示单位矩阵，但我更喜欢$I$，因为它也用于算子理论。
}

显然，如果 $I: \mathbb{F}^n \to \mathbb{F}^n$ 是 $\mathbb{F}^n$ 中的恒等变换，它的矩阵是
$$
I = I_n = \begin{pmatrix}
1 & 0 & \dots & 0 \\
0 & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & 1
\end{pmatrix}
$$
（主对角线上为 1，其他地方为 0。）当我们要强调矩阵的大小，我们使用符号 $I_n$；否则，我们只使用 $I$。

显然，对于任意线性变换 $A$，等式 $$AI = A,~~~~IA = A$$ 成立（只要乘积有定义）。

\textbf{6.2. 可逆变换}

\textbf{定义}~
设 $A: V \to W$ 是一个线性变换。我们说变换 $A$ 是\textbf{左可逆}的，如果存在一个线性变换 $B: W \to V$ 使得 

$BA = I$（此处 $I = I_V$）。
\\
变换 $A$ 称为\textbf{右可逆}的，如果存在一个线性变换 $C: W \to V$ 使得 

$AC = I$（此处 $I = I_W$）。
\\
变换 $B$ 和 $C$ 分别称为 $A$ 的\textbf{左逆}(left inverse)和\textbf{右逆}(right inverse)。注意，我们没有假定 $B$ 或 $C$ 的唯一性，并且通常左逆和右逆不是唯一的。

\textbf{定义}~线性变换 $A: V \to W$ 称为\textbf{可逆的}(invertible)，如果它既是右可逆又是左可逆。

\textbf{定理 6.1} 如果线性变换 $A: V \to W$ 是可逆的，那么它的左逆和右逆 $B$ 和 $C$ 是唯一并且相等的。

\textbf{推论}
\footnote{
更为常见的是，这个性质被用于对可逆变换的定义。
}
~ 变换 $A: V \to W$ 是可逆的，当且仅当存在一个唯一的线性变换（记作 $A^{-1}$），$A^{-1}: W \to V$，使得 $$A^{-1} A = I_V,~~~~A A^{-1} = I_W.$$

变换 $A^{-1}$ 称为 $A$ 的\textbf{逆}(inverse)。

\textbf{定理 6.1 的证明} ~设 $BA = I$ 且 $AC = I$。那么 
$$BAC = B(AC) = BI = B.$$
另一方面，
$$BAC = (BA)C = IC = C,$$
因此 $B = C$。

假设对于某个变换 $B_1$ 有 $B_1 A = I$。重复上面的推理，用 $B_1$ 而不是 $B$，我们得到 $B_1 = C$。因此左逆 $B$ 是唯一的。 $C$ 的唯一性同理可证。

\textbf{推论}~矩阵被称为\textbf{可逆}（分别地，\textbf{左可逆}，\textbf{右可逆}），如果相应的线性变换是可逆的（分别地，左可逆，右可逆）。

定理 6.1断言，如果存在唯一的矩阵 $A^{-1}$ 使得 $A^{-1} A = I$, $A A^{-1} = I$，那么矩阵 $A$ 是可逆的。这个矩阵 $A^{-1}$ 称为（惊喜！）$A$ 的\textbf{逆}。

\textbf{例子}~

1. 恒等变换（矩阵）是可逆的，$I^{-1} = I$；

2. 旋转 $R_\gamma$ 
$$R_\gamma = \begin{pmatrix} \cos \gamma & -\sin \gamma \\ \sin \gamma & \cos \gamma \end{pmatrix}$$
是可逆的，并且其逆由 $(R_\gamma)^{-1} = R_{-\gamma}$ 给出。这个等式从 $R_\gamma$ 的几何描述中就清楚了，也可以通过矩阵乘法来验证；

3. 列向量 $(1, 1)^T$ 是左可逆但不是右可逆的。可能的左逆之一是行向量 $(1/2, 1/2)$。要证明这个矩阵不是右可逆的，我们只需注意到它有不止一个左逆。\textbf{练习}：描述这个矩阵的所有左逆。

4. 行向量 $(1, 1)$ 是右可逆但不是左可逆的。列向量 $(1/2, 1/2)^T$ 是一个可能的右逆。

\textbf{注释 6.2} 可逆矩阵\textbf{必须}是方阵（待会证明）。而且，如果一个方阵 $A$ 既有左逆又有右逆，那么它就是可逆的。所以，只需检验 $A A^{-1} = I$ 或 $A^{-1} A = I$ 其中一个即可。

这个事实将在后面证明。在此之前，我们不会使用它。我在这里呈现它只是为了阻止学生尝试错误的证明方向。

\textbf{6.2.1. 逆变换的性质}

\textbf{定理 6.3（乘积的逆）}~ 如果线性变换 $A$ 和 $B$ 是可逆的（并且乘积 $AB$ 有定义），那么乘积 $AB$ 也是可逆的，并且 $$(AB)^{-1} = B^{-1} A^{-1}.$$
注意顺序的变化！

\textbf{证明}~ 直接计算表明：$$(AB)(B^{-1} A^{-1}) = A(BB^{-1})A^{-1} = AIA^{-1} = AA^{-1} = I$$
同理
$$(B^{-1} A^{-1})(AB) = B^{-1}(A^{-1} A)B = B^{-1}IB = B^{-1}B = I.$$

\textbf{注释 6.4} ~乘积 $AB$ 的可逆性并不意味着因子 $A$ 和 $B$ 的可逆性（你能想到一个例子吗？）。然而，如果其中一个因子（无论是 $A$ 还是 $B$）以及乘积 $AB$ 都是可逆的，那么第二个因子也是可逆的。

我们将此事实的证明留作练习。

\textbf{定理 6.5（$A^T$ 的逆）}~ 如果矩阵 $A$ 是可逆的，那么 $A^T$ 也是可逆的，并且 $$(A^T)^{-1} = (A^{-1})^T.$$

\textbf{证明}~ 使用 $(AB)^T = B^T A^T$ 我们得到 $$(A^{-1})^T A^T = (AA^{-1})^T = I^T = I,$$
同理 
$$A^T (A^{-1})^T = (A^{-1} A)^T = I^T = I.$$

最后，如果 $A$ 是可逆的，那么 $A^{-1}$ 也是可逆的，$(A^{-1})^{-1} = A$。

所以，让我们总结一下逆的三个主要性质：

1. 如果 $A$ 可逆，那么 $A^{-1}$ 也可逆，$(A^{-1})^{-1} = A$；

2. 如果 $A$ 和 $B$ 可逆且乘积 $AB$ 有定义，那么 $AB$ 可逆且 $(AB)^{-1} = B^{-1} A^{-1}$。

3. 如果 $A$ 可逆，那么 $A^T$ 也可逆且 $(A^T)^{-1} = (A^{-1})^T$。

\textbf{6.3. 同构~同构空间}

可逆线性变换 $A: V \to W$ 称为\textbf{同构}(isomorphism)。我们这里没有引入任何新东西，这只是我们已经研究过的对象的另一个名称。

两个向量空间 $V$ 和 $W$ 称为\textbf{同构}（记作 $V \cong W$），如果存在一个同构 $A: V \to W$。

同构空间可以被视为同一个空间的不同的表示，这意味着所有涉及向量空间运算的性质和构造在同构下都被保留。

下面的定理说明了这一点。

\textbf{定理 6.6} ~设 $A: V \to W$ 是一个同构，设 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是 $V$ 中的一个基。那么向量系统 $A \mathbf{v}_1, A \mathbf{v}_2, \dots, A \mathbf{v}_n$ 是 $W$ 中的一个基。

我们把这个定理的证明留作练习。

\textbf{注释}~ 在上面的定理中，我们可以用“线性无关”、“生成”或“线性相关”来替换“基”——所有这些性质在同构下都被保留。

\textbf{注释}~ 如果 $A$ 是同构，那么 $A^{-1}$ 也是同构。因此，在上面的定理中，我们可以说 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是基当且仅当 $A \mathbf{v}_1, A \mathbf{v}_2, \dots, A \mathbf{v}_n$ 是基。

定理 6.6 的逆命题也成立。

\textbf{定理 6.7} 设 $A: V \to W$ 是线性映射，设 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 和 $\mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_n$ 分别是 $V$ 和 $W$ 中的基。如果 $A \mathbf{v}_k = \mathbf{w}_k$, $k = 1, 2, \dots, n$，那么 $A$ 是一个同构。

\textbf{证明}~ 定义逆变换 $A^{-1}$ 为 $A^{-1} \mathbf{w}_k = \mathbf{v}_k$, $k = 1, 2, \dots, n$（正如我们所知，线性变换由其在基上的值定义）。

\textbf{例子}~

1. $A: \mathbb{F}^{n+1} \to \mathbb{P}^\mathbb{F}_n$ （$\mathbb{P}^\mathbb{F}_n$ 是 $\sum_{k=0}^n a_k t^k$, $\alpha_k \in \mathbb{F}$ 的形式的 $n$ 次多项式集）定义为 
$$A \mathbf{e}_1 = 1, A \mathbf{e}_2 = t, \dots, A \mathbf{e}_n = t^{n-1}, A \mathbf{e}_{n+1} = t^n.$$

根据定理 6.7，$A$ 是同构，所以 $\mathbb{P}^\mathbb{F}_n \cong \mathbb{F}^{n+1}$。

2. 设 $V$ 是一个向量空间（在 $\mathbb{F}$ 上）有一个基 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$。定义变换 $A: \mathbb{F}^n \to V$ 为 
$$A \mathbf{e}_k = \mathbf{v}_k,~~~~k = 1, 2, \dots, n,$$
其中 $\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n$ 是 $\mathbb{F}^n$ 的标准基。根据定理 6.7，$A$ 是同构，所以 $V \cong \mathbb{F}^n$。

3. $M^\mathbb{F}_{2 \times 3}$ 空间（$\mathbb{F}$ 中的 $2 \times 3$ 矩阵）同构于 $\mathbb{R}^6$。

4. 更一般地，$M^\mathbb{F}_{m \times n} \cong \mathbb{F}^{m \cdot n}$。

\textbf{6.4. 可逆性与方程}

\textbf{定理 6.8} ~设 $A: X \to Y$ 是一个线性变换。那么 $A$ 是可逆的，当且仅当对于任意右侧 $\mathbf{b} \in Y$，方程 $$A \mathbf{x} = \mathbf{b}$$ 有唯一解 $\mathbf{x} \in X$。

\textbf{证明}~ 假设 $A$ 是可逆的。那么 $\mathbf{x} = A^{-1} \mathbf{b}$ 解决了方程 $A \mathbf{x} = \mathbf{b}$。为了证明解是唯一的，假设对于另一个向量 $\mathbf{x}_1 \in X$, 
$$A \mathbf{x}_1 = \mathbf{b}.$$
将这个恒等式从左边乘以 $A^{-1}$，我们得到 $$A^{-1} A \mathbf{x}_1 = A^{-1} \mathbf{b},$$
因此 $\mathbf{x}_1 = A^{-1} \mathbf{b} = \mathbf{x}$。注意，这里使用了两个恒等式，$AA^{-1} = I$ 和 $A^{-1} A = I$。

现在假设方程 $A \mathbf{x} = \mathbf{b}$ 对于任意 $\mathbf{b} \in Y$ 都有唯一解 $\mathbf{x} \in X$。让我们用 $\mathbf{y}$ 代替 $\mathbf{b}$。我们知道，对于给定的 $\mathbf{y} \in Y$，方程 
$$A \mathbf{x} = \mathbf{y}$$
有唯一解 $\mathbf{x} \in X$。让我们称这个解为 $B(\mathbf{y})$。

注意，$B(\mathbf{y})$ 对所有 $\mathbf{y} \in Y$ 都有定义，因此我们定义了一个变换 $B: Y \to X$。

让我们检验 $B$ 是否是线性变换。我们需要证明 $B(\alpha \mathbf{y}_1 + \beta \mathbf{y}_2) = \alpha B(\mathbf{y}_1) + \beta B(\mathbf{y}_2)$。设 $\mathbf{x}_k := B(\mathbf{y}_k)$, $k = 1, 2$，即
$A \mathbf{x}_k = \mathbf{y}_k$, $k = 1, 2$。那么
$$A(\alpha \mathbf{x}_1 + \beta \mathbf{x}_2) = \alpha A \mathbf{x}_1 + \beta A \mathbf{x}_2 = \alpha \mathbf{y}_1 + \beta \mathbf{y}_2,$$
这意味着
$$B(\alpha \mathbf{y}_1 + \beta \mathbf{y}_2) = \alpha B(\mathbf{y}_1) + \beta B(\mathbf{y}_2).$$

最后，让我们证明 $B$ 确实是 $A$ 的逆。取 $\mathbf{x} \in X$，设 $\mathbf{y} = A \mathbf{x}$，所以根据 $B$ 的定义，我们有 $\mathbf{x} = B \mathbf{y}$。那么对于所有 $\mathbf{x} \in X$, 
$$BA \mathbf{x} = B \mathbf{y} = \mathbf{x},$$
所以 $BA = I$。类似地，对于任意 $\mathbf{y} \in Y$，设 $\mathbf{x} = B \mathbf{y}$，所以 $\mathbf{y} = A \mathbf{x}$。那么对于所有 $\mathbf{y} \in Y$,
$$AB \mathbf{y} = A \mathbf{x} = \mathbf{y},$$
所以 $AB = I$。

回忆基的定义，我们得到以下定理 6.6 和 6.7 的推论。

\textbf{推论 6.9} 一个 $m \times n$ 矩阵可逆，当且仅当它的列在 $\mathbb{F}^m$ 中构成一个基。

\textbf{练习}~

6.1. 证明，如果 $A: V \to W$ 是一个同构（即一个可逆线性变换），并且 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是 $V$ 中的一个基，那么 $A \mathbf{v}_1, A \mathbf{v}_2, \dots, A \mathbf{v}_n$ 是 $W$ 中的一个基。

6.2. 找到行向量 $A = (1, 1)$ 的所有右逆。由此得出 $A$ 行向量不是左可逆的。

6.3. 找到列向量 $(1, 2, 3)^T$ 的所有左逆。

6.4. 列向量 $(1, 2, 3)^T$ 是右可逆的吗？请给出理由。

6.5. 找到两个矩阵 $A$ 和 $B$，使得 $AB$ 是可逆的，但 $A$ 和 $B$ 都不是可逆的。提示：$A$ 和 $B$ 的方阵将不起作用。注意：即使 $AB$ 是 $1 \times 1$ 矩阵（标量），也很容易构造这样的 $A$ 和 $B$。但是你能得到 $2 \times 2$ 矩阵 $AB$ 吗？$3 \times 3$？$n \times n$？

6.6. 假设乘积 $AB$ 是可逆的。证明 $A$ 是右可逆的，$B$ 是左可逆的。提示：你可以直接写出右逆和左逆的公式。

6.7. 假设 $A$ 和 $AB$ 都是可逆的（假设乘积 $AB$ 有定义）。证明 $B$ 是可逆的。

6.8. 设 $A$ 是一个 $n \times n$ 矩阵。证明如果 $A^2 = 0$ 则 $A$ 不可逆。

6.9. 假设 $AB = 0$ 对某个非零矩阵 $B$ 成立。 $A$ 能是可逆的吗？请给出理由。

6.10. 在 $\mathbb{F}^5$ 中找到代表如下变换的矩阵 $T_1$ 和 $T_2$：$T_1$ 交换向量 $x$ 的坐标 $x_2$ 和 $x_4$，而 $T_2$ 只是将 $x_4$ 的 $a$ 倍加到坐标 $x_2$ 上，而不改变其他坐标，即
$$T_1 \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} = \begin{pmatrix} x_1 \\ x_4 \\ x_3 \\ x_2 \\ x_5 \end{pmatrix},~~~~T_2 \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} = \begin{pmatrix} x_1 \\ x_2 + ax_4 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix};$$
这里 $a$ 是某个固定数。

证明 $T_1$ 和 $T_2$ 是可逆变换，并写出它们的逆矩阵。\textbf{提示}：先描述逆变换，然后再求它的矩阵，可能比猜测（或计算）$T_1$, $T_2$ 的逆矩阵要简单。

6.11. 找到 $\mathbb{R}^3$ 中绕向量 $(1, 2, 3)^T$ 轴绕 $\alpha$ 角旋转的变换的矩阵。我们假设旋转是从向量的尖端看向原点时逆时针旋转的。

您可以将答案表示为几个矩阵的乘积：您不必执行乘法。

6.12. 给出 $2 \times 2$ 矩阵的例子，使得：

a) $A+B$ 不可逆，尽管 $A$ 和 $B$ 都可逆；

b) $A+B$ 可逆，尽管 $A$ 和 $B$ 都不可逆；

c) $A$, $B$ 和 $A+B$ 都可逆。

6.13. 设 $A$ 是一个可逆的对称矩阵 ($A^T = A$)。$A$ 的逆是否对称？请给出理由。


\section{子空间}

向量空间 $V$ 的一个\textbf{子空间}(subspace)是 $V$ 的一个非空子集 $V_0 \subset V$，它对向量加法和标量乘法是封闭的，即

1. 如果 $\mathbf{v} \in V_0$，则 $\alpha \mathbf{v} \in V_0$ 对所有标量 $\alpha$；

2. 对于任何 $\mathbf{u}, \mathbf{v} \in V_0$，它们的和 $\mathbf{u} + \mathbf{v} \in V_0$；

再次，条件 1 和 2 可以被以下一个条件替换：

$\alpha \mathbf{u} + \beta \mathbf{v} \in V_0$ 对所有 $\mathbf{u}, \mathbf{v} \in V_0$，以及所有标量 $\alpha, \beta$。

请注意，子空间 $V_0 \subset V$ 连同从 $V$ 继承的运算（向量加法和标量乘法），是一个向量空间。实际上，由于 $V$ 非空，它至少包含 1 个向量 $\mathbf{v}$，并且由于 $\mathbf{0} = 0\mathbf{v}$，所以上述条件 1.意味着零向量 $\mathbf{0}$ 在 $V$ 中。此外，对于任何 $\mathbf{v} \in V$，它的加法逆元 $-\mathbf{v}$ 由 $-\mathbf{v} = (-1)\mathbf{v}$ 给出，所以再次根据性质 1，$-\mathbf{v} \in V$。向量空间的其余公理之所以成立，是因为所有运算都源于向量空间 $V$。唯一可能出错的是某个运算的结果不属于 $V_0$。但子空间的定义禁止了这一点！

现在我们来看一些例子：

1. 空间 $V$ 的\textbf{平凡}(trivial)子空间，即 $V$ 本身和 $\{\mathbf{0}\}$（仅包含零向量的子空间）。注意，空集 $\emptyset$ 不是向量空间，因为它不包含零向量，所以它不是子空间。

任何线性变换 $A: V \to W$ 都可以关联以下两个子空间：

2. $A$ 的\textbf{零空间}（null space）或\textbf{核}（kernel），记作 $\text{Null } A$ 或 $\text{Ker } A$，由所有满足 $A \mathbf{v} = \mathbf{0}$ 的向量 $\mathbf{v} \in V$ 组成。

3. \textbf{像空间}（range）$\text{Ran } A$ 定义为所有可以表示为 $\mathbf{w} = A \mathbf{v}$ 的向量 $\mathbf{w} \in W$ 的集合，其中某个 $\mathbf{v} \in V$。

如果 $A$ 是一个矩阵，即 $A: \mathbb{F}^m \to \mathbb{F}^n$，那么回忆矩阵-向量乘法的“按列坐标规则”，我们可以看到任何向量 $\mathbf{w} \in \text{Ran } A$ 都可以表示为 $A$ 的列的线性组合。这解释了为什么\textbf{列空间}（表示为 $\text{Col } A$）这个术语经常用来表示矩阵的像空间。因此，对于矩阵 $A$，符号 $\text{Col } A$ 通常用于代替 $\text{Ran } A$。

还有最后一个例子。

4. 给定向量系统 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r \in V$，它的\textbf{线性张成}(linear span)（有时简单称为\textbf{张成}）$\mathcal{L}\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r\}$ 是 $V$ 中所有可以表示为向量 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r$ 的线性组合 $\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_r \mathbf{v}_r$ 的向量的集合。符号 $\text{span}\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r\}$ 也用于代替 $\mathcal{L}\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_r\}$。

可以很容易地检验出在所有这些例子中我们确实得到了子空间。我们将此作为读者的练习。其中一些陈述将在本书后面证明。


\textbf{练习}~

7.1. 设 $X$ 和 $Y$ 是向量空间 $V$ 的子空间。证明 $X \cap Y$ 是 $V$ 的子空间。

7.2. 设 $V$ 是一个向量空间。对于 $X, Y \subset V$，和 $X+Y$ 是所有可以表示为 $\mathbf{v} = \mathbf{x} + \mathbf{y}$, $\mathbf{x} \in X$, $\mathbf{y} \in Y$ 的向量的集合。证明如果 $X$ 和 $Y$ 是 $V$ 的子空间，那么 $X+Y$ 也是子空间。

7.3. 设 $X$ 是向量空间 $V$ 的子空间，设 $\mathbf{v} \in V$, $\mathbf{v} \notin X$。证明如果 $\mathbf{x} \in X$，则 $\mathbf{x} + \mathbf{v} \notin X$。

7.4. 设 $X$ 和 $Y$ 是向量空间 $V$ 的子空间。利用上一道练习，证明 $X \cup Y$ 是子空间当且仅当 $X \subset Y$ 或 $Y \subset X$。

7.5. 包含所有上三角矩阵（$a_{j,k} = 0$ $\forall j > k$）和所有对称矩阵（$A = A^T$）的 $4 \times 4$ 矩阵空间中最小的子空间是什么？包含在这两个子空间中的最大子空间是什么？


\section{应用于计算机图形学}

在本节中，我们将介绍一些线性代数在计算机图形学中的应用。我们将不深入细节，只是解释一些思想。特别是我们将解释为什么对三维图像的操作会简化为 $4 \times 4$ 矩阵的乘法。

\textbf{8.1.二维操作}

$x-y$ 平面（更准确地说，平面上的一个矩形）是计算机显示器的一个很好的模型。显示器上的任何对象都表示为\textbf{像素}(pixel)的集合，每个像素被分配一个特定的颜色。每个像素的位置由其列和行确定，它们充当平面上的 $x$ 和 $y$ 坐标。所以，具有 $x-y$ 坐标的平面上的矩形是计算机屏幕的一个好模型：而图形对象只是点的集合。

\textbf{注释}~ 有两种类型的图形对象：位图对象，其中描述了对象的每个像素，以及矢量对象，其中我们只描述\textbf{关键点}(critical points)，然后图形引擎将它们连接起来以重建对象。照片是位图对象的一个好例子：它的每个像素都被描述。位图对象可能包含很多点，所以处理位图需要大量的计算能力。任何使用过位图处理程序（如 Adobe Photoshop）的人都知道，你需要一台相当强大的计算机，即使是现代和强大的计算机，操作也可能需要一些时间。

这就是为什么出现在计算机屏幕上的大多数对象都是矢量对象的原因：计算机只需要记住关键点。例如，要描述一个多边形，你只需要给出其顶点的坐标，以及哪些顶点连接到哪个顶点。当然，并非屏幕上的所有对象都可以表示为多边形，有些对象，如字母，具有平滑弯曲的边界。但是，存在标准方法可以允许我们通过一组点绘制平滑曲线，例如 Bezier 样条，在 PostScript 和 Adobe PDF（以及许多其他格式）中使用。

无论如何，这是另一本书的主题，我们在这里不讨论它。对我们来说，一个图形对象将是一组点（无论是线框模型(wireframe model)还是位图），我们想展示如何对这些对象执行一些操作。

最简单的变换是\textbf{平移}(translation)（移动）(shift)，其中每个点（向量）$\mathbf{v}$ 被平移 $\mathbf{a}$，即向量 $\mathbf{v}$ 被替换为 $\mathbf{v} + \mathbf{a}$（表示 $ \mathbf{v} \mapsto \mathbf{v} + \mathbf{a} $）。向量加法非常适合计算机，因此平移很容易实现。注意，平移不是线性变换（如果 $\mathbf{a} \neq 0$）：虽然它保留了直线，但它不保留 $\mathbf{0}$。

计算机图形学中使用的所有其他变换都是线性的。第一个想到的就是旋转。绕原点 $\mathbf{0}$ 的 $\gamma$ 角旋转由我们上面讨论过的旋转矩阵 $R_\gamma$ 给出， $$R_\gamma = \begin{pmatrix} \cos \gamma & -\sin \gamma \\ \sin \gamma & \cos \gamma \end{pmatrix}.$$
如果我们想绕一个点 $\mathbf{a}$ 旋转，我们首先需要平移图像 $-\mathbf{a}$，将点 $\mathbf{a}$ 移动到 $\mathbf{0}$，然后绕 $\mathbf{0}$ 旋转（乘以 $R_\gamma$），然后将所有内容平移回 $\mathbf{a}$。

另一个非常有用的变换是\textbf{缩放}(scaling)，由矩阵 
$$\begin{pmatrix} a & 0 \\ 0 & b \end{pmatrix}$$
给出，$a, b \ge 0$。如果 $a=b$ 它是\textbf{均匀缩放}(uniform scaling)，它放大（缩小）对象，保持其形状。如果 $a \neq b$ 则 $x$ 和 $y$ 坐标缩放不同；对象变得“更高”或“更宽”。

另一个经常使用的变换是\textbf{反射}(reflection)：例如矩阵 
$$\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$$
定义了关于 $x$ 轴的反射。

我们将在本书后面证明， $\mathbb{R}^2$ 中的任何线性变换都可以表示为缩放、旋转和反射的复合。然而，有时考虑一些不同的变换，如\textbf{剪切变换}(shear transformation)，由矩阵 
$$\begin{pmatrix} 1 & \tan \phi \\ 0 & 1 \end{pmatrix}$$
给出。这个变换使得所有对象倾斜，水平线保持水平，但垂直线变成与水平线成 $\phi$ 角的倾斜线。

\textbf{8.2. 三维图形}

三维图形更复杂。首先我们需要能够操作三维物体，然后需要将其表示在二维平面（显示器）上。

三维物体的操作非常直接，我们有相同的基本变换：平移、平面反射、缩放、旋转。这些变换的矩阵与其二维对应物的矩阵非常相似。例如，矩阵
$$
\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \end{pmatrix}, \quad \begin{pmatrix} a & 0 & 0 \\ 0 & b & 0 \\ 0 & 0 & c \end{pmatrix}, \quad \begin{pmatrix} \cos \gamma & -\sin \gamma & 0 \\ \sin \gamma & \cos \gamma & 0 \\ 0 & 0 & 1 \end{pmatrix}
$$
分别代表了关于 $x-y$ 平面的反射、缩放和绕 $z$ 轴的旋转。

请注意，上述旋转本质上是二维变换，它不改变 $z$ 坐标。类似地，可以为绕 $x$ 轴和绕 $y$ 轴的其他 2 个基本旋转写出矩阵。后面将表明，任意轴上的旋转可以表示为基本旋转的复合。因此，我们知道如何操作三维物体。

现在让我们讨论如何将三维物体表示在二维平面上。最简单的方法是将其投影到平面，比如 $x-y$ 平面。要执行这种投影，只需将 $z$ 坐标替换为 0，这个投影(projection)的矩阵是
$$
\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}.
$$
这种方法通常用于技术插图。旋转一个物体并对其进行投影相当于从不同的点看它。然而，这种方法没有给出非常逼真的图像，因为它没有考虑透视，即远处的物体看起来更小的事实。

为了获得更逼真的图像，我们需要使用所谓的\textbf{透视投影}(perspective projection)。要定义透视投影，我们需要选择一个点（投影中心或焦点）和一个要投影到的平面。然后，$\mathbb{R}^3$ 中的每个点被投影到一个平面上的点，使得该点、它的像以及\textbf{投影中心}(center of the projection)位于同一条线上，见图 2。

\red{这里放图片}

图 2. 透视投影到 $x-y$ 平面：$F$ 是投影中心（焦点）

这正是相机的工作方式，也是我们眼睛工作方式的一个合理初步近似。

让我们得到投影的公式。假设焦点是 $(0, 0, d)^T$，并且我们投影到 $x-y$ 平面，见图 3 a）。考虑一个点 $\mathbf{v} = (x, y, z)^T$，让 $\mathbf{v}^* = (x^*, y^*, 0)^T$ 是它的投影。分析相似三角形，见图 3 b），我们得到

\red{Put the figure 3 here.}

图 3. 找到点 $(x, y, z)^T$ 的透视投影的坐标 $x^*, y^*$。

$$\frac{x^*}{d} = \frac{x}{d-z},$$
所以 
$$x^* = \frac{xd}{d-z} = \frac{x}{1 - z/d},$$
类似地 
$$y^* = \frac{y}{1 - z/d}.$$
注意，这个公式在 $z > d$ 和 $z < 0$ 时也有效：你可以画出相应的相似三角形来验证它。


因此，透视投影将点 $(x, y, z)^T$ 映射到点 $(\frac{x}{1-z/d}, \frac{y}{1-z/d}, 0)^T.$

这个变换肯定不是线性的（由于分母中的 $z$）。然而，通过引入所谓的\textbf{齐次坐标}(homogeneous coordinates)，仍然可以将其表示为线性变换。

在齐次坐标中，$\mathbb{R}^3$ 中的每个点都由 4 个坐标表示，最后一个（第四个）坐标起到了缩放系数的作用。因此，要从 $\mathbf{v} = (x, y, z)^T$ 的齐次坐标$\mathbf{v} = (x_1, x_2, x_3, x_4) ^T$得到其通常的三维坐标，需要将所有项除以最后一个坐标 $x_4$，然后取前 3 个坐标
\footnote{
如果我们对齐次坐标下一个在$\mathbb{R}^2$中的点乘上一个非零标量，我们没有改变这个点。换而言之，在齐次坐标下，一个在$\mathbb{R}^3$中的点被表示为在$\mathbb{R}^4$中有一行为0的点。
}
（如果 $x_4 = 0$，则此方法不适用，因此我们假设 $x_4 = 0$ 的情况对应于无穷远点）。

因为在齐次坐标中，向量$\mathbf{v}^*$可以被表示为$x, y, 0, 1-z/d)^T$,因此，在齐次坐标中，透视投影是一个线性变换：
$$
\begin{pmatrix} x \\ y \\ 0 \\ 1 - z/d \end{pmatrix} = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & -1/d & 1 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \\ 1 \end{pmatrix}.
$$
请注意，在齐次坐标中，平移也是一个线性变换：
$$
\begin{pmatrix} x + a_1 \\ y + a_2 \\ z + a_3 \\ 1 \end{pmatrix} = \begin{pmatrix} 1 & 0 & 0 & a_1 \\ 0 & 1 & 0 & a_2 \\ 0 & 0 & 1 & a_3 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \\ 1 \end{pmatrix}.
$$

但是，如果投影中心不是 $(0, 0, d)^T$ 这样的点，而是任意点 $(d_1, d_2, d_3)^T$ 呢？
那么我们首先需要应用 $-( d_1 , d_2 , 0 )^T$ 的平移来将中心移到 $(0, 0, d_3)^T$，同时保持 $x-y$ 平面不变，应用投影，然后通过 $(d_1, d_2, 0)^T$ 的平移将所有内容移回。类似地，如果投影平面不是 $x-y$ 平面，我们通过使用旋转和平移将其移到 $x-y$ 平面，等等。

所有这些操作只是 $4 \times 4$ 矩阵的乘法。这就解释了为什么现代图形卡将 $4 \times 4$ 矩阵运算嵌入到处理器中。

当然，这里我们只触及了三维图形背后的数学，还有更多内容。例如，如何确定物体的哪些部分可见，哪些部分被隐藏，如何制作逼真的照明、阴影等等。


\textbf{练习}~

8.1. $\mathbb{R}^3$ 中齐次坐标为 $(10, 20, 30, 5)^T$ 的向量是什么？

8.2. 证明 $\gamma$ 角的旋转可以表示为两次剪切-缩放变换的复合 
$$T_1 = \begin{pmatrix} 1 & 0 \\ \sin \gamma & \cos \gamma \end{pmatrix} ，~~~~~~~~T_2 = \begin{pmatrix} \sec \gamma & -\tan \gamma \\ 0 & 1 \end{pmatrix}.$$
应该以什么顺序进行变换？

8.3. 一个2维向量乘以一个任意的$2\times 2$矩阵通常需要4次乘法。

假设$2 \times 1000$ 矩阵 $D$ 包含 $\mathbb{R}^2$ 中 1000 个点的坐标。使用两个任意 $2 \times 2$ 矩阵 $A$ 和 $B$ 来变换这些点需要多少次乘法？比较两种可能性，$A(BD)$ 和 $(AB)D$。

8.4. 写一个 $4 \times 4$ 矩阵，执行透视投影到 $x-y$ 平面，其中心为 $(d_1, d_2, d_3)^T$。

8.5. 变换 $T$ 在 $\mathbb{R}^3$ 中是 $x-y$ 平面中直线 $y = 2x+3$ 绕 $\gamma$ 角的旋转。写出与此变换对应的 $4 \times 4$ 矩阵。你可以将结果表示为矩阵的乘积。


