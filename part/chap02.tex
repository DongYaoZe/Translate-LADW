

\chapter{第二章~~线性方程组}

\section{1. 线性方程组的不同表示}

关于线性方程组，或者简而言之\textbf{线性系统}(linear system)，存在几种观点。第一种，朴素的观点是，它仅仅是 $n$ 个未知数 $x_1, x_2, \dots, x_n$ 的 $m$ 个线性方程的集合：
$$
\begin{cases}
a_{1,1} x_1 + a_{1,2} x_2 + \dots + a_{1,n} x_n = b_1 \\
a_{2,1} x_1 + a_{2,2} x_2 + \dots + a_{2,n} x_n = b_2 \\
\cdots \\
a_{m,1} x_1 + a_{m,2} x_2 + \dots + a_{m,n} x_n = b_m.
\end{cases}
$$

求解该系统是指找到所有满足这 $m$ 个方程的 $n$ 元数组 $x_1, x_2, \dots, x_n$. ~

如果我们记 $\xx := (x_1, x_2, \dots, x_n)^T \in \FF^n$, $\bb = (b_1, b_2, \dots, b_m)^T \in \FF^m$，以及
$$
A = \begin{pmatrix}
a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
a_{2,1} & a_{2,2} & \dots & a_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \dots & a_{m,n}
\end{pmatrix},
$$
那么上述线性系统可以用\textbf{矩阵形式}(matrix form)（作为\textbf{矩阵-向量方程}(matrix-vector equation)）写成 
$$A \xx = \bb.$$
求解上述方程是指找到所有满足 $A \xx = \bb$ 的向量 $\xx \in \FF^n$. ~

最后，回忆矩阵-向量乘法的“按列坐标规则”，我们可以将系统写成一个\textbf{向量方程}(vector equation)：
$$
x_1 \aaa_1 + x_2 \aaa_2 + \dots + x_n \aaa_n = \bb,
$$
其中 $\aaa_k$ 是矩阵 $A$ 的第 $k$ 列，$\aaa_k = (a_{1,k}, a_{2,k}, \dots, a_{m,k})^T$, $k = 1, 2, \dots, n$. ~

注意，这三个例子本质上只是同一个数学对象的不同表示。

在解释如何求解线性系统之前，让我们注意到，无论我们如何称呼未知数，例如 $x_k$, $y_k$ ，或其他名称，这都不重要。因此，所有求解系统所需的信息都包含在矩阵 $A$ 中，该矩阵称为系统的\textbf{系数矩阵}（coefficient matrix），以及向量（处在右侧的）$\bb$. ~因此，我们所需的所有信息都包含在以下矩阵中：
$$
\begin{pmatrix}
a_{1,1} & a_{1,2} & \dots & a_{1,n} & | & b_1 \\
a_{2,1} & a_{2,2} & \dots & a_{2,n} & | & b_2 \\
\vdots & \vdots & \ddots & \vdots & | & \vdots \\
a_{m,1} & a_{m,2} & \dots & a_{m,n} & | & b_m
\end{pmatrix}.
$$
该矩阵是通过将列 $b$ 连接到矩阵 $A$ 上形成的。这个矩阵称为系统的\textbf{增广矩阵}（augmented matrix）。我们通常会放一条垂直线来分隔 $A$ 和 $\bb$，以区分增广矩阵和系数矩阵。


\section{2. 线性方程组的求解~~阶梯形与简化阶梯形}

线性系统可以通过\textbf{高斯-若尔当消元法}(Gauss-Jordan elimination)（有时称为\textbf{行约简}(row reduction)）求解。通过对系统增广矩阵的行（即方程）执行运算，我们将它简化为一种简单的形式，即所谓的\textbf{阶梯形}(echelon form)。当系统处于阶梯形时，我们可以轻松地写出解。

\subsection{2.1. 行运算}

我们使用的行运算有三种类型：

1. 行交换：交换矩阵的任意两行；

2. 缩放：用一个非零标量 $a$ 乘以某一行；

3. 行替换：用第 $j$ 行的常数倍加上第 $k$ 行来整体替换第 $k$ 行；其余行保持不变。

可以清楚地看出，运算 1 和 2 不会改变系统的解集；它们基本上不改变系统。

至于运算 3，可以很容易地看出它不会丢失解。也就是说，设一个“新”系统是通过类型 3 的行运算从“旧”系统中得到的，那么“旧”系统的任何解也都是“新”系统的解。

为了证明我们没有得到任何额外的东西，即“新”系统的任何解也是“旧”系统的解，我们只需注意到类型 3 的行运算是\textbf{可逆}的，也就是说，“旧”系统也可以通过应用类型 3 的行运算从“新”系统中获得。（你能说出是哪一种吗？）

\subsubsection{2.1.1. 行运算与初等矩阵的乘法}

还有另一种更“高级”的解释来说明为什么上述行运算是合法的。也就是说，每个行运算都相当于从左边乘以一个特殊的初等矩阵。

也即，乘以矩阵
\[
\begin{array}{@{}c@{\,}c}
    & \begin{array}{@{}c@{\hspace{2.5em}}c@{}}
        j & k
      \end{array}
    \\
    \begin{array}{@{}c@{}} \\ j \\ \\ k \\ \\ \end{array}
    &
    \left(
    \begin{array}{ccccccc}
        1      &          & \vdots   &        & \vdots   &        &  \oo\\
               & \ddots   & \vdots   &        & \vdots   &        &            \\
        \cdots & \cdots   & 0        & \cdots & 1        & \cdots &            \\
               &          & \vdots   & \ddots & \vdots   &        &            \\
        \cdots & \cdots   & 1        & \cdots & 0        & \cdots &            \\
               &          &          &        &          & \ddots &            \\
        \oo &        &          &        &        &        & 1
    \end{array}
    \right)
\end{array}
\]
其中第 $j$ 行和第 $k$ 行可以看作是对单位矩阵 $I$ 的第 $j$ 行和第 $k$ 行的交换。
乘以这个矩阵
\[
\begin{array}{@{}c@{\,}c}
    % 行标签
    \begin{array}{@{}c@{}} \\ \\ k \\ \\ \end{array}
    &
    % 矩阵主体
    \left(
    \begin{array}{ccccccc}
        1      &    0    &    &   \vdots     &          &          & \oo \\
       0       & \ddots   &    &    \vdots    &          &          &            \\
               &          & 1        & 0      &          &          &            \\
        \cdots & \cdots   & 0        & a      & 0        & \cdots   &            \\
               &          &          & 0      & 1        &          &            \\
               &          &          &        &    &   \ddots &   \vdots        \\
        \oo &      &          &       & &  \cdots  & 1        
    \end{array}
    \right)
\end{array}
\]
其中第 $k$ 行乘以 $a$. ~最后，乘以这个矩阵

% 这是一个将第 j 行的 a 倍加到第 k 行的矩阵
\[
\begin{array}{@{}c@{\,}c}
    % 行标签
    \begin{array}{@{}c@{}} \\ j \\ \\ k \\ \\ \end{array}
    &
    % 矩阵主体
    \left(
    \begin{array}{ccccccc}
        1      &          & \vdots   &        & \vdots   &        & \oo \\
               & \ddots   & \vdots   &        & \vdots   &        &            \\
        \cdots & \cdots   & 1        & \cdots & 0        & \cdots &            \\
               &          & \vdots   & \ddots & \vdots   &        &            \\
        \cdots & \cdots   & a        & \cdots & 1        &        &            \\
               &          &          &        &          & \ddots &            \\
        \oo &        &          &        &        &        & 1
    \end{array}
    \right)
\end{array}
\]
其中第 $k$ 行加上第 $j$ 行的 $a$ 倍，而其他行不变。

如果要看到这些初等矩阵的乘法确实是按照预期执行的，你可以简单地看看它们如何作用于向量（列）。

注意，所有这些矩阵都是可逆的（与行运算的可逆性进行比较）。第一个矩阵的逆是它本身。要得到第二个矩阵的逆，只需将 $a$ 替换为 $1/a$. ~最后，第三个矩阵的逆是通过将 $a$ 替换为 $-a$ 来获得的。要看到逆确实是这样获得的，我们（再次）可以简单地验证它们如何作用于列。

因此，对系统 $A \xx = \bb$ 的增广矩阵执行行运算，相当于将系统（从左边）乘以一个特殊的初等矩阵 $E$. ~将等式 $A \xx = \bb$ 从左边乘以 $E$，我们得到 $$A \xx = \bb$$
的任何解，也是 $$EA \xx = E \bb$$
的解。将这个方程从左边乘以 $E^{-1}$，我们得到它的任何解也是 $$E^{-1} EA \xx = E^{-1} E \bb$$
的解，也就是原始方程 $A \xx = \bb$. ~所以，行运算不改变系统的解集。

\subsection{2.2. 行约简}

行约简的主步骤包括三个子步骤：

1. 找到矩阵中最左边的非零列；

2. 通过使用类型1，行运算，（必要时进行行交换），确保该列的第一个（最上面的）项非零。这个项将被称为\textbf{主元项}（pivot entry）或简称为\textbf{主元}（pivot）；

3. 通过从第 2、3、...、m 行减去第一行的适当倍数来“消去”(Kill)主元下的所有非零项（即让其为 0）。

我们将主步骤应用于一个矩阵，然后将第一行单独处理，并对第 2、...、m 行应用主步骤，然后对第 3、...、m 行应用主步骤，等等。

需要记住的一点是，在将一行的适当倍数减去此行所有下面的行（步骤 3）之后，我们要就需将该行抛诸脑后，不再操作它，甚至不与其他行交换。

在应用主步骤有限次（最多 $m$ 次）之后，我们得到所谓的矩阵的\textbf{阶梯形}。

\subsubsection{2.2.1. 行约简的一个例子}

让我们考虑以下线性系统：
$$
\begin{cases}
x_1 + 2x_2 + 3x_3 = 1 \\
3x_1 + 2x_2 + x_3 = 7 \\
2x_1 + x_2 + 2x_3 = 1
\end{cases}
$$
系统的增广矩阵是
$$
\begin{pmatrix} 1 & 2 & 3 & | & 1 \\ 3 & 2 & 1 & | & 7 \\ 2 & 1 & 2 & | & 1 \end{pmatrix}
$$
从第二行减去第一行的3倍，并从第三行减去第一行的2倍，我们得到：
$$
\begin{pmatrix} 1 & 2 & 3 & | & 1 \\ 3 & 2 & 1 & | & 7 \\ 2 & 1 & 2 & | & 1 \end{pmatrix} \xrightarrow[R_3-2R_1]{R_2-3R_1} \begin{pmatrix} 1 & 2 & 3 & | & 1 \\ 0 & -4 & -8 & | & 4 \\ 0 & -3 & -4 & | & -1 \end{pmatrix}
$$
将第二行乘以 $-1/4$ 得到：
$$
\begin{pmatrix} 1 & 2 & 3 & | & 1 \\ 0 & 1 & 2 & | & -1 \\ 0 & -3 & -4 & | & -1 \end{pmatrix}
$$
将第三行加上第二行的 3 倍得到：
$$
\begin{pmatrix} 1 & 2 & 3 & | & 1 \\ 0 & 1 & 2 & | & -1 \\ 0 & -3 & -4 & | & -1 \end{pmatrix} \xrightarrow{R_3+3R_2} \begin{pmatrix} 1 & 2 & 3 & | & 1 \\ 0 & 1 & 2 & | & -1 \\ 0 & 0 & 2 & | & -4 \end{pmatrix}
$$
现在我们可以使用所谓的\textbf{反代入}(back substitution)来求解系统。即，从最后一行（方程）我们得到 $x_3 = -2$. ~然后从第二个方程我们得到 
$$x_2 = -1 - 2x_3 = -1 - 2(-2) = 3,$$
最后，从第一行（方程）$$x_1 = 1 - 2x_2 - 3x_3 = 1 - 6 + 6 = 1.$$

所以，解是 
$$\begin{cases} x_1 = 1 \\ x_2 = 3 \\ x_3 = -2 ,\end{cases}$$
或者向量形式 
$$\xx = \begin{pmatrix} 1 \\ 3 \\ -2 \end{pmatrix}.
$$
或 $\xx = (1, 3, -2)^T$.我们可以通过乘以系数矩阵 $A$ 来验算解。

换一种思路，与其使用反代入，不如从下到上进行行约简，消去系数矩阵主对角线以上的所有项。我们从把最后一行乘以 $1/2$ 开始，其余的都很直观：
$$
\begin{pmatrix} 1 & 2 & 3 & | & 1 \\ 0 & 1 & 2 & | & -1 \\ 0 & 0 & 1 & | & -2 \end{pmatrix} \xrightarrow[R_2-2R_3]{R_1-3R_3} \begin{pmatrix} 1 & 2 & 0 & | & 7 \\ 0 & 1 & 0 & | & 3 \\ 0 & 0 & 1 & | & -2 \end{pmatrix} \xrightarrow{R_1-2R_2} \begin{pmatrix} 1 & 0 & 0 & | & 1 \\ 0 & 1 & 0 & | & 3 \\ 0 & 0 & 1 & | & -2 \end{pmatrix}
$$
我们只需从简化阶梯形矩阵中读出解 $\xx = (1, 3, -2)^T$. ~

我们把阐述从下到上阶段的行约简算法留给读者作练习。

\subsection{2.3. 阶梯形}

一个矩阵被称为\textbf{阶梯形}（echelon form），如果它满足以下两个条件：

1. 所有零行(zero rows)（即所有项都等于 0 的行），如果存在的话，都位于所有非零项的下方。

对于非零行，让最左边的非零项称为\textbf{前导项}(leading entry)。那么阶梯形的第二性质可以表述如下：

2. 对于任何非零行，其前导项严格位于前一行前导项的右侧。

阶梯形中的每一行的前导项也称为\textbf{主元项}，或简称为\textbf{主元}，因为这些项正是我们在行约简中使用的主元。

我们上面得到的例子中的一个特殊情况是所谓的\textbf{三角}形(triangular)形式。在该形式中，系数矩阵是方阵（$n \times n$），其主对角线上的所有项都非零，并且主对角线下的所有项都为零。右侧，即增广矩阵的最右边一列，可以是任意的。

在行约简的向后阶段也完成之后，我们得到所谓的矩阵的\textbf{简化阶梯形}：系数矩阵等于 $I$，如上例所示，这是简化阶梯形的一个特例。

一般定义如下：我们说一个矩阵处于\textbf{简化阶梯形}，如果它处于阶梯形并且

3. 所有主元项都等于 1；

4. 主元上方的所有项都为 0。
注意，由于阶梯形的原因，主元下方的所有项也为 0。

为了从阶梯形得到简化阶梯形，我们从下往上，从右往左工作，使用行替换来消去主元上方的所有项。

简化阶梯形的一个例子是系数矩阵等于 $I$ 的系统。在这种情况下，只需从简化阶梯形中读出解。通常情况下，也可以轻松地从阶梯形读出解。例如，设系统（增广矩阵）的简化阶梯形是
$$
\begin{pmatrix}
\fbox{$1$} & 2 & 0 & 0 & 0 & | & 1 \\
0 & 0 & \fbox{$1$} & 5 & 0 & | & 2 \\
0 & 0 & 0 & 0 & \fbox{$1$} & | & 3
\end{pmatrix}
$$
这里我们框出了主元。这个想法是，将与没有主元的列对应的变量（所谓的\textbf{自由变量}(free variables)）移到右侧，这样我们就可以直接写出解。
$$
\begin{cases}
x_1 = 1 - 2x_2 \\
x_2 \text{ 是自由变量} \\
x_3 = 2 - 5x_4 \\
x_4 \text{ 是自由变量} \\
x_5 = 3
\end{cases}
$$
或者，在向量形式下：
$$
\xx = \begin{pmatrix} 1 - 2x_2 \\ x_2 \\ 2 - 5x_4 \\ x_4 \\ 3 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2 \\ 0 \\ 3 \end{pmatrix} + x_2 \begin{pmatrix} -2 \\ 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} + x_4 \begin{pmatrix} 0 \\ 0 \\ -5 \\ 1 \\ 0 \end{pmatrix}, \quad x_2, x_4 \in \FF
$$

也可以通过反代入从阶梯形得到解：其思想是从下往上工作，将所有自由变量移到右侧。


\begin{exer} \textbf{练习}~~

2.1. 将以下方程组写成矩阵形式和向量方程形式：

a) $\begin{cases} x_1 + 2x_2 - x_3 = -1 \\ 2x_1 + 2x_2 + x_3 = 1 \\ 3x_1 + 5x_2 - 2x_3 = -1 \end{cases}$

b) $\begin{cases} x_1 - 2x_2 - x_3 = 1 \\ 2x_1 - 3x_2 + x_3 = 6 \\ 3x_1 - 5x_2 = 7 \\ x_1 + 5x_3 = 9 \end{cases}$

c) $\begin{cases} x_1 + 2x_2 + 2x_4 = 6 \\ 3x_1 + 5x_2 - x_3 + 6x_4 = 17 \\ 2x_1 + 4x_2 + x_3 + 2x_4 = 12 \\ 2x_1 - 7x_3 + 11x_4 = 7 \end{cases}$

d) $\begin{cases} x_1 - 4x_2 - x_3 + x_4 = 3 \\ 2x_1 - 8x_2 + x_3 - 4x_4 = 9 \\ -x_1 + 4x_2 - 2x_3 + 5x_4 = -6 \end{cases}$

e) $\begin{cases} x_1 + 2x_2 - x_3 + 3x_4 = 2 \\ 2x_1 + 4x_2 - x_3 + 6x_4 = 5 \\ x_2 + 2x_4 = 3 \end{cases}$

f) $\begin{cases} 2x_1 - 2x_2 - x_3 + 6x_4 - 2x_5 = 1 \\ x_1 - x_2 + x_3 + 2x_4 - x_5 = 2 \\ 4x_1 - 4x_2 + 5x_3 + 7x_4 - x_5 = 6 \end{cases}$

g) $\begin{cases} 3x_1 - x_2 + x_3 - x_4 + 2x_5 = 5 \\ x_1 - x_2 - x_3 - 2x_4 - x_5 = 2 \\ 5x_1 - 2x_2 + x_3 - 3x_4 + 3x_5 = 10 \\ 2x_1 - x_2 - 2x_4 + x_5 = 5 \end{cases}$
\\
求解这些系统，并以向量形式写出答案。

2.2. 找到向量方程 $$x_1 \vv_1 + x_2 \vv_2 + x_3 \vv_3 = \oo$$
的所有解，其中 $\vv_1 = (1, 1, 0)^T$, $\vv_2 = (0, 1, 1)^T$ 和 $\vv_3 = (1, 0, 1)^T$. ~你能从这里得出关于向量系统 $\vv_1, \vv_2, \vv_3$ 线性无关（或相关）的什么结论？\end{exer}


\section{3. 主元的分析}

关于解的存在性和唯一性的所有问题都可以通过分析增广矩阵在阶梯形（简化阶梯形）中的主元来回答。

首先，让我们研究一下方程 $A \xx = \bb$ \textbf{不一致}(inconsistent)（即它无解）的情况。如果我们稍微思考一下，答案立即得出：

% \noindent % 防止段首缩进
\fbox{%
  \begin{minipage}{0.9\textwidth} % 创建一个占页面宽度90%的文本框
当增广矩阵的阶梯形中最后一个列有一个主元时，线性方程组才是不一致的（无解），即增广矩阵的阶梯形包含一行 $(0 ~\ 0 ~\ \dots ~\ 0 ~\ | \ b)$，其中 $b \neq 0$.
\end{minipage}%
}

实际上，这样的行对应于方程 $0 x_1 + 0 x_2 + \dots + 0 x_n = b \neq 0$，它确实无解。
如果我们没有这样的行，我们只需将其化为简化阶梯形，然后从中读出解。

现在，还有三个陈述。所有这些陈述都只涉及\textbf{系数矩阵}，而不是系统的增广矩阵。

1. 解（如果它存在）是唯一的，当且仅当没有自由变量，也就是说，当且仅当系数矩阵的阶梯形在每一列都有一个主元；

2. 方程 $A \xx = \bb$ 对于所有右侧 $\bb$ 都是\textbf{一致}(consistent)的，当且仅当系数矩阵的阶梯形在每一行都有一个主元。

3. 方程 $A \xx = \bb$ 对于任意右侧 $\bb$都有 \textbf{唯一解}(unique solution)，当且仅当系数矩阵 $A$ 的阶梯形在每一列和每一行都有一个主元。

第一个陈述是显然的，因为自由变量的存在导致了所有的不唯一性。我应该只强调这个陈述\textbf{并不说明}关于存在性的任何信息。

第二个陈述稍微复杂一些。如果我们有一个系数矩阵 $A$ ，其阶梯形的每一行都有一个竺院，那么我们不可能在\textbf{增广}矩阵的最后一列中有一个主元，所以系统总是有一致的解，无论右侧 $\bb$ 是什么。

让我们证明，如果我们有一个系数矩阵 $A$ 的阶梯形中的零行，那么我们可以选择一个右侧 $\bb$ 使得系统 $A \xx = \bb$ 不一致。设 $A_e$ 是系数矩阵 $A$ 的阶梯形。那么 
$$A_e = EA,$$
其中 $E$ 是对应于行运算的初等矩阵的乘积，$E = E_N \dots E_2 E_1$. ~如果 $A_e$ 有一个零行，那么最后一行也是零。因此，如果我们取 $\bb_e = (0, \dots, 0, 1)^T$（所有项都是 $0$，除了最后一个是 $1$），那么方程 
$$A_e \xx = \bb_e$$
无解。从左边乘以 $E^{-1}$ 并回忆 $E^{-1} A_e = A$，我们得到方程 
$$A \xx = E^{-1} \bb_e$$
无解。

最后，陈述 3 直接从陈述 1 和 2 得出。

上述对主元的分析带来了几个非常重要的推论。我们在观察中使用的主要事实是：

\fbox{%
  \begin{minipage} {0.9\textwidth}
在阶梯形中，每一行和每一列最多有一个主元（也可以没有主元）.
\end{minipage}
}

\subsection{3.1. 关于线性无关和基的推论~~维数}

关于向量系统在 $\FF^n$ 中是否是基、线性无关或生成系统的问题，都可以通过行约简轻松回答。

\textbf{命题 3.1}~~ 假设我们有一个向量系统 $\vv_1, \vv_2, \dots, \vv_m \in \FF^n$，并且令 $A = [\vv_1, \vv_2, \dots, \vv_m]$ 是以 $\vv_1, \vv_2, \dots, \vv_m$ 为列的 $n \times m$ 矩阵。那么

1. 系统 $\vv_1, \vv_2, \dots, \vv_m$ 线性无关，当且仅当 $A$ 的阶梯形在每一列都有一个主元；

2. 系统 $\vv_1, \vv_2, \dots, \vv_m$ 是 $\FF^n$ 中的完备（生成）集，当且仅当 $A$ 的阶梯形在每一行都有一个主元；

3. 系统 $\vv_1, \vv_2, \dots, \vv_m$ 是 $\FF^n$ 中的基，当且仅当 $A$ 的阶梯形在每一列和每一行都有一个主元。

\textbf{证明}~~ 向量系统 $\vv_1, \vv_2, \dots, \vv_m \in \FF^n$ 线性无关，当且仅当方程 
$$x_1 \vv_1 + x_2 \vv_2 + \dots + x_m \vv_m = \oo$$
只有唯一的（平凡）解 $x_1 = x_2 = \dots = x_m = 0$，或者等价地说，方程 $A \xx = \oo$ 有唯一解 $\xx = \oo$. ~根据上面的陈述 1，这发生在当且仅当矩阵的主元在每一列时。

类似地，系统 $\vv_1, \vv_2, \dots, \vv_m \in \FF^n$ 是 $\FF^n$ 中的完备集，当且仅当方程 
$$x_1 \vv_1 + x_2 \vv_2 + \dots + x_m \vv_m = \bb$$
对任何右侧 $\bb \in \FF^n$ 都有解。根据上面的陈述 2，这发生在当且仅当 $A$ 的矩阵的阶梯形在每一行都有一个主元。

最后，系统 $\vv_1, \vv_2, \dots, \vv_m \in \FF^n$ 是 $\FF^n$ 中的基，当且仅当方程 
$$x_1 \vv_1 + x_2 \vv_2 + \dots + x_m \vv_m = \bb$$
对任何右侧 $\bb \in \FF^n$ 都有唯一解。根据陈述 3，这发生在当且仅当 $A$ 的阶梯形在每一列和每一行都有一个主元。

\textbf{命题 3.2}~~ $\FF^n$ 中的任何线性无关系统不能包含超过 $n$ 个向量。

\textbf{证明}~~ 设系统 $\vv_1, \vv_2, \dots, \vv_m \in \FF^n$ 是线性无关的，并且设 $A = [\vv_1, \vv_2, \dots, \vv_m]$ 是以 $\vv_1, \vv_2, \dots, \vv_m$ 为列的 $n \times m$ 矩阵。根据命题 3.1， $A$ 的阶梯形必须在每一列都有一个主元，这在 $m > n$ 时是不可能的（主元数量不能超过行数）。

\textbf{命题 3.3}~~ 向量空间 $V$ 中的任何两个基具有相同数量的向量。

\textbf{证明}~~ 设 $\vv_1, \vv_2, \dots, \vv_n$ 和 $\ww_1, \ww_2, \dots, \ww_m$ 是 $V$ 中的两个不同的基。不失一般性，我们假设 $n \le m$. ~考虑一个同构 $A: \FF^n \to V$，定义为
$$A \ee_k = \vv_k, \quad k = 1, 2, \dots, n,$$
其中 $\ee_1, \ee_2, \dots, \ee_n$ 是 $\RR^n$ 的标准基。

由于 $A^{-1}$ 也是一个同构，系统 
$$A^{-1} \ww_1, A^{-1} \ww_2, \dots, A^{-1} \ww_m$$
是一组基（见第 1 章定理 6.6）。所以它是线性无关的，根据命题 3.2， $m \le n$. ~结合假设 $n \le m$，我们得到 $m=n$. ~

上述命题的一个特例是以下命题。

\textbf{命题 3.4}~~ $\FF^n$ 中的任何基必须恰好有 $n$ 个向量。

\textbf{证明}~~ 这个事实直接源于前面的命题，但也有一个直接的证明。设 $\vv_1, \vv_2, \dots, \vv_m$ 是 $\FF^n$ 中的一组基，令 $A$ 为以 $\vv_1, \vv_2, \dots, \vv_m$ 为列的 $n \times m$ 矩阵。系统是基的事实意味着方程 
$$A \xx = \bb$$
对任何（所有可能的）右侧 $\bb$ 都有唯一解。存在性意味着在（简化）阶梯形矩阵的每一行中都有一个主元，因此主元数量恰好是 $n$. ~唯一性意味着在系数矩阵（的阶梯形）的每一列中都有一个主元，所以 

~~~~~~~~$m =$ 列数 $=$ 主元数 $= n$.

\textbf{命题 3.5}~~ $\FF^n$ 中的任何生成集必须至少有 $n$ 个向量。

\textbf{证明}~~ 设 $\vv_1, \vv_2, \dots, \vv_m$ 是 $\FF^n$ 中的完备集，令 $A$ 为以 $\vv_1, \vv_2, \dots, \vv_m$ 为列的 $n \times m$ 矩阵。命题 3.1 的陈述 2 暗示 $A$ 的阶梯形在每一行都有一个主元。由于主元数量不能超过列的数量，所以 $n \le m$. ~

\subsection{3.2. 可逆矩阵的推论}

\textbf{命题 3.6}~~ 一个矩阵 $A$ 是可逆的，当且仅当它的阶梯形在每一列和每一行都有一个主元。

\textbf{证明}~~ 正如我们在本节开头所讨论的，方程 $A \xx = \bb$ 对于任何右侧 $\bb$ 都有唯一解，当且仅当 $A$ 的阶梯形在每一行和每一列都有一个主元。但是，我们知道（见第 1 章定理6.8），矩阵$A$是可逆的，当且仅当方程$A \xx = \bb$ 对右侧$ \bb$ 的每一项有唯一解。

也存在一个备选的证明。我们知道，一个矩阵是可逆的，当且仅当它的列（见第 1 章第 6.4 节的推论 6.9）构成一组基。前面的命题 3.4 表明了，这发生在当且仅当在每一行和每一列都有一个主元时。

上述命题立即蕴含了以下推论。

\textbf{推论 3.7}~~ 可逆矩阵\textbf{必须}是方阵 ($n \times n$)。

\textbf{命题 3.8}~~ 如果一个$n \times n$方阵  $A$ 是左可逆的，或者它是右可逆的，那么它就是可逆的。换句话说，要检查方阵 $A$ 的可逆性，只需检查 $A A^{-1} = I$ 或 $A^{-1} A = I$ 其中一个条件即可。

注意，这个命题仅适用于方阵！

\textbf{证明}~~ 我们知道，矩阵 $A$ 是可逆的，当且仅当方程 $A \xx = \bb$ 对于任何右侧 $\bb$ 都有唯一解。这发生在当且仅当 $A$ 的阶梯形在每一行和每一列都有一个主元。

如果矩阵 $A$ 是左可逆的，那么方程 $A \xx = \oo$ 有唯一解 $\xx = \oo$. ~实际上，如果 $B$ 是 $A$ 的左逆（即 $BA = I$），并且 $\xx$ 满足 
$$A \xx = \oo,$$
那么从左边将这个恒等式乘以 $B$，我们得到 $x = 0$，所以解是唯一的。因此，$A$ 的阶梯形在每一列都有一个主元（没有自由变量）。如果矩阵 $A$ 是方阵 ($n \times n$)，那么阶梯形也在每一行都有一个主元（$n$ 个主元，而一行最多有一个主元），所以矩阵是可逆的。

如果矩阵 $A$ 是右可逆的，并且 $C$ 是它的右逆 ($AC = I$)，那么对于 $\xx = C \bb$, $\bb \in \FF^n$， 
$$A \xx = AC \bb = I \bb = \bb.$$
因此，对于任何右侧 $\bb$，方程 $A \xx = \bb$ 都有解 $\xx = C \bb$. ~因此，$A$ 的阶梯形在每一行都有一个主元。如果 $A$ 是方阵，那么它也在每一列都有一个主元。所以 $A$ 是可逆的。

\begin{exer} \textbf{练习}~~

3.1. 对于 $b$ 的哪个值，系统 $$\begin{pmatrix} 1 & 2 & 2 \\ 2 & 4 & 6 \\ 1 & 2 & 3 \end{pmatrix} \xx = \begin{pmatrix} 1 \\ 4 \\ b \end{pmatrix}$$ 有解？对于该值 $b$，找到系统的通解。

3.2. 确定向量 

$$\begin{pmatrix} 1 \\ 1 \\ 0 \\ 0 \end{pmatrix},\quad \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix},\quad \begin{pmatrix} 0 \\ 0 \\ 1 \\ 1 \end{pmatrix},\quad \begin{pmatrix} 0 \\ 1 \\ 0 \\ 1 \end{pmatrix}$$

是否线性无关或相关。

这四个向量是否张成 $\RR^4$？（换句话说，它们是生成系统吗？）对于 $\CC^4$ 呢？

3.3. 确定以下向量系统是否是 $\RR^3$ 的基：

a) $(1, 2, -1)^T$, $(1, 0, 2)^T$, $(2, 1, 1)^T$;

b) $(-1, 3, 2)^T$, $(-3, 1, 3)^T$, $(2, 10, 2)^T$;

c) $(67, 13, -47)^T$, $(\pi, -7.84, 0)^T$, $(3, 0, 0)^T$.

哪个系统是 $\CC^3$ 的基？

3.4. 多项式 $t^3 + 2t$, $t^2 + t + 1$, $t^3 + 5$ 是否生成（张成）$\PP_3$？给出你的理由。

3.5. $\FF^4$ 中的 5 个向量可能线性无关吗？给出你的理由。

3.6. 证明或证伪：如果一个方阵（$n \times n$）$A$ 的列是线性无关的，那么 $A^2 = AA$ 的列也是线性无关的。

3.7. 证明或证伪：如果一个方阵（$n \times n$）$A$ 的列是线性无关的，那么 $A^3 = AAA$ 的行也是线性无关的。

3.8. 证明，如果方程 $A \xx = \oo$ 只有唯一解（即，如果 $A$ 的阶梯形在每一列都有一个主元），那么 $A$ 是左可逆的。\textbf{提示}：想想初等矩阵可能会有帮助。

\textbf{注记}： 这可能是一个非常难的问题，因为它需要对主题有深入的理解。但是，当你理解了该做什么之后，问题就变得几乎显然了。

3.9. 一个矩阵的简化阶梯形是唯一的吗？给出你的结论和理由。

也就是说，假设通过执行某些行运算（不一定遵循任何算法）我们得到了一个简化阶梯形矩阵。那么我们总是得到相同的矩阵，还是可能得到不同的矩阵？请注意，我们只允许执行行运算，“列运算”是被禁止的。

\textbf{提示}：如果以可逆矩阵开始，会发生什么？此外，主元是否总是在相同的列中，还是这取决于你执行的行运算？如果你能在不诉诸行运算的情况下知道主元列是什么，那么主元列的位置就不依赖于它们。\end{exer}


\section{4. 通过行约简求 $A^{-1}$}

正如我们在上面讨论的，可逆矩阵必须是方阵，并且其阶梯形在每一行和每一列都必须有主元。故而，可逆矩阵的简化阶梯形是单位矩阵 $I$. ~因此，

\fbox{%
  \begin{minipage} {0.8\textwidth}
任何可逆矩阵都可通过行约简（即通过行运算）化为单位矩阵。
\end{minipage}
}

下面有一个简单的算法，可以让我们来找到一个 $n \times n$ 矩阵的逆：

1. 通过在 $A$ 的右侧拼接 $n \times n$ 单位矩阵来形成一个 $n \times 2n$ 的\textbf{增广}矩阵 $(A | I)$；

2. 对增广矩阵执行行运算，将 $A$ 转化为单位矩阵 $I$；

3. 原本对应$I$ 的地方将被自动转化为 $A^{-1}$；

4. 如果通过行运算无法将 $A$ 转化为$n \times n$ 单位矩阵，则 $A$ 是不可逆的。

对于以上算法，我们有几个解释。第一个，朴素的解释：我们知道（对于可逆矩阵 $A$），向量 $A^{-1} \bb$ 是方程 $A \xx = \bb$ 的解。所以，要找到 $A^{-1}$ 的第 $k$ 列，我们需要找到 $A \xx = \ee_k$ 的解，其中 $\ee_1, \ee_2, \dots, \ee_n$ 是 $\RR^n$ 的标准基。上述算法只是同时求解方程 $$A \xx = \ee_k, \quad k = 1, 2, \dots, n.$$

我们也提供另一个看起来更“高级”的解释：正如我们上面讨论的，每个行运算都可以通过左乘一个初等矩阵来实现。设 $E_1, E_2, \dots, E_N$ 是对应于我们执行的行运算的初等矩阵，令 $E = E_N \dots E_2 E_1$ 为它们的乘积
\footnote{
 虽然在这里并不重要，但请注意，如果行运算 $E_1$ 最先得到执行，那么 $E_1$ 必须是乘积中最右边的项。
}
。我们知道行运算会将 $A$ 转化为单位矩阵，即 $EA = I$，所以 $E = A^{-1}$. ~那么，相同的行运算就将增广矩阵 $(A | I)$ 转化为 $(EA | E) = (I | A^{-1})$. ~



这个“高级”解释利用初等矩阵表述了一个重要的命题，该命题将在以后经常被使用。

\textbf{定理 4.1}~~ 任何可逆矩阵都可以表示为初等矩阵的乘积。

\textbf{证明}~~ 正如我们在上一段中所讨论的，$A^{-1} = E_N \dots E_2 E_1$，所以
$$A = (A^{-1})^{-1} = (E_N \dots E_2 E_1)^{-1} = E_1^{-1} E_2^{-1} \dots E_N^{-1}$$
（初等矩阵的逆也是初等矩阵）。

\textbf{一个例子}~~ 假设我们想找到矩阵
$$
\begin{pmatrix} 1 & 4 & -2 \\ -2 & -7 & 7 \\ 3 & 11 & -6 \end{pmatrix}
$$
的逆。将其与单位矩阵拼接为增广矩阵，并进行行约简，我们得到
$$
\begin{pmatrix} 1 & 4 & -2 & | & 1 & 0 & 0 \\ -2 & -7 & 7 & | & 0 & 1 & 0 \\ 3 & 11 & -6 & | & 0 & 0 & 1 \end{pmatrix} \xrightarrow[{R_3-3R_1}]{R_2+2R_1} \begin{pmatrix} 1 & 4 & -2 & | & 1 & 0 & 0 \\ 0 & 1 & 3 & | & 2 & 1 & 0 \\ 0 & -1 & 0 & | & -3 & 0 & 1 \end{pmatrix} \xrightarrow{R_3+R_2}
$$

$$
\begin{pmatrix} 1 & 4 & -2 & | & 1 & 0 & 0 \\ 0 & 1 & 3 & | & 2 & 1 & 0 \\ 0 & 0 & 3 & | & -1 & 1 & 1 \end{pmatrix}
\xrightarrow{R_1\times 3} \begin{pmatrix} 3 & 12 & -6 & | & 3 & 0 & 0 \\ 0 & 1 & 3 & | & 2 & 1 & 0 \\ 0 & 0 & 3 & | & -1 & 1 & 1 \end{pmatrix} \xrightarrow[R_2-2R_3]{R_1+2R_3} 
$$
在最后一步行运算中，我们将第一行乘以 3 以避免在向后阶段的行约简中出现分数。再次进行行约简，我们得到
$$
\begin{pmatrix} 3 & 12 & 0 & | & 1 & 2 & 2 \\ 0 & 1 & 0 & | & 3 & 0 & -1 \\ 0 & 0 & 3 & | & -1 & 1 & 1 \end{pmatrix} \xrightarrow{R_1-12R_2} \begin{pmatrix} 3 & 0 & 0 & | & -35 & 2 & 14 \\ 0 & 1 & 0 & | & 3 & 0 & -1 \\ 0 & 0 & 3 & | & -1 & 1 & 1 \end{pmatrix}
$$
将第一行和最后一行除以 3，我们得到逆矩阵
$$
\begin{pmatrix} -35/3 & 2/3 & 14/3 \\ 3 & 0 & -1 \\ -1/3 & 1/3 & 1/3 \end{pmatrix}
$$

\begin{exer} \textbf{练习}~~

4.1. 找到以下矩阵的逆：
$$\begin{pmatrix} 1 & 2 & 1 \\ 3 & 7 & 3 \\ 2 & 3 & 4 \end{pmatrix},\quad \begin{pmatrix} 1 & -1 & 2 \\ 1 & 1 & -2 \\ 1 & 1 & 4 \end{pmatrix}.$$

写出所有步骤。
\end{exer}

\section{5. 维数~有限维空间}

\textbf{定义}~~ 向量空间 $V$ 的\textbf{维数} $\dim V$ 是基中向量的数量。

对于仅由零向量 $\oo$ 组成的向量空间，我们设 $\dim V = 0$.如果 $V$ 不存在（有限）基，我们设 $\dim V = \infty$.

如果 $\dim V$ 是有限的，我们称空间 $V$ 为\textbf{有限维的}(finite-dimensional)；否则，我们称其为\textbf{无限维的}(infinite-dimensional)。

命题 3.3 表明维数是良好定义的，即它不依赖于基的选择。

第 1 章的命题 2.8 表明，有限维向量空间中的任何有限生成集都包含一组基。这直接蕴含了以下命题。

\textbf{命题 5.1}~~ 向量空间 $V$ 是有限维的，当且仅当它有一个有限生成集。

假设我们有一个有限维向量空间中的向量系统，并且我们想检查它是否是基（或者它是否线性无关，或者是否完备），最简单的方法可能就是使用同构 $A: V \to \RR^n$, $n = \dim E$ 将问题转移到 $\RR^n$，在 $\RR^n$ 中，所有这些问题都可以通过行约简（研究主元）来回答。

请注意，如果 $\dim V = n$，那么总存在一个同构 $A: V \to \RR^n$.实际上，如果 $\dim V = n$，则存在一组基 $\vv_1, \vv_2, \dots, \vv_n \in V$，并且可以定义一个同构 $A: V \to \RR^n$ 为 $$A \vv_k = \ee_k, \quad k = 1, 2, \dots, n.$$

例如，让我们给出命题 3.2 和 3.5 的两个推论如下：

\textbf{命题 5.2}~~ 有限维向量空间 $V$ 中的任何线性无关系统不能包含超过 $\dim V$ 个向量。

\textbf{证明}~~ 设 $\vv_1, \vv_2, \dots, \vv_m \in V$ 是线性无关系统，令  $A: V \to \RR^n$为一个同构 。那么 $A \vv_1, A \vv_2, \dots, A \vv_m$ 是 $\RR^n$ 中的线性无关系统，根据命题 3.2， $m \le n$.

% 那么 $\vv_1, \vv_2, \dots, \vv_m$ 为列的 $n \times m$ 矩阵。
\textbf{命题 5.3}~~ 有限维向量空间$V$ 中的任何生成系统必须至少有 $\dim V$ 个向量。

\textbf{证明}~~ 设 $\vv_1, \vv_2, \dots, \vv_m \in V$ 是 $V$ 中的完备系统，令  $A: V \to \RR^n$为一个同构 。
那么 $A \vv_1, A \vv_2, \dots, A \vv_m$ 是 $\RR^n$ 中的完备系统，根据命题 3.5， $m \geq n$.


%命题 3.1 的陈述 2 暗示 $A$ 的阶梯形在每一行都有一个主元。由于主元数量不能超过列的数量，所以 $m \ge n$. ~

\subsection{5.1. 将线性无关系统补全为基}

以下陈述将在后面扮演重要角色。

\textbf{命题 5.4（补全为基）}~~ 有限维空间中线性无关系统的向量可以补全为基，即，给定有限维向量空间 $V$ 中的线性无关向量 $\vv_1, \vv_2, \dots, \vv_r$，可以找到向量 $\vv_{r+1}, \vv_{r+2}, \dots, $ $\vv_n$ 使得系统中向量 $\vv_1, \vv_2, \dots, \vv_n$ 是 $V$ 中的一组基。

\textbf{证明}~~ 设 $\dim V = n$. ~选择一个不属于 $\text{span}\{\vv_1, \vv_2, \dots, \vv_r\}$ 的向量并称之为 $\vv_{r+1}$（由于系统 $\vv_1, \vv_2, \dots, \vv_r$ 不是生成的，总能做到这一点）。根据第 1 章练习 2.5，系统 $\vv_1, \dots, \vv_r, \vv_{r+1}$ 是线性无关的（注意在这种情况下 $r < n$，根据命题 5.2）。用新向量 $\vv_{r+2}$ 重复这个过程，依此类推。

直到得到一个生成系统后，这个过程才会停止。注意，这个过程不能无限进行，因为向量空间 $V$ 中的线性无关向量系统不能包含超过 $n = \dim V$ 个向量。

\subsection{5.2. 有限维空间的子空间}

\textbf{定理 5.5}~~ 设 $V$ 是 $W$ 的一个子空间，且$\dim W < \infty$. ~那么 $V$ 是有限维的，并且 $\dim V \le \dim W$. ~

此外，如果 $\dim V = \dim W$，则 $V = W$（这里我们仍然假设 $V$ 是 $W$ 的子空间）。

\textbf{注记}~~ 这个定理看起来像是一个平凡的论断，就像是命题 5.2 的一个简单的推论。但是，我们只能在已知 $V$ 的基的情况下才能应用命题 5.2。现在，我们只知道 $W$ 的基，而无法知道这个基中的多少向量属于 $V$；实际上，很容易构造一个例子，其中 $W$ 的基向量中没有一个属于 $V$. ~

\textbf{定理 5.5 的证明}~~ 如果 $V = \{\oo\}$，那么定理是平凡的，所以我们假设不为此情况。

我们想在 $V$ 中找到一组基。取一个非零向量 $\vv_1 \in V$. ~如果 $V = \text{span}\{\vv_1\}$，我们就找到了基（由单个向量 $\vv_1$ 组成）。

如果不是，我们继续归纳。假设我们已经构造了 $r$ 个线性无关向量 $\vv_1, \dots, \vv_r \in V$. ~如果 $V = \text{span}\{\vv_k : 1 \le k \le r\}$，那么我们已经找到了 $V$ 中的一组基。如果不是，则存在一个向量 $\vv_{r+1} \in V$, $\vv_{r+1} \notin \text{span}\{\vv_k : 1 \le k \le r\}$. ~根据第 1 章练习 2.5，系统 $\vv_1, \dots, \vv_r, \vv_{r+1}$ 是线性无关的。

% 重复这个过程，用新向量 $\vv_{r+2}$，依此类推。我们将停止这个过程，直到得到一个生成系统。注意，这个过程不能无限进行，因为向量空间 $V$ 中的线性无关向量系统不能包含超过 $n = \dim V$ 个向量。


\begin{exer} \textbf{练习}~~

5.1. 判断正误：

a) 任何由有限集生成的向量空间都有基；

b) 任何向量空间都有（有限）基；

c) 一个向量空间不能有多个基；

d) 如果一个向量空间有有限基，那么所有基中的向量数量是相同的；

e) $\PP_n$ 的维数是 $n$；

f) $M_{m \times n}$ 的维数是 $m+n$；

g) 如果向量 $\vv_1, \vv_2, \dots, \vv_n$ 生成（张成）向量空间 $V$，那么 $V$ 中的每个向量都可以唯一地表示为向量 $\vv_1, \vv_2, \dots, \vv_n$ 的线性组合；

h) 任何有限维空间的子空间都是有限维的；

i) 如果向量空间 $V$ 的维数是 $n$，那么 $V$ 只有一个零维子空间和一个 $n$ 维子空间。

5.2. 证明：如果 $V$ 是一个 $n$ 维向量空间，那么 $V$ 中的向量系统 $\vv_1, \vv_2, \dots, \vv_n$ 是线性独立的，当且仅当它张成 $V$. ~

5.3. 证明 $V$ 中的线性无关向量系统 $\vv_1, \vv_2, \dots, \vv_n$ 是基当且仅当 $n = \dim V$. ~

5.4. （重温一个旧问题：现在这个问题应该很容易回答了）是否可能，向量 $\vv_1, \vv_2, \vv_3$ 是线性相关的，但向量 $\ww_1 = \vv_1 + \vv_2$, $\ww_2 = \vv_2 + \vv_3$ 和 $\ww_3 = \vv_3 + \vv_1$ 是线性独立的？\textbf{提示}：向量空间 $\text{span}(\vv_1, \vv_2, \vv_3)$ 可以是什么维数？

5.5. 设 $\uu, \vv, \ww$ 是 $V$ 中的一组基。证明 $\uu+\vv+\ww$, $\vv+\ww$, $\ww$ 也是 $V$ 中的一组基。

5.6. 在 $\RR^5$ 空间中考虑向量 $\vv_1 = (2, -1, 1, 5, -3)^T$, $\vv_2 = (3, -2, 0, 0, 0)^T$, $\vv_3 = (1, 1, 50, -921, 0)^T.$

a) 证明这些向量是线性无关的。

b) 将此向量系统补全为基。

（如果你先做了 b) 部分，你可以完全不进行计算。）
\end{exer}

\section{6. 线性系统的通解}

在本节中，我们将讨论线性系统的通解（所有解，即解集）的结构。

我们称一个系统 $A \xx = \bb$ 为\textbf{齐次}(homogeneous)的，如果右侧 $\bb = \oo$，即齐次系统是 $A \xx = \oo$ 的形式。

并且对于每个系统 $$A \xx = \bb,$$
我们可以关联一个齐次系统，只需令 $\bb$ 为 $\oo$. ~

\textbf{定理 6.1（线性方程的通解）}~~ 设向量 $\xx_1$ 满足方程 $A \xx = \bb$，并设 $H$ 是相关齐次系统 $$A \xx = \oo$$
的所有解的集合。那么集合 
$$\{\xx = \xx_1 + \xx_h : \xx_h \in H\}$$
是方程 $A \xx = \bb$ 的所有解的集合。

换句话说，这个定理可以陈述为：

\fbox{$A \xx = \bb$ 的通解} ~~$=$~~ \fbox{$A \xx = \bb$ 的一个特解} ~~$+$~~ \fbox{$A \xx = \oo$ 的通解.}

\textbf{证明}~~ 固定一个满足 $A \xx_1 = \bb$ 的向量 $\xx_1$. ~设向量 $\xx_h$ 满足 $A \xx_h = \oo$. ~那么对于 $\xx = \xx_1 + \xx_h$，我们有 
$$A \xx = A(\xx_1 + \xx_h) = A \xx_1 + A \xx_h = \bb + \oo = \bb,$$
所以任何形式为 
$$\xx = \xx_1 + \xx_h,\quad \xx_h \in H$$
的 $\xx$ 都是 $A \xx = \bb$ 的解。

现在设 $\xx$ 满足 $A \xx = \bb$. ~那么对于 $\xx_h := \xx - \xx_1$，我们得到 

$$A \xx_h = A(\xx - \xx_1) = A \xx - A \xx_1 = \bb - \bb = \oo,$$
所以 $\xx_h \in H.$因此，$A \xx = \bb$ 的\textbf{任何}解都可以表示为 $\xx = \xx_1 + \xx_h$的形式，其中某个 $\xx_h \in H.$

这个定理的威力在于它的普适性。它适用于所有线性方程，而不必假设向量空间是有限维的。你将在微分方程、积分方程、偏微分方程等领域遇到这个定理。

除了展示解集结构之外，这个定理还允许我们将唯一性与存在性的研究分开。也就是说，为了研究唯一性，我们只需要分析齐次方程 $A \xx = \oo$ 的唯一性，而它总是有一个解。

在本节中，我们立即可以应用它：这个定理让我们能检查系统$A \xx = \bb$的解。例如，考虑系统
$$
\begin{pmatrix} 2 & 3 & 1 & 4 & -9 \\ 1 & 1 & 1 & 1 & -3 \\ 1 & 1 & 1 & 2 & -5 \\ 2 & 2 & 2 & 3 & -8 \end{pmatrix} \xx = \begin{pmatrix} 17 \\ 6 \\ 8 \\ 14 \end{pmatrix} 
$$
通过行约简，可以找到这个系统的解
$$
(6.1)\quad \xx = \begin{pmatrix} 3 \\ 1 \\ 0 \\ 2 \\ 0 \end{pmatrix} + x_3 \begin{pmatrix} -2 \\ 1 \\ 1 \\ 0 \\ 0 \end{pmatrix} + x_5 \begin{pmatrix} 2 \\ -1 \\ 0 \\ 2 \\ 1 \end{pmatrix}, \quad x_3, x_5 \in \FF
$$
参数 $x_3, x_5$ 在这里可以记作其他字母，例如 $t$ 和 $s$；我们在这里保留符号 $x_3$ 和 $x_5$ 仅仅是为了提醒我们，参数来自相应的自由变量。

现在，假设我们只是得到了这个解，并且我们想检查它是否正确。当然，我们可以重复行运算，但这太耗时了。而且，如果解是通过某种非标准行运算方法得到的，它可能看起来与我们从行约简得到的结果不同。例如，通解
$$
 (6.2)\quad \xx = \begin{pmatrix} 3 \\ 1 \\ 0 \\ 2 \\ 0 \end{pmatrix} + s \begin{pmatrix} -2 \\ 1 \\ 1 \\ 0 \\ 0 \end{pmatrix} + t \begin{pmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 1 \end{pmatrix}, \quad s, t \in \FF
$$
给出与 (6.1) 相同的解集（你能说明为什么吗？）；这里我们只是将 (6.1) 中的最后一个向量替换为它自身加上第二个向量的和。所以，这个式子看起来与我们从行约简得到的解不同，但它仍然是正确的。

检查 (6.1) 和 (6.2) 是否给出正确的解的最简单方法是检查第一个向量（3, 1, 0, 2, 0）$^T$ 是否满足方程 $A \xx = \bb$，而其他两个向量（带参数的向量，$x_3$ 和$x_5$或 $s$ 和 $t$ 在它们前面）应该满足相应的齐次方程 $A \xx = \oo$. ~

如果检查通过了，我们就能确保由 (6.1) 或 (6.2) 定义的任何向量 $\xx$ 确实是一个解。

请注意，这种检查解的方法并不能保证 (6.1)（或 (6.2)）给出所有解。例如，如果我们只是不知怎么就遗漏了 $x_3$ 相关的项，上述方法仍然能正常工作。

因此，我们如何保证我们没有遗漏任何自由变量，并且在(6.1)中不需要额外的项 ？

这时浮现在我们脑海中的办法，就是再次计算主元的数量。在这个例子中，如果进行行运算，主元的数量是 3。所以确实应该有 2 个自由变量，而且看起来我们没有遗漏 (6.1) 中的任何内容。

为了能够\textbf{证明}这一点确实成立，我们需要引入基本子空间和矩阵秩的新概念。
我还得指出，在上面的例子中，我们不必进行所有行运算，就能检查出只有 2 个自由变量，并且公式 (6.1) 和 (6.2) 都给出了正确的通解。

\begin{exer} \textbf{练习}~~

6.1. 判断正误：

a) 任何线性方程组都有至少一个解；

b) 任何线性方程组最多有一个解；

c) 任何齐次线性方程组至少有一个解；

d) 含$n$ 个未知数的 $n$ 个线性方程组至少有一个解；

e) 含$n$ 个未知数的 $n$ 个线性方程组最多有一个解；

f) 如果与给定线性方程组相对应的齐次方程组有解，则给定方程组有解；

g) 如果 含$n$ 个未知数的 $n$ 个齐次线性方程组的系数矩阵是可逆的，那么该系统没有非零解；

h)  含$n$ 个未知数$m$ 个方程的任何线性方程组的解集是 $\RR^n$ 中的一个子空间；

i) 任何齐次线性方程组的解集 $m$ 个方程 $n$ 个未知数是 $\RR^n$ 中的一个子空间。

6.2. 找到一个 $2 \times 3$ 系统（含有3 个未知数的 2 个方程），使得其通解具有形式 $$\begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} + s \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix},\quad s \in \RR.$$
\end{exer}

\section{7. 矩阵的基本子空间~~秩}

正如我们在第 1 章第 7 节中所讨论的，任何线性变换 $A: V \to W$ 都可以关联两个子空间，即它的核或零空间 $$\text{Ker } A = \text{Null } A := \{ \vv \in V : A \vv = \oo \} \subset V,$$
以及它的像空间 $$\text{Ran } A = \{ \ww \in W : \ww = A \vv \text{ 对于某个 } \vv \in V \}~~\subset W.$$
换句话说，核 $\text{Ker } A$ 是齐次方程 $A \xx = \oo$ 的解集，而像空间 $\text{Ran } A$ 正是方程 $A \xx = \bb$ 有解的所有右侧 $ \bb \in W$ 的集合。

如果 $A$ 是一个 $m \times n$ 矩阵，即从 $\FF^n$ 到 $\FF^m$ 的一个线性变换，那么回忆矩阵乘法的“按列坐标规则”，我们可以看到任何向量 $\ww \in \text{Ran } A$ 都可以表示为 $A$ 的列的线性组合。这解释了为什么\textbf{列空间}（表示为 $\text{Col } A$）这个术语经常用来表示矩阵的像空间。因此，对于矩阵 $A$，符号 $\text{Col } A$ 通常用于代替 $\text{Ran } A$. ~

如果 $A$ 是一个矩阵，那么除了 $\text{Ran } A$ 和 $\text{Ker } A$ 之外，我们还可以考虑转置矩阵 $A^T$ 的像空间和核空间。通常\textbf{行空间}(row space)用于表示 $\text{Ran } A^T$，而\textbf{左零空间}(left null space)用于表示 $\text{Ker } A^T$（但通常没有特殊的符号来表示）。

四个子空间 $\text{Ran } A$, $\text{Ker } A$, $\text{Ran } A^T$, $\text{Ker } A^T$ 称为矩阵 $A$ 的\textbf{基本子空间}(fundamental subspaces)。
在本节中，我们将研究它们的维数之间重要的关系。

我们需要以下定义，这是线性代数的基本概念之一。

\textbf{定义}~~ 给定一个线性变换（矩阵）$A$，它的\textbf{秩}，$\text{rank } A$\footnote{译者注：或表示为$r(A)$，后者在国内教科书中更常见}，是它的像空间的维数：
$$
\text{rank } A := \dim \text{Ran } A
$$


\subsection{7.1. 计算基本子空间和秩}

要计算矩阵的基本子空间和秩，就需进行行约简。也就是说，设 $A$ 是矩阵，且 $A_e$ 是其阶梯形：

1. 原始矩阵$A$ 的\textbf{主元列}（即行约简后将有主元的列）给出了 $\text{Ran } A$ 的一组基（它是众多可能的基之一）。

2. 阶梯形 $A_e$ 的\textbf{主元行}给出了行空间的基。当然，也可以简单地转置矩阵，然后进行行约简。但是，如果我们已经得到了 $A$ 的阶梯形，例如在计算 $\text{Ran } A$ 时，那么我们就能自然得到 $\text{Ran } A^T$. ~

3. 要找到零空间 $\text{Ker } A$ 的基，需求解齐次方程 $A \xx = \oo$：具体细节将从下面的例子中看出。

\textbf{例子}~~ 考虑矩阵
$$
\begin{pmatrix}
1 & 1 & 2 & 2 & 1 \\
2 & 2 & 1 & 1 & 1 \\
3 & 3 & 3 & 3 & 2 \\
1 & 1 & -1 & -1 & 0
\end{pmatrix}.
$$
进行行运算我们得到阶梯形
$$
\begin{pmatrix}
\fbox{$1$} & 1 & 2 & 2 & 1 \\
0 & 0 & \fbox{$-3$} & -3 & -1 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
$$
（这里主元已被框出）。因此，\textbf{原始矩阵}的第 1 列和第 3 列，即列向量
$$
\begin{pmatrix} 1 \\ 2 \\ 3 \\ 1 \end{pmatrix}, \quad \begin{pmatrix} 2 \\ 1 \\ 3 \\ -1 \end{pmatrix}
$$
给出了 $\text{Ran } A$ 的一组基。我们也自然得到了行空间 $\text{Ran } A^T$ 的基：$A$ 的\textbf{阶梯形}的第一行和第二行，即向量
$$
\begin{pmatrix} 1 \\ 1 \\ 2 \\ 2 \\ 1 \end{pmatrix}, \quad \begin{pmatrix} 0 \\ 0 \\ -3 \\ -3 \\ -1 \end{pmatrix}
$$
（这里我们将向量垂直放置。放在这里的向量到底是作为列，还是作为行，这真的只是一个约定问题。我们将其垂直放置的原因是，尽管我们称 $\text{Ran } A^T$ 为\textbf{行空间}，但我们将其定义为 $A^T$ 的列空间）。

为了计算零空间 $\text{Ker } A$ 的基，我们需要求解方程 $A \xx = \oo$. ~为此计算 $A$ 的\textbf{简化}阶梯形，在这个例子中得到的是
$$
\begin{pmatrix}
\fbox{$1$} & 1 & 0 & 0 & 1/3 \\
0 & 0 & \fbox{$1$} & 1 & 1/3 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}.
$$
注意，在求解齐次方程 $A \xx = \oo$ 时，不必写出整个增广矩阵，只处理系数矩阵就足够了。实际上，在这种情况下，增广矩阵的最后一列是零列，它在行运算下不会改变。所以，我们可以不实际写出它以节省笔墨，只在心中记住有这一列即可。在心中记住它时，我们可以从上面的简化阶梯形中读出解：
$$
\begin{cases}
x_1 = -x_2 - \frac{1}{3} x_5,\\ 
x_2 \text{ 是自由变量} \\
x_3 = -x_4 - \frac{1}{3} x_5 ,\\
x_4 \text{ 是自由变量} \\
x_5  \text{ 是自由变量}
\end{cases}
$$
或者，在向量形式下：
$$
 (7.1)\quad \xx = \begin{pmatrix} -x_2 - \frac{1}{3} x_5 \\ x_2 \\ -x_4 - \frac{1}{3} x_5 \\ x_4 \\ x_5 \end{pmatrix} = x_2 \begin{pmatrix} -1 \\ 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} + x_4 \begin{pmatrix} 0 \\ 0 \\ -1 \\ 1 \\ 0 \end{pmatrix} + x_5 \begin{pmatrix} -1/3 \\ 0 \\ -1/3 \\ 0 \\ 1 \end{pmatrix}.
% \quad x_2, x_4, x_5 \in \FF
$$
向量在每个自由变量处，即在我们的例子中，向量
$$
\begin{pmatrix} -1 \\ 1 \\ 0 \\ 0 \\ 0 \end{pmatrix}, \quad \begin{pmatrix} 0 \\ 0 \\ -1 \\ 1 \\ 0 \end{pmatrix}, \quad \begin{pmatrix} -1/3 \\ 0 \\ -1/3 \\ 0 \\ 1 \end{pmatrix}
$$
构成 $\text{Ker } A$ 的一组基。

不幸的是，想找到 $\text{Ker } A^T$ 的基没有捷径，必须求解方程 $A^T \xx = \oo$. ~知道 $A$ 的阶梯形在此处无济于事。

\subsection{7.2. 基本子空间基的计算方法的解释}

那么，为什么上述方法确实给出了基本子空间的基呢？

\subsubsection{7.2.1. 零空间}
 
 $\text{Ker } A$~~
零空间 $\text{Ker } A$ 的情况可能是最简单的：由于我们求解了方程 $A \xx = \oo$，即找到了所有解，那么 $\text{Ker } A$ 中的任何向量都是我们得到的向量的线性组合。因此，我们得到的向量构成了 $\text{Ker } A$ 中的一个生成系统。要看到系统是线性无关的，我们可以让每个向量乘以相应的自由变量并将所有向量相加，见 (7.1)。那么对于每个自由变量 $x_k$，结果向量的第 $k$ 个分量恰好是 $x_k$，再次见 (7.1)，所以这个向量（线性组合）是 $\oo$ 的唯一方式是当所有自由变量都为 0 时。

\subsubsection{7.2.2. 列空间}

$\text{Ran } A$~~
让我们现在解释为什么用于找到列空间 $\text{Ran } A$ 的基的方法有效。首先，注意到$A$ 的简化阶梯形， $A_{re}$ 的\textbf{主元列}构成了 $\text{Ran } A_{re}$ 的基（不是原始矩阵的列空间，而是其简化阶梯形列空间的基！）。由于行运算只是可逆矩阵的左乘，它们不改变线性无关性。因此，\textbf{原始矩阵} $A$ 的主元列是线性无关的。

让我们现在证明 $A$ 的主元列张成 $A$ 的列空间。设 $\vv_1, \vv_2, \dots, \vv_r$ 是 $A$ 的主元列，设 $\vv$ 是 $A$ 的任意一列。我们想证明 $\vv$ 可以表示为主元列 $\vv_1, \vv_2, \dots, \vv_r$ 的线性组合， 
$$\vv = \alpha_1 \vv_1 + \alpha_2 \vv_2 + \dots + \alpha_r \vv_r.$$

简化阶梯形$A_{re}$ 是通过左乘 $$A_{re} = EA$$
从 $A$ 得到的，其中 $E$ 是初等矩阵的乘积，所以 $E$ 是可逆矩阵。向量 $E \vv_1, E \vv_2, \dots, E \vv_r$ 是 $A_{re}$ 的主元列，而 $A$ 的列 $\vv$ 被变换为 $A_{re}$ 的列 $E \vv$. ~由于 $A_{re}$ 的主元列构成了 $\text{Ran } A_{re}$ 的基，向量 $E \vv$ 可以表示为线性组合 
$$E \vv = \alpha_1 E \vv_1 + \alpha_2 E \vv_2 + \dots + \alpha_r E \vv_r.$$
将这个等式两边同时左乘 $E^{-1}$，我们得到表示 
$$\vv = \alpha_1 \vv_1 + \alpha_2 \vv_2 + \dots + \alpha_r \vv_r,$$
因此 $A$ 的主元列确实张成了 $\text{Ran } A$. ~

\subsubsection{7.2.3. 行空间}
 
 $\text{Ran } A^T$~~
可以很容易地看出，$A$的阶梯形 $A_e$ 的\textbf{主元行}是线性无关的。实际上，设 $\ww_1, \ww_2, \dots, \ww_r$ 是 $A_e$ 的转置（因为我们公认总是将向量垂直放置）的主元行。假设 $$\alpha_1 \ww_1 + \alpha_2 \ww_2 + \dots + \alpha_r \ww_r = \oo.$$
考虑 $\ww_1$ 的第一个非零项。由于对于所有其他向量 $\ww_2, \ww_3, \dots, \ww_r$，相应的项等于 0（根据阶梯形的定义），我们可以得出 $\alpha_1 = 0$. ~所以我们可以消去这一项。

现在考虑 $\ww_2$ 的第一个非零项。向量 $\ww_3, \dots, \ww_r$ 的相应项为 0，所以 $\alpha_2 = 0$. ~重复这个过程，我们得到 $\alpha_k = 0  \forall k = 1, 2, \dots, r$. ~

要证明向量 $\ww_1, \ww_2, \dots, \ww_r$ 张成了行空间，我们注意到\textbf{“行运算不改变行空间”}。这可以从直接分析行运算得到，但我们在这里提供一种更正式的方式来演示这个事实。

对于变换 $A$ 和集合 $X$，我们用 $A(X)$ 来表示所有可以被表示为 $y = A(x)$, $x \in X$ 的元素 $y$ 的集合，即
$$A(X) := \{ y = A(x) : x \in X \}.$$

如果 $A$ 是一个 $m \times n$ 矩阵， $A_e$ 是它的阶梯形，$A_e$ 是通过左乘 
$$A_e = EA$$
得到的，其中 $E$ 是一个 $m \times m$ 可逆矩阵（对应初等矩阵的乘积）。那么
$$\text{Ran } A_e^T = \text{Ran}(A^T E^T) = A^T(\text{Ran } E^T) = A^T(\RR^m) = \text{Ran } A^T,$$
所以确实 $\text{Ran } A^T = \text{Ran } A_e^T$.

\subsection{7.3. 秩定理~基本子空间的维数}

在许多应用中，我们需要找到列空间或零空间的基。例如，正如上面所显示的，求解齐次方程 $A \xx = \oo$ 等价于找到零空间 $\text{Ker } A$ 的基。找到列空间的基意味着通过移除不必要的向量（列），来从生成集中提取基。

然而，基本子空间计算方法最重要的应用是得到它们维数之间的关系。

\textbf{定理 7.1（秩定理）}~~ 对于矩阵 $A$，
$$\text{rank } A = \text{rank } A^T.$$

这个定理通常表述为：

\fbox{矩阵的\textbf{列秩}等于它的\textbf{行秩}。}

这个定理的证明是显然的，因为 $\text{Ran } A$ 和 $\text{Ran } A^T$ 的维数都等于 $A$ 的阶梯形中的主元数量。

以下定理为我们提供了基本子空间维数之间重要的关系。它通常也称为秩定理。

\textbf{定理 7.2}~~ 设 $A$ 是一个 $m \times n$ 矩阵，即从 $\FF^n$ 到 $\FF^m$ 的线性变换。那么

1. $\dim \text{Ker } A + \dim \text{Ran } A = \dim \text{Ker } A + \text{rank } A = n$（$A$ 的定义域的维数）；

2. $\dim \text{Ker } A^T + \dim \text{Ran } A^T = \dim \text{Ker } A^T + \text{rank } A^T = \dim \text{Ker } A^T + \text{rank } A = m$（$A$ 的目标空间的维数）；

\textbf{证明}~~ 这个证明，在上述计算基本子空间基的方法所传达的意思中，几乎是显然的。第一个陈述仅仅是自由变量的数量（$\dim \text{Ker } A$）加上基本变量的数量（即主元数量，即 $\text{rank } A$）等于列的数量（即等于 $n$）。

第二个陈述，考虑到 $\text{rank } A = \text{rank } A^T$，仅仅是将第一个陈述应用于 $A^T$. ~

作为上述定理的一个应用，让我们回顾一下第 6 节的例子。在那里，我们考虑了系统
$$
\begin{pmatrix} 2 & 3 & 1 & 4 & -9 \\ 1 & 1 & 1 & 1 & -3 \\ 1 & 1 & 1 & 2 & -5 \\ 2 & 2 & 2 & 3 & -8 \end{pmatrix} \xx = \begin{pmatrix} 17 \\ 6 \\ 8 \\ 14 \end{pmatrix},
$$
并且我们声称它的通解由
$$
\xx = \begin{pmatrix} 3 \\ 1 \\ 0 \\ 2 \\ 0 \end{pmatrix} + x_3 \begin{pmatrix} -2 \\ 1 \\ 1 \\ 0 \\ 0 \end{pmatrix} + x_5 \begin{pmatrix} 2 \\ -1 \\ 0 \\ 2 \\ 1 \end{pmatrix}, \quad x_3, x_5 \in \FF
$$
或者由
$$
\xx = \begin{pmatrix} 3 \\ 1 \\ 0 \\ 2 \\ 0 \end{pmatrix} + s \begin{pmatrix} -2 \\ 1 \\ 1 \\ 0 \\ 0 \end{pmatrix} + t \begin{pmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 1 \end{pmatrix}, \quad s, t \in \FF
$$
给出。我们在第 6 节中检查了由任一公式给出的向量 $\xx$ 确实是方程的解。但是，我们如何保证 (6.1)或 (6.2)中的任何一个公式都给出了\textbf{所有}的解？

首先，我们知道在任一公式中，最后两个向量（被参数乘的向量）都属于 $\text{Ker } A$. ~很容易看出，在任一情况下，这两个向量都是线性无关的（两个向量线性相关当且仅当其中一个是另一个的某一标量倍）。

现在，让我们计算维数：将第一行和第二行交换，并进行第一轮行运算
$$
\begin{pmatrix} 1 & 1 & 1 & 1 & -3 \\ 2 & 3 & 1 & 4 & -9 \\ 1 & 1 & 1 & 2 & -5 \\ 2 & 2 & 2 & 3 & -8 \end{pmatrix} \xrightarrow[{R_3-R_1; }{R_4-2R_1}]{R_2-2R_1} \begin{pmatrix} 1 & 1 & 1 & 1 & -3 \\ 0 & 1 & -1 & 2 & -3 \\ 0 & 0 & 0 & 1 & -2 \\ 0 & 0 & 0 & 1 & -2 \end{pmatrix}
$$
我们看到已经有三个主元了，所以 $\text{rank } A \ge 3$. ~（实际上，我们可以已经看出秩是 3，但这里只需估计就足够了）。根据定理 7.2， $\text{rank } A + \dim \text{Ker } A = 5$，因此 $\dim \text{Ker } A \le 2$，所以 $\text{Ker } A$ 中不能有超过 2 个线性无关向量。因此，任一公式中的最后 2 个向量构成了 $\text{Ker } A$ 的基，所以任一公式都给出了方程的所有解。

秩定理的一个重要推论，是以下将存在性和唯一性联系起来的关于线性方程的定理。

\textbf{定理 7.3}~~ 设 $A$ 是一个 $m \times n$ 矩阵。那么方程 
$$A \xx = \bb$$
对于每一个 $\bb \in \RR^m$ 都有解，当且仅当对偶方程 
$$A^T \xx = \oo$$
只有唯一的（仅平凡的）解。（注意，在第二个方程中我们有 $A^T$，而不是 $A$）。

\textbf{证明}~~ 证明直接从定理 7.2 得出，只需计算维数即可。我们将具体细节留给读者作为练习。

上述定理有一个很好的几何解释。也就是说，陈述 1 表明，如果一个变换 $A: \FF^n \to \FF^m$ 具有平凡核（Ker $A = \{\oo\}$），那么定义域 $\FF^n$ 和像空间 $\text{Ran } A$ 的维数相等。如果核不是平凡的，那么变换“消去”(kills)了 $\dim \text{Ker } A$ 个维数，所以 $\dim \text{Ran } A = n - \dim \text{Ker } A$.

\subsection{7.4. 将线性无关系统补全为基}

正如上面第 5 节的命题 5.4 所断言的，任何线性无关系统都可以补全为基，即，给定有限维向量空间 $V$ 中的线性无关向量 $\vv_1, \vv_2, \dots, \vv_r$，可以找到向量 $\vv_{r+1}, \vv_{r+2},$ $ \dots, \vv_n$ 使得向量系统 $\vv_1, \vv_2, \dots, \vv_n$ 是 $V$ 中的一组基。

理论上，这个命题的证明为我们提供了寻找向量 $\vv_{r+1}, \vv_{r+2}, \dots, \vv_n$ 的算法，但这个算法看起来不太实用。

本节的思想为我们提供了一种更实用的补全为基的方法。

首先，注意到如果一个 $m \times n$ 矩阵处于阶梯形，那么它的非零行（它们显然是线性无关的）可以很容易地补全为 $\FF^n$ 中的基：我们只需要在适当的位置添加一些行，使得结果矩阵仍然是阶梯形并且在每一列都有主元。

然后，新矩阵的非零行构成一组基。我们可以按任何我们想要的顺序排列它们，因为作为基的性质不依赖于顺序。

假设现在我们有线性无关向量 $\vv_1, \vv_2, \dots, \vv_r$, $\vv_k \in \FF^n$. ~考虑以 $\vv_1^T, \vv_2^T, \dots, \vv_r^T$ 为行的矩阵 $A$，并执行行运算得到阶梯形 $A_e$. ~正如我们上面所讨论的，$A_e$ 的行可以很容易地补全为 $\RR^n$ 中的基；
结果是，能够补全 $A_e$ 的行使其成为一组基的向量，同样能够补全原始向量 $\vv_1, \vv_2, \dots, \vv_r$ 为一组基。

为了证明这一点，设向量 $\vv_{r+1}, \dots, \vv_n$ 补全了 $A_e$ 的行，使其在 $\FF^n$ 中成为一组基。然后，如果我们向矩阵 $A_e$ 添加行 $\vv_{r+1}^T, \dots, \vv_n^T$，我们会得到一个可逆矩阵。称此矩阵为 $\tilde{A}_e$，并设 $\tilde{A}$ 是通过添加行 $\vv_{r+1}^T, \dots, \vv_n^T$ 从 $A$ 得到的矩阵。矩阵 $\tilde{A}_e$ 可以通过行运算从 $\tilde{A}$ 得到，所以
$$
\tilde{A}_e = E \tilde{A},
$$
其中 $E$ 是相应初等矩阵的乘积。那么 $\tilde{A} = E^{-1}\tilde{A_e}$，并且 $\tilde{A}$ 作为可逆矩阵的乘积，也是可逆的。

但这表示 $\tilde{A}$ 的行在 $\FF^n$ 中构成一组基，这正是我们所需要的。



\textbf{注记}~~ 上面描述的补全为基的方法可能不是最简单的方法，但其主要优点之一是它适用于任意域上的向量空间。


\begin{exer} \textbf{练习}~~

7.1. 判断正误：

a) 矩阵的秩等于其非零列的数量；

b) $m \times n$ 零矩阵是唯一的秩为 0 的 $m \times n$ 矩阵；

c) 初等行运算保持秩；

d) 初等列运算不一定保持秩；

e) 矩阵的秩等于矩阵中线性无关列的最大数量；

f) 矩阵的秩等于矩阵中线性无关行的最大数量；

g) $n \times n$ 矩阵的秩最多为 $n$；

h) 秩为 $n$ 的 $n \times n$ 矩阵是可逆的。

7.2. 一个 $54 \times 37$ 的矩阵秩为 $31$.~所有 （4 个）基本子空间的维数是多少？

7.3. 计算矩阵 
$$\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 1 & 0 \end{pmatrix},\quad \begin{pmatrix} 1 & 2 & 3 & 1 & 1 \\ 1 & 4 & 0 & 1 & 2 \\ 0 & 2 & -3 & 0 & 1 \\ 1 & 0 & 0 & 0 & 0 \end{pmatrix}$$
的秩和所有四个基本子空间的基。

7.4. 证明，如果 $A: X \to Y$ ，且 $V$ 是 $X$ 的子空间，那么 $\dim AV \le \text{rank } A$. ~（这里 $AV$ 表示变换了 $A$ 变换后的子空间 $V$，即 $AV$ 中的任何向量都可以表示为 $A \vv$, $\vv \in V$）。由此推导出 $\text{rank}(AB) \le \text{rank } A$. ~

\textbf{注记}： 这里你可以利用 $V \subset W$ 则 $\dim V \le \dim W$ 的事实。你知道它为什么是对的吗？

7.5. 证明，如果 $A: X \to Y$ ，且 $V$ 是 $X$ 的子空间，那么 $\dim AV \le \dim V$. ~由此推导出 $\text{rank}(AB) \le \text{rank } B$. ~

7.6. 证明，如果两个 $n \times n$ 矩阵 $A$ 和 $B$ 的乘积 $AB$ 是可逆的，那么 $A$ 和 $B$ 都是可逆的。（即使你知道行列式，也请不要使用，因为我们现在还没有引入它。）\textbf{提示}：使用前两问的结论。

7.7. 证明，如果 $A \xx = \oo$ 只有唯一解，那么方程 $A^T \xx = \bb$ 对于每个右侧 $\bb$ 都有解。\textbf{提示}：计算主元。

7.8. 构造一个具有所需性质的矩阵，或解释为什么不存在这样的矩阵：

a) 列空间包含 $(1, 0, 0)^T$, $(0, 0, 1)^T$，行空间包含 $(1, 1)^T$, $(1, 2)^T$；

b) 列空间由 $(1, 1, 1)^T$ 张成，零空间由 $(1, 2, 3)^T$ 张成；

c) 列空间是 $\RR^4$，行空间是 $\RR^3$. ~

\textbf{提示}：首先检查维数是否匹配。

7.9. 如果 $A$ 和 $B$ 具有相同的四个基本子空间，那么 $A = B$ 成立吗？

7.10. 将以下向量的行补全为 $\RR^7$ 的基：
\[
\begin{pmatrix}
e^{3} & 3 & 4 & 0 & -\pi & 6 & -2 \\
0 & 0 & 2 & -1 & \pi^{e} & 1 & 1 \\
0 & 0 & 0 & 0 & 3 & -3 & 2 \\
0 & 0 & 0 & 0 & 0 & 0 & 1
\end{pmatrix}.
\]

7.11. 对于矩阵 $$ \begin{pmatrix} 1 & 2 & -1 & 2 & 3 \\ 2 & 2 & 1 & 5 & 5 \\ 3 & 6 & -3 & 0 & 24 \\ -1 & -4 & 4 & -7 & 11 \end{pmatrix},$$
找到其列空间和行空间的基。

7.12. 对于上题的矩阵，将行空间的基补全为 $\RR^5$ 的基。

7.13. 对于矩阵 
$$A = \begin{pmatrix} 1 & \rm i \\ \rm i & -1 \end{pmatrix},$$
计算 $\text{Ran } A$ 和 $\text{Ker } A$. ~你能说出这些子空间之间的关系吗？

7.14. 对于实数矩阵 $A$，$\text{Ran } A = \text{Ker } A^T$ 是否可能？若对于复数矩阵 $A$ ，是否可能？

7.15. 将向量 $(1, 2, -1, 2, 3)^T$, $(2, 2, 1, 5, 5)^T$, $(-1, -4, 4, 7, -11)^T$ 补全为 $\RR^5$ 的基。\end{exer}


\section{8. 任意基下线性变换的表示~~坐标变换公式}

我们在第 1 章中学到的关于线性变换及其矩阵的知识，可以很容易地推广到有限基下的抽象向量空间中的变换。在本节中，我们将区分线性变换 $T$ 和它的矩阵，原因是我们将考虑不同的基，所以线性变换可以有不同的矩阵表示。

\subsection{8.1. 坐标向量}

设 $V$ 是一个向量空间，具有基 $\B := \{\bb_1, \bb_2, \dots, \bb_n\}$. ~任何向量 $\vv \in V$ 都可以唯一地表示为线性组合
$$
\vv = x_1 \bb_1 + x_2 \bb_2 + \dots + x_n \bb_n = \sum_{k=1}^n x_k \bb_k.
$$
系数 $x_1, x_2, \dots, x_n$ 称为向量 $\vv$ 在基 $\B$ 下的\textbf{坐标}(coordinates)。可以方便地将这些坐标组合成向量 $\vv$ 相对于基 $\B$ 的所谓的\textbf{坐标向量}(coordinate vector)，这是一个列向量 
$$[\vv]_\B := \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \in \FF^n.$$

注意，映射 $$\vv \mapsto [\vv]_\B$$
是 $V$ 和 $\FF^n$ 之间的同构。它将基 $\bb_1, \bb_2, \dots, \bb_n$ 映射到 $\FF^n$ 中的标准基 $\ee_1, \ee_2, \dots, \ee_n$. ~

\subsection{8.2. 线性变换的矩阵}

设 $T: V \to W$ 是一个线性变换，设 $\A := \{\aaa_1, \aaa_2, \dots, \aaa_n\}$, $\B := \{\bb_1, \bb_2, \dots, \bb_m\}$ 分别是 $V$ 和 $W$ 中的基。

变换 $T$ 在基 $\A$ 和 $\B$ 下（或相对于基 $\A$ 和 $\B$）的矩阵是 $m \times n$ 矩阵，记作 $[T]_{\B\A}$，它关联了坐标向量 $[T \vv]_\B$ 和 $[\vv]_\A$：
$$
[T \vv]_\B = [T]_{\B\A} [\vv]_\A
$$
注意这里 $\A$ 和 $\B$ 下标符号的对应：这是我们将第一组基 $\A$ 置于第二个位置的原因。

矩阵 $[T]_{\B\A}$ 很容易找到：它的第 $k$ 列就是坐标向量 $[T a_k]_\B$（与从 $\FF^n$ 到 $\FF^m$ 的线性变换矩阵的寻找方法进行比较！）。

正如在 $\FF^n$ 空间和标准基情况一样，线性变换的复合等价于它们的矩阵乘法：我们只需要在基方面更小心一些。也就是说，设 $T_1: X \to Y$ 和 $T_2: Y \to Z$ 是线性变换，设 $\A, \B, \C$ 分别是 $X, Y, Z$ 中的基。那么对于复合 $T = T_2 T_1,$
$$T: X \to Z, \quad T \xx := T_2(T_1(\xx)),$$
我们有
$$
(8.1)\quad \quad [T]_{\C\A} = [T_2 T_1]_{\C\A} = [T_2]_{\C\B} [T_1]_{\B\A} 
$$
（再次注意这里的下标对应）。

这个证明与 $\FF^n$ 空间在标准基下的证明完全相同，所以我们在此不再重复。另一种可能性是，通过坐标同构 $\vv \mapsto [\vv]_\B$ 将所有内容传递到 $\FF^n$ 空间。然后我们就不需要任何证明了，一切都遵循矩阵乘法的相关结果。

\subsection{8.3. 坐标变换矩阵}

设我们在向量空间 $V$ 中有两个基 $\A = \{\aaa_1, \aaa_2, \dots, \aaa_n\}$ 和 $\B = \{\bb_1, \bb_2, \dots, \bb_n\}$. ~考虑恒等变换 $I = I_V$ 以及它在这些基下的矩阵 $[I]_{\B\A}$. ~根据定义， 
$$[ \vv ]_\B = [I]_{\B\A} [\vv]_\A, \quad \forall \vv \in V,$$
也就是说，对于任何向量 $\vv \in V$，矩阵 $[I]_{\B\A}$ 将其在基 $\A$ 下的坐标变换为在基 $\B$ 下的坐标。矩阵 $[I]_{\B\A}$ 通常称为\textbf{坐标变换矩阵}(change of coordinates matrix)（从基 $\A$ 到基 $\B$）。

矩阵 $[I]_{\B\A}$ 很容易计算：根据线性变换矩阵的一般寻找规则，它的第 $k$ 列是基$\A$下第 $k$ 个基元素 的坐标表示 $[\aaa_k]_\B$. ~

注意 $$[I]_{\A\B} = ([I]_{\B\A})^{-1}$$（这从矩阵乘法规则 (8.1) 中立即得出），所以任何坐标变换矩阵总是可逆的。

\subsubsection{8.3.1. 从标准基进行坐标变换的例子}

设我们的空间 $V$ 是 $\FF^n$，并且我们有一组基 $\B = \{\bb_1, \bb_2, \dots, \bb_n\}$. ~我们还有标准基 $\SSS = \{\ee_1, \ee_2, \dots, \ee_n\}$. ~从 $\B$ 到 $\SSS$ 的坐标变换矩阵 $[I]_{\SSS\B}$ 很容易计算：
$$[I]_{\SSS\B} = [\bb_1, \bb_2, \dots, \bb_n] =: B,$$
即它只是矩阵 $B$，其第 $k$ 列是（列）向量$\vv_k$. ~反向也成立 $$[I]_{\B\SSS} = [I]_{\SSS\B}^{-1} = B^{-1}.$$

例如，考虑 $\FF^2$ 中的一组基
$$B = \{ \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \end{pmatrix} \},$$
设 $\SSS$ 表示那里的标准基。那么 
$$[I]_{\SSS\B} = \begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix} =: B,$$
并且
$$[I]_{\B\SSS} = [I]_{\SSS\B}^{-1} = B^{-1} = \frac{1}{3} \begin{pmatrix} -1 & 2 \\ 2 & -1 \end{pmatrix}$$
（我们知道如何计算逆，并且也很容易验证上述矩阵确实是 $B$ 的逆）。

\subsubsection{8.3.2. 通过标准基进行变换的例子}

在最多为 1 次的多项式空间中，我们还有基 $$\A = \{1, 1+x\},\quad \text{和}\quad \B = \{1+2x, 1-2x\},$$
并且我们想找到坐标变换矩阵 $[I]_{\B\A}$. ~

当然，我们总是可以从基 $\A$ 中取向量并尝试在基 $\B$ 中分解它们；这涉及到求解线性系统，而我们知道如何做到这一点。

然而，我认为以下方法更简单。
在 $\PP_1$ 中，我们还有标准基 $\SSS = \{1, x\}$, 对于这个基 
$$[I]_{\SSS\A} = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} =: A,\quad
[I]_{\SSS\B} = \begin{pmatrix} 1 & 1 \\ 2 & -2 \end{pmatrix} =: B,$$
并且取逆 
$$[I]_{\A\SSS} = A^{-1} = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix},\quad
[I]_{\B\SSS} = B^{-1} = \frac{1}{4} \begin{pmatrix} 2 & 1 \\ 2 & -1 \end{pmatrix}.$$

那么 
\footnote{
注意这里的下标对应。
}
$$[I]_{\B\A} = [I]_{\B\SSS} [I]_{\SSS\A} = B^{-1} A = \frac{1}{4} \begin{pmatrix} 2 & 1 \\ 2 & -1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix},$$
并且
$$[I]_{\A\B} = [I]_{\A\SSS} [I]_{\SSS\B} = A^{-1} B = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 2 & -2 \end{pmatrix}.$$

\subsection{8.4. 变换的矩阵与坐标变换}

设 $T: V \to W$ 是一个线性变换，设 $\A, \tilde{\A}$ 是 $V$ 中的两个基，设 $\B, \tilde{\B}$ 是 $W$ 中的两个基。假设我们知道矩阵 $[T]_{\B\A}$，并且我们想找到关于新基 $\tilde{\A}, \tilde{\B}$ 的矩阵表示，即矩阵 $[T]_{\tilde{\B}\tilde{\A}}$. ~规则非常简单：

\fbox{\begin{minipage}{0.9\textwidth}
要得到“新”基下的矩阵，需要用坐标变换矩阵将“旧”基下的矩阵包围起来。
\end{minipage}}
\\
我在这里没有提及应该将哪个坐标变换矩阵放在哪里，因为如果我们遵循下标对应规则，我们别无选择。也就是说，线性变换的矩阵表示遵循以下公式：

\fbox{
$
[T]_{\tilde{\B}\tilde{\A}} = [I]_{\tilde{\B}\B} [T]_{\B\A} [I]_{\A\tilde{\A}}
$
}
\\
（请注意这里的下标对应）。
证明可以通过分析每个矩阵的作用来完成。

\subsection{8.5. 单个基的情况：相似矩阵}

设 $V$ 是一个向量空间，设 $\A = \{\aaa_1, \aaa_2, \dots, \aaa_n\}$ 是 $V$ 中的一组基。考虑一个线性变换 $T: V \to V$，并设 $[T]_{\A\A}$ 是它在该基下的矩阵（我们对“输入”和“输出”使用相同的基）。
\footnote{
符号 $[T]_{\A}$ 常常被用来代替 $[T]_{\A\A}$. ~它虽然更简洁，但是双下标表示法能更好地适用于“下标对应规则”。
}

当我们对“输入”和“输出”使用相同基底时，这种情况非常重要（因为此时我们可以将一个矩阵与自身相乘），所以让我们更仔细地研究一下。请注意，在这种情况下，人们常常使用更简洁的记号 $[T]_{\A}$ 来代替 $[T]_{\A\A}$. ~然而，双下标表示法 $[T]_{\A\A}$ 更适用于“下标对应规则”，所以我建议在进行坐标变换时使用它（或至少时刻牢记这一点）。

设 $\B = \{\bb_1, \bb_2, \dots, \bb_n\}$ 是 $V$ 中的另一组基。根据上面的坐标变换规则 
$$[T]_{\B\B} = [I]_{\B\A} [T]_{\A\A} [I]_{\A\B}.$$
回忆
$$[I]_{\B\A} = [I]_{\A\B}^{-1}$$
并记 $Q := [I]_{\A\B}$，我们可以将上述公式改写为 
$$[T]_{\B\B} = Q^{-1} [T]_{\A\A} Q.$$
这为以下定义提供了动机：

\textbf{定义8.1.}~~ 我们说矩阵 $A$ 与矩阵 $B$ \textbf{相似}，如果存在一个可逆矩阵 $Q$ 使得 $A = Q^{-1} BQ$. ~

因为可逆矩阵必须是方阵，且通过计算维数后，我们可以看出，相似矩阵 $A$ 和 $B$ 必须是方阵且大小相同。如果 $A$ 与 $B$ 相似，即如果 $A = Q^{-1} BQ$，那么 $$B = Q A Q^{-1} = (Q^{-1})^{-1} A (Q^{-1})$$
（因为 $Q^{-1}$ 是可逆的），因此 $B$ 与 $A$ 相似。所以，我们可以简单地说 $A$ 和 $B$ 是\textbf{相似的}(similar)。

上述推理表明，将 $Q$ 和 $Q^{-1}$ 放在哪里并不重要：人们也可以使用公式 $A = QBQ^{-1}$ 来定义相似性。

上述讨论表明，我们可以将相似的矩阵视为同一线性算子（变换）的不同矩阵表示。

\begin{exer} \textbf{练习}~~

8.1. 判断正误：

a) 任何坐标变换矩阵都是方阵；

b) 任何坐标变换矩阵都是可逆的；

c) 如果矩阵 $A$ 和 $B$ 相似，那么 $B = Q^T A Q$ 对于某些矩阵 $Q$ 成立；

d) 如果矩阵 $A$ 和 $B$ 相似，那么 $B = Q^{-1} A Q$ 对于某些矩阵 $Q$ 成立；

e) 相似矩阵不一定是方阵。

8.2. 考虑向量系统 
$$(1, 2, 1, 1)^T,\quad (0, 1, 3, 1)^T,\quad (0, 3, 2, 0)^T,\quad (0, 1, 0, 0)^T.$$

a) 证明它们是 $\FF^4$ 中的一组基。尽量少做计算。

b) 找到将此基下的坐标变为 $\FF^4$ 中标准坐标（即标准基 $\ee_1, \dots, \ee_4$ 下的坐标）的坐标变换矩阵。

8.3. 找到将 $\PP_1$ 中的基 $1, 1+t$ 下的坐标变为基 $1-t, 2t$ 下的坐标的坐标变换矩阵。

8.4. 设 $T$ 是 $\FF^2$ 中的线性算子，定义为（在标准坐标下）
$$T\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 3x + y \\ x - 2y \end{pmatrix}.$$
找到 $T$ 在标准基下的矩阵，以及在基 $$\begin{pmatrix} 1 \\ 1 \end{pmatrix}\quad \text{和}\quad \begin{pmatrix} 1 \\ 2 \end{pmatrix}$$
下的矩阵。

8.5. 证明，如果 $A$ 和 $B$ 相似，那么 $\text{trace } A = \text{trace } B$. ~\textbf{提示}：回忆 $\text{trace}(XY)$ 和 $\text{trace}(YX)$ 是如何关联的。

8.6. 矩阵 
$$\begin{pmatrix} 1 & 3 \\ 2 & 2 \end{pmatrix}\text{和}\begin{pmatrix} 0 & 2 \\ 4 & 2 \end{pmatrix}$$
 是否相似？请给出理由。
\end{exer}

