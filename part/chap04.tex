

\chapter{第四章~~谱理论简介（特征值与特征向量）}

谱理论是帮助我们理解线性算子结构的主要工具。在本章中，我们只考虑从一个向量空间映到其自身的算子（或等价地说，$n \times n$ 矩阵）。如果我们有一个线性变换 $A: V \to V$，我们可以将其与自身相乘，取其任意幂，或任意多项式。

谱理论的主要思想是将算子分解为简单的块，并分别分析每个块。

为了解释其主要思想，让我们考虑\textbf{差分方程}(difference equations)。
许多过程可以用以下类型的方程描述：
$$\xx_{n+1} = A\xx_n,\quad n = 0, 1, 2, \dots,$$
其中 $A: V \to V$ 是一个线性变换，而 $\xx_n$ 是系统在时刻 $n$ 的状态。给定初始状态 $\xx_0$，我们希望知道时刻 $n$ 的状态 $\xx_n$，分析 $\xx_n$ 的长期行为等。\footnote{
差分方程是微分方程 $\xx'(t) = A\xx(t)$ 的离散时间类似物。为了求解微分方程，需要计算 $e^{tA} := \sum_{k=0}^\infty \frac{t^k A^k}{k!}$，而谱理论也有助于完成此操作。} 


乍一看，这个问题似乎很简单：解由公式 $\xx_n = A^n \xx_0$ 给出。但如果 $n$ 非常大呢：成千上万，数百万时，又会如何呢？或者如果我们想分析 $\xx_n$ 当 $n \to \infty$ 时的行为呢？

这时\textbf{特征值}和\textbf{特征向量}的概念就出现了。
假设 $A\xx_0 = \lambda \xx_0$，其中 $\lambda$ 是某个标量。那么 $A^2 \xx_0 = \lambda^2 \xx_0$, $A^3 \xx_0 = \lambda^3 \xx_0$, $\dots$, $A^n \xx_0 = \lambda^n \xx_0$，解的行为就能得到很好的解释。

在本章中，我们只考虑有限维空间中的算子。无穷维空间中的谱理论要复杂得多，这里提出的结果大多在无穷维情况下不成立。

\section{1. 基本定义}

\subsection{1.1.特征值、特征向量、谱}

标量 $\lambda$ 被称为算子 $A: V \to V$ 的\textbf{特征值}(eigenvalue)，如果存在一个\textbf{非零}向量 $\vv \in V$ 使得 $$A\vv = \lambda \vv.$$
向量 $\vv$ 被称为 $A$ 的\textbf{特征向量}(eigenvector)（对应于特征值 $\lambda$）。

如果我们知道了 $\lambda$ 是一个特征值，那么寻找特征向量将很容易：只需解方程 $A\xx = \lambda \xx$，或者等价地 $$(A - \lambda I)\xx = 0.$$
所以，找到对应于特征值 $\lambda$ 的\textbf{所有}特征向量，就是找到 $A - \lambda I$ 的零空间。零空间 $\text{Ker}(A - \lambda I)$，即所有特征向量和零向量的集合，被称为\textbf{特征子空间}(eigenvector)。

所有算子 $A$ 的特征值集合被称为 $A$ 的\textbf{谱}(spectrum)，通常记作 $\sigma(A)$.~

\subsection{1.2. 寻找特征值：特征多项式}

标量 $\lambda$ 是特征值当且仅当零空间 $\text{Ker}(A - \lambda I)$ 非平凡（因此方程 $(A - \lambda I)\xx = 0$ 有非平凡解）。

设 $A$ 作用于 $\FF^n$（即 $A: \FF^n \to \FF^n$）。由于 $A$ 的矩阵是方阵，$A - \lambda I$ 有非平凡零空间当且仅当它不可逆。我们知道一个方阵不可逆当且仅当它的行列式为 $0$.~因此

% \noindent
\fbox{
  \begin{minipage}{0.9\textwidth}
$\lambda \in \sigma(A)$，即 $\lambda$ 是 $A$ 的特征值 $\Leftrightarrow \det(A - \lambda I) = 0$.
\end{minipage}
}

如果 $A$ 是一个 $n \times n$ 矩阵，那么 $\det(A - \lambda I)$ 是关于变量 $\lambda$ 的 $n$ 次多项式。这个多项式被称为 $A$ 的\textbf{特征多项式}(characteristic polynomial)。所以，要找到 $A$ 的所有特征值，只需计算特征多项式并找到它所有的根。

用这种方法寻找算子的谱在更高维度下并不实用。求解高次多项式的根可能是一个非常困难的问题，并且对于次数大于 4 的方程，无法用求根公式求解。所以，在更高维度下，通常使用不同的数值方法来寻找特征值和特征向量。

\subsection{1.3. 寻找抽象算子的特征多项式和特征值}

因此，我们已经知道了如何找到矩阵的谱。但如何找到作用在抽象向量空间中的算子的特征值呢？方法很简单：

% \noindent
\fbox{
  \begin{minipage}{0.9\textwidth}
选取任意一组基，然后计算该基下算子的矩阵的特征值。
\end{minipage}
}

但我们如何知道这个结果不依赖于基的选取呢？

有几种可能的解释。一种是基于\textbf{相似矩阵}的概念。让我们回忆一下，方阵 $A$ 和 $B$ 被称为相似的，如果存在一个可逆矩阵 $S$ 使得 
$$A = SBS^{-1}.$$
注意，相似矩阵的行列式是相等的。的确 
$$\det A = \det(SBS^{-1}) = \det S \det B \det S^{-1} = \det B,$$
因为 $\det S^{-1} = 1/\det S$.~注意，如果 $A = SBS^{-1}$，那么 
$$A - \lambda I = SBS^{-1} - \lambda SIS^{-1} = S(B - \lambda I)S^{-1},$$
所以矩阵 $A - \lambda I$ 和 $B - \lambda I$ 是相似的。因此 
$$\det(A - \lambda I) = \det(B - \lambda I),$$
即\\
\fbox{\begin{minipage}{0.9\textwidth}
相似矩阵的特征多项式是相同的。
\end{minipage}}


如果 $T: V \to V$ 是一个线性变换，并且 $\A$ 和 $\B$ 是 $V$ 中的两个基，那么 
$$[T]_{\A\A} = [I]_{\A\B}[T]_{\B\B}[I]_{\B\A},$$
并且由于 $[I]_{\B\A} = ([I]_{\A\B})^{-1}$，所以矩阵$[T]_{\A\A}$和$[T]_{\B\B}$在不同基下是相似的。

换句话说，线性变换的矩阵在不同基下是相似的。

因此，我们可以将算子的特征多项式定义为它在某个基下的矩阵的特征多项式。正如我们上面讨论的，结果不依赖于基的选择，所以算子的特征多项式是良好定义的。

\subsection{1.4. 复空间与实空间}

代数基本定理断言，任何（至少一次）多项式都有一个复根。这意味着有限维\textbf{复}向量空间中的算子至少有一个特征值，因此它的谱是非空的。

另一方面，很容易在实向量空间中构造一个没有\textbf{实数}特征值的线性变换，例如在 $\RR^2$ 中旋转 $R_\alpha$, $\alpha \neq k\pi$ ($k \in \mathbb{Z}$) 就是一个例子。由于通常假设特征值应该属于标量域（如果一个算子作用在域 $\FF$ 上的向量空间中，则特征值应该在 $\FF$ 中），这样的算子具有空谱。

因此，复数情况（即作用在复向量空间中的算子）似乎是谱理论最自然的环境。由于 $\RR \subset \CC$，我们可以始终将实数 $n \times n$ 矩阵视为 $\CC^n$ 中的算子，以允许复数特征值。将实数矩阵视为 $\CC^n$ 中的算子在谱理论中是很典型的，我们也将遵循这个约定。寻找矩阵的特征值（除非另有说明）将始终意味着寻找所有\textbf{复数}特征值，而不是仅限于实数特征值。

注意，抽象实向量空间中的算子也可以解释为复空间中的算子。一种朴素的方法是固定一组基（本章所有空间都是有限维的），然后在该基下使用坐标，允许使用复数坐标：这本质上是从实数矩阵到具有复数特征值的算子的过程。

这种构造描述了所谓的\textbf{复化}(complexification)，结果不依赖于基的选择。后面第 5 章第 8.2 节将给出复化的“高明”抽象构造，解释为什么结果不依赖于基的选择。

\subsection{1.5. 特征值的重数}

提醒读者，如果 $p$ 是一个多项式，而 $\lambda$ 是它的一个根（即 $p(\lambda) = 0$），那么 $z - \lambda$ 整除 $p(z)$，即 $p$ 可以表示为 $p(z) = (z - \lambda)q(z)$，其中 $q$ 是某个多项式。如果 $q(\lambda) = 0$，那么 $q$ 也可以被 $z - \lambda$ 整除，所以 $(z - \lambda)^2$ 整除 $p$ 等等。

能够整除 $p(z)$ 的 $(z - \lambda)$ 的最大正整数 $k$ 被称为根 $\lambda$ 的\textbf{重数}(multiplicity)。

如果 $\lambda$ 是算子（矩阵）$A$ 的一个特征值，那么它就是特征多项式 $p(z) = \det(A - zI)$ 的一个根。这个根的重数被称为特征值 $\lambda$ 的\textbf{（代数）重数}。

次数为 $n$ 的任何多项式 $p(z) = \sum_{k=0}^n a_k z^k$ 恰好有 $n$ 个复数根，\textbf{计入重数}。计入重数的意思是，如果一个根的重数是 $d$，我们就必须列出（计数）它 $d$ 次。换句话说，$p$ 可以表示为 
$$p(z) = a_n (z - \lambda_1)(z - \lambda_2)\dots(z - \lambda_n),$$
其中 $\lambda_1, \lambda_2, \dots, \lambda_n$ 是它的复数根，计入重数。

还有另一种关于特征值重数的概念：特征子空间 $\text{Ker}(A - \lambda I)$ 的维数被称为特征值 $\lambda$ 的\textbf{几何重数}。

几何重数不像代数重数那样被广泛使用。所以，当人们简单地说“重数”时，他们通常指的是\textbf{代数重数}。

我们在此顺便提一下，特征值的代数重数和几何重数可能不同。

\textbf{命题 1.1}~~特征值的几何重数不能超过其代数重数。

\textbf{证明}~~见下面的练习 1.9。

\subsection{1.6. 迹与行列式}

\textbf{定理 1.2}~~设 $A$ 是一个 $n \times n$ 矩阵，设 $\lambda_1, \lambda_2, \dots, \lambda_n$ 是它的（复数）特征值（计入重数）。那么

1. $\text{trace } A = \lambda_1 + \lambda_2 + \dots + \lambda_n$.~

2. $\det A = \lambda_1 \lambda_2 \dots \lambda_n$.~

\textbf{证明}~~见下面的练习 1.10, 1.11。

\subsection{1.7. 三角矩阵的特征值}

计算特征值等价于寻找矩阵的特征多项式的根（或使用某种数值方法），这可能非常耗时。然而，有一个特殊情况，在这种情况下我们可以直接从矩阵中读出特征值。即，

\fbox{\begin{minipage}{0.9\textwidth}
{三角矩阵的特征值（计入重数）正是对角线上的元素 $a_{1,1}, a_{2,2}, \dots, a_{n,n}$.~}\end{minipage}}

这里三角矩阵是指上三角或下三角矩阵。由于对角矩阵是三角矩阵的一个特例（它既是上三角也是下三角），所以

\fbox{\begin{minipage}{0.9\textwidth}
对角矩阵的特征值是其对角线元素.
\end{minipage}}


其证明是微不足道的：我们需要从 $A$ 的对角元素中减去 $\lambda$，并利用三角矩阵的行列式是其对角线元素乘积这一事实。由此我们得到
特征多项式 
$$\det(A - \lambda I) = (a_{1,1} - \lambda)(a_{2,2} - \lambda)\dots(a_{n,n} - \lambda),$$
其根正是 $a_{1,1}, a_{2,2}, \dots, a_{n,n}$.~

\begin{exer} \textbf{练习}~

1.1. 判断正误：

a) 每个 $n$ 维向量空间中的线性算子都有 $n$ 个不同的特征值；

b) 如果一个矩阵只有一个特征向量，那么它有无限多个特征向量；

c) 存在一方实数方阵没有实数特征值；

d) 存在一个方阵，它没有（复数）特征向量；

e) 相似矩阵总是具有相同的特征值；

f) 相似矩阵总是具有相同的特征向量；

g) 矩阵 $A$ 的两个特征向量之和（非零）总是 $A$ 的特征向量；

h) 对应于同一特征值 $\lambda$ 的矩阵 $A$ 的两个特征向量之和总是算子 $A$ 的特征向量。

1.2. 找出以下矩阵的特征多项式、特征值和特征向量：

$$\begin{pmatrix} 4 & -5 \\ 2 & -3 \end{pmatrix},\quad \begin{pmatrix} 2 & 1 \\ -1 & 4 \end{pmatrix},\quad \begin{pmatrix} 1 & 3 & 3 \\ -3 & -5 & -3 \\ 3 & 3 & 1 \end{pmatrix}.$$

1.3. 计算旋转矩阵 
$$\begin{pmatrix} \cos \alpha & -\sin \alpha \\ \sin \alpha & \cos \alpha \end{pmatrix}$$
的特征值和特征向量。
注意，特征值（和特征向量）不一定必须是实数。

1.4. 计算以下矩阵的特征多项式和特征值：

$$\begin{pmatrix} 1 & 2 & 5 & 67 \\ 0 & 2 & 3 & 6 \\ 0 & 0 & -2 & 5 \\ 0 & 0 & 0 & 3 \end{pmatrix},\quad \begin{pmatrix} 2 & 1 & 0 & 2 \\ 0 & \pi & 43 & 2 \\ 0 & 0 & 16 & 1 \\ 0 & 0 & 0 & 54 \end{pmatrix},\quad \begin{pmatrix} 4 & 0 & 0 & 0 \\ 1 & 3 & 0 & 0 \\ 2 & 4 & e & 0 \\ 3 & 3 & 1 & 1 \end{pmatrix},\quad \begin{pmatrix} 4 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 2 & 4 & 0 & 0 \\ 3 & 3 & 1 & 1 \end{pmatrix}.$$

不要展开特征多项式，将其保留为乘积形式即可。

1.5. 证明三角矩阵的特征值（计入重数）与其对角线元素相等。

1.6. 称算子 $A$ 为\textbf{幂零}(nilpotent)的，如果 $A^k = \oo$ 对某个 $k$ 成立。证明如果 $A$ 是幂零的，那么 $\sigma(A) = \{0\}$（即 $0$ 是 $A$ 的唯一特征值）。


1.7. 证明分块三角矩阵 $$\begin{pmatrix} A & * \\ \oo & B \end{pmatrix}$$
的特征多项式（其中 $A$ 和 $B$ 是方阵）与$\det(A - \lambda I) \det(B - \lambda I)$ 相等。（使用第 3 章的练习 3.11）。

1.8. 设 $\vv_1, \vv_2, \dots, \vv_n$ 是向量空间 $V$ 中的一组基。还假设基的前 $k$ 个向量 $\vv_1, \vv_2, \dots, \vv_k$ 是算子 $A$ 的特征向量，对应于特征值 $\lambda$ （即 $A\vv_j = \lambda \vv_j, j = 1, 2, \dots, k$）。证明在该基下，算子 $A$ 的矩阵具有分块三角形式 
$$\begin{pmatrix} \lambda I_k & * \\ \oo & B \end{pmatrix},$$
其中 $I_k$ 是 $k \times k$ 的单位矩阵， $B$ 是某个 $(n-k) \times (n-k)$ 矩阵。

1.9. 使用前面两个练习来证明一个特征值的几何重数不能超过其代数重数。

1.10. 证明矩阵 $A$ 的行列式是其特征值的乘积（计重数）。

\textbf{提示}： 首先证明 $\det(A - \lambda I) = (\lambda_1 - \lambda)(\lambda_2 - \lambda)\dots(\lambda_n - \lambda)$，其中 $\lambda_1, \lambda_2, \dots, \lambda_n$ 是特征值（计重数）。然后比较常数项（不含 $\lambda$ 的项）或代入 $\lambda = 0$ 来得出结论。

1.11. 分三步证明矩阵的迹等于特征值之和。\\
首先，计算等式 $$\det(A - \lambda I) = (\lambda_1 - \lambda)(\lambda_2 - \lambda)\dots(\lambda_n - \lambda)$$
右侧 $\lambda^{n-1}$ 的系数。然后证明 $\det(A - \lambda I)$ 可以表示为 
$$\det(A - \lambda I) = (a_{1,1} - \lambda)(a_{2,2} - \lambda)\dots(a_{n,n} - \lambda) + q(\lambda),$$
其中 $q(\lambda)$ 是一个最多为 $n-2$ 次的多项式。最后，通过比较 $\lambda^{n-1}$ 的系数来得出结论。\end{exer}


\section{2. 对角化}

谱理论的一个应用是对算子的\textbf{对角化}(diagonalization)，即给定一个算子，找到一组基，使得该算子在该基下的矩阵是对角矩阵。这样的基并非总能找到，也就是说，并非所有算子都能对角化（都是可对角化的）。算子可对角化的重要性在于，对角矩阵的幂以及更一般的函数很容易计算。因此如果我们对一个算子进行对角化，我们就可以轻松地计算它的函数。

我们将在本节中解释如何计算可对角化算子的函数。我们还将给出一个算子可对角化的充要条件，以及一些简单的充分条件。

此外，对于 $\FF^n$ 中的算子（矩阵），$A$ 的可对角化性质意味着它可以表示为 $A = SDS^{-1}$，其中 $D$ 是一个对角矩阵，$S$ 是一个可逆矩阵；我们将在稍后解释这一点。

除非另有说明，本节中的所有结果对于复数和实数向量空间（甚至对于任意域 $\FF$ 上的向量空间）都成立。

\subsection{2.1. 预备知识}

假设一个向量空间 $V$ 中的算子 $A$ 具有一个由 $A$ 的特征向量组成的基 $B = \{\bb_1, \bb_2, \dots, \bb_n\}$，其中 $\lambda_1, \lambda_2, \dots, \lambda_n$ 是相应的特征值。那么 $A$ 在此基下的矩阵是对角矩阵，对角线上是 $\lambda_1, \lambda_2, \dots, \lambda_n$，其余位置都是$0$，省略不写：
$$(2.1) \quad [A]_{\B\B} = \text{diag}\{\lambda_1, \lambda_2, \dots, \lambda_n\} = \begin{pmatrix} \lambda_1 & & & \\ & \lambda_2 & & \\ & & \ddots & \\ & & & \lambda_n \end{pmatrix}$$

另一方面，如果一个算子 $A$ 在基 $B = \{\bb_1, \bb_2, \dots, \bb_n\}$ 下的矩阵由 (2.1) 给出，那么显然 $A\bb_k = \lambda_k \bb_k$，即 $\lambda_k$ 是特征值，$\bb_k$ 是相应的特征向量。

再次注意，上述推理对于复数和实数向量空间（甚至对于任意域 $\FF$ 上的向量空间）都成立。

将上述推理应用于 $\FF^n$ 中的算子（矩阵），我们立即得到以下定理。注意，虽然本书中 $\FF$ 是 $\CC$ 或 $\RR$，但本定理对任意域 $\FF$ 都成立。

\textbf{定理 2.1}~~一个矩阵 $A$（每项的值都在 $\FF$内）允许表示为 $A = SDS^{-1}$，其中 $D$ 是一个对角矩阵，$S$ 是一个可逆矩阵（两者项中的值都在 $\FF$内），当且仅当存在 $\FF^n$ 的一个由 $A$ 的特征向量组成的基。

而且，在这种情况下，$D$ 的对角线上的项都是特征值，而 $S$ 的列是相应的特征向量（第 $k$ 列对应于 $D$ 的第 $k$ 个对角线元素）。

\textbf{证明}~~设 $D = \text{diag}\{\lambda_1, \lambda_2, \dots, \lambda_n\}$，并设 $\bb_1, \bb_2, \dots, \bb_n$ 是 $S$ 的列（注意，由于 $S$ 可逆，它的列构成了 $\FF^n$ 的一组基）。

那么恒等式 $A = SDS^{-1}$ 意味着 $D = S^{-1}AS = [I]_{\SSS,\B}A[I]_{\B,\SSS}$，其中 $S = [I]_{\SSS,\B}$ 是从 $B$ 到标准基 $S$ 的坐标变换矩阵。这正好意味着 $D = [A]_{\B,\B}$.~

正如我们上面讨论的，当且仅当 $\lambda_k$ 是 $A$ 的特征值，$\bb_k$ 是 $A$ 的相应特征向量时，$[A]_{\B,\B} = D = \text{diag}\{\lambda_1, \lambda_2, \dots, \lambda_n\}$.~

\textbf{注记} ~注意，如果一个矩阵允许表示为 $A = SDS^{-1}$ 并且 $D$ 是一个对角矩阵，那么通过简单的直接计算可以表明，$S$ 的列是 $A$ 的特征向量，而 $D$ 的对角线元素是相应的特征值。这为定理 2.1 中相应陈述提供了另一种证明。

正如我们上面讨论的，一个可对角化的算子 $A: V \to V$ 恰好有 $n = \dim V$ 个特征值（计入重数）；一个复向量空间中的算子恰好有 $n$ 个特征值（计入重数）；另一方面，一个实数空间中的算子可能没有实数特征值。

我们将遵循谱理论中的惯例，将实数矩阵视为 $\CC^n$ 中的算子，从而允许复数特征值和特征向量。除非另有说明，我们将把对矩阵的复数对角化简单说成对角化，即 $A = SDS^{-1}$中，矩阵 $S$ 和 $D$ 的项可以是复数。

一个实数矩阵何时允许实数对角化（$A = SDS^{-1}$，其中 $S$ 和 $D$ 都是实数矩阵）的问题，实际上是一个非常简单的问题，见下面的定理 2.9。

\subsection{2.2. 一些动机：算子函数}

设一个算子 $A$ 在基 $\B = \{\bb_1, \bb_2, \dots, \bb_n\}$ 下的矩阵是 (2.1) 中给出的对角矩阵。那么很容易找到算子 $A$ 的 $N$ 次幂。即，在基 $\B$ 下 $A^N$ 的矩阵是 
$$[A^N]_{\B\B} = \text{diag}\{\lambda_1^N, \lambda_2^N, \dots, \lambda_n^N\} = \begin{pmatrix} \lambda_1^N & & & \\ & \lambda_2^N & & \\ & & \ddots & \\ & & & \lambda_n^N \end{pmatrix}.$$
而且，算子的函数也相对容易计算：例如，算子（矩阵）指数 $e^{tA}$ 定义为 $e^{tA} = I + tA + \frac{t^2 A^2}{2!} + \frac{t^3 A^3}{3!} + \dots = \sum_{k=0}^\infty \frac{t^k A^k}{k!}$，并且它在基 $B$ 下的矩阵是 $$[e^{tA}]_{\B\B} = \text{diag}\{e^{\lambda_1 t}, e^{\lambda_2 t}, \dots, e^{\lambda_n t}\} = \begin{pmatrix} e^{\lambda_1 t} & & & \\ & e^{\lambda_2 t} & & \\ & & \ddots & \\ & & & e^{\lambda_n t} \end{pmatrix}.$$

设 $A$ 是 $\FF^n$ 中的一个算子。为了在标准基 $\SSS$ 下找到算子 $A^N$ 和 $e^{tA}$ 的矩阵，我们需要回忆一下坐标变换矩阵 $[I]_{\SSS\B}$ 是一个以 $\bb_1, \bb_2, \dots, \bb_n$ 为列的矩阵。设这个矩阵为 $S$，那么根据坐标变换公式我们得到
$$A = [A]_{\SSS\SSS} = S \begin{pmatrix} \lambda_1 & & & \\ & \lambda_2 & & \\ & & \ddots & \\ & & & \lambda_n \end{pmatrix} S^{-1} = SDS^{-1},$$
其中我们使用 $D$ 来表示中间的对角矩阵。

类似地，$$A^N = SD^N S^{-1} = S \begin{pmatrix} \lambda_1^N & & & \\ & \lambda_2^N & & \\ & & \ddots & \\ & & & \lambda_n^N \end{pmatrix} S^{-1},$$
并且对于 $e^{tA}$ 类似。

另一种思考可对角化算子的幂（或其他函数）的方式是，注意到，如果算子 $A$ 可以表示为 $A = SDS^{-1}$，那么
$$A^N = \underset{N \text{ times} }{\underbrace{(SDS^{-1})(SDS^{-1})\dots(SDS^{-1})} }= SD^N S^{-1}$$
计算对角矩阵的 $N$ 次幂就很容易了。

\subsection{2.3. $n$ 个不同特征值的情况}

我们现在给出一个算子可对角化的非常简单的\textbf{充分}条件，见下面的推论 2.3。

\textbf{定理 2.2}~~设 $\lambda_1, \lambda_2, \dots, \lambda_r$ 是 $A$ 的不同特征值，设 $\vv_1, \vv_2, \dots, \vv_r$ 是相应的特征向量。那么向量 $\vv_1, \vv_2, \dots, \vv_r$ 是线性无关的。

\textbf{证明}~~我们将使用数学归纳法处理 $r$.~$r=1$ 的情况是平凡的，因为根据定义，特征向量是非零的，并且由一个非零向量组成的系统是线性无关的。

假设定理的陈述对 $r-1$ 是正确的。假设存在一个非平凡线性组合
$$(2.2)\quad c_1 \vv_1 + c_2 \vv_2 + \dots + c_r \vv_r = \sum_{k=1}^r c_k \vv_k = \oo.$$

将 $(A - \lambda_r I)$ 作用于 (2.2) 并利用 $(A - \lambda_r I)\vv_r = \oo$ 的事实，我们得到
$$\sum_{k=1}^{r-1} c_k (\lambda_k - \lambda_r) \vv_k = \oo.$$
根据归纳假设，向量 $\vv_1, \vv_2, \dots, \vv_{r-1}$ 是线性无关的，所以 $c_k(\lambda_k - \lambda_r) = 0$ 对于 $k=1, 2, \dots, r-1$ 成立。由于 $\lambda_k \neq \lambda_r$，我们可以得出 $c_k = 0$ 对于 $k < r$成立。然后从 (2.2) 可知 $c_r = 0$，也就是说我们得到了平凡线性组合。

\textbf{推论 2.3}~~如果一个算子 $A: V \to V$ 恰好有 $n = \dim V$ 个不同的特征值，那么它是可对角化的。

\textbf{证明}~~对于每个特征值 $\lambda_k$，设 $\vv_k$ 是一个相应的特征向量（对每个特征值只选取一个特征向量）。根据定理 2.2，系统 $\{\vv_1, \vv_2, \dots, \vv_n\}$ 是线性无关的，并且由于它恰好包含 $n = \dim V$ 个向量，所以它是一组基。

\subsection{2.4. 子空间的基（又名子空间的直和）}

为了描述可对角化算子，我们需要引入一些新定义。

设 $V_1, V_2, \dots, V_p$ 是向量空间 $V$ 的子空间。我们说子空间的系统是 $V$ 的一组基，如果任何向量 $\vv \in V$ 都存在唯一的表示为和
$$(2.3)\quad \vv = \vv_1 + \vv_2 + \dots + \vv_p = \sum_{k=1}^p \vv_k,\quad \vv_k \in V_k.$$
我们也说，子空间的系统 $\{V_1, V_2, \dots, V_p\}$ 是线性无关的，如果方程 
$$\vv_1 + \vv_2 + \dots + \vv_p = \oo,\quad \vv_k \in V_k$$
只有平凡解 ($\vv_k = \oo, \forall k = 1, 2, \dots, p$)。

另一种表述方式是，子空间的系统 $\{V_1, V_2, \dots, V_p\}$ 是线性无关的，当且仅当任何由非零向量 $\vv_k$ ($\vv_k \in V_k$) 组成的系统是线性无关的。

我们说子空间的系统 $\{V_1, V_2, \dots, V_p\}$ 是生成（或完备，或张成）的，如果任何向量 $\vv \in V$ 可以表示为 (2.3)（不一定是唯一的）。

\textbf{注记2.4}~~从上述定义可以立即看出，定理 2.2 实际上表明，算子 $A$ 的特征子空间 
$$E_k := \text{Ker}(A - \lambda_k I),\quad \lambda_k \in \sigma(A)$$ 的系统是线性无关的。

\textbf{注记2.5}~~很容易看出，与向量基类似，子空间的系统 $\{V_1, V_2, \dots, V_p\}$ 是一组基当且仅当它既是生成集又是线性无关的。我们将此事实的证明留给读者作为练习。

有一个子空间基的简单例子。设 $V$ 是一个向量空间，有一组基 $\{\vv_1, \vv_2, \dots, \vv_n\}$.~将下标集 $\{1, 2, \dots, n\}$ 分为 $p$ 个子集 $\Lambda_1, \Lambda_2, \dots, \Lambda_p$，并定义子空间 $V_k := \text{span}\{\vv_j : j \in \Lambda_k\}$.~显然，子空间 $V_k$ 构成 $V$ 的一组基。

下面的定理表明，在有限维情况下，这是子空间基唯一可能的例子。

\textbf{定理 2.6}~~设 $\{V_1, V_2, \dots, V_p\}$ 是一些子空间的基，并且在每个子空间 $V_k$ 中都有一组基（向量基）$\B_k$.~
\footnote{
我们不具体列出 $\B_k$ 中的向量，只需记住每个 $\B_k$ 都包含有限数量的向量。
}
那么这些基的并集 $\cup_k \B_k$ 是 $V$ 的一组基。

为了证明定理，我们需要以下引理：

\textbf{引理 2.7}~~设 $\{V_1, V_2, \dots, V_p\}$ 是一个线性无关的子空间族，并且在每个子空间 $V_k$ 中都有一个线性无关的向量系统 $\B_k$.~
\footnote{
同样，这里我们不单独命名 $\B_k$ 中的每个向量，我们只是记住每个集合 $\B_k$ 都包含有限数量的向量。
}
那么这些基的并集 $\B := \cup_k \B_k$ 是一个线性无关的系统。

\textbf{证明}~~如果稍微思考一下，引理的证明几乎是平凡的。书写证明的主要困难在于选择合适的记号。为了避免使用两个下标（一个表示 $k$，另一个表示 $\B_k$ 中向量的编号），让我们使用“扁平化”的记号。

即，设 $n$ 是 $\B := \cup_k \B_k$ 中向量的数量。让我们对 $B$ 中的向量集进行排序，例如如下：首先列出 $\B_1$ 中的所有向量，然后是 $\B_2$ 中的所有向量，依此类推，最后列出 $\B_p$ 中的所有向量。

这样，我们将 $\B$ 中的所有向量用整数 $1, 2, \dots, n$ 下标，并且下标集 $\{1, 2, \dots, n\}$ 被分成集合 $\Lambda_1, \Lambda_2, \dots, \Lambda_p$，使得集合 $\B_k$ 由向量 $\{\bb_j : j \in \Lambda_k\}$ 组成。

假设我们有一个非平凡的线性组合
$$(2.4)\quad c_1 \bb_1 + c_2 \bb_2 + \dots + c_n \bb_n = \sum_{j=1}^n c_j \bb_j = \oo.$$
设 $$\vv_k := \sum_{j \in \Lambda_k} c_j \bb_j.$$
那么 (2.4) 可以重写为
$$\vv_1 + \vv_2 + \dots + \vv_p = \oo.$$

由于 $\vv_k \in V_k$ 并且子空间 $\{V_1, V_2, \dots, V_p\}$ 是线性无关的，所以 $\vv_k = \oo$ $\forall k$.~这意味着对于每个 $k$，
$$\sum_{j \in \Lambda_k} c_j \bb_j = \oo,$$
并且由于向量系统 $\{\bb_j : j \in \Lambda_k\}$（即系统 $B_k$）是线性无关的，我们得到 $c_j = 0$ 对于所有 $j \in \Lambda_k$.~由于这对所有 $\Lambda_k$ 都成立，我们可以得出 $c_j = 0$ 对于所有 $j$.~

\textbf{定理 2.6 的证明}~~为了证明定理，我们将使用与引理 2.7 证明相同的记号，即系统 $B_k$ 由向量 $\{\bb_j : j \in \Lambda_k\}$ 组成。

引理 2.7 断言向量系统 $\{\bb_j : j = 1, 2, \dots, n\}$ 是线性无关的，所以剩下的就是证明这个系统是完备的。

由于子空间系统 $\{V_1, V_2, \dots, V_p\}$ 是一组基，任何向量 $\vv \in V$ 可以表示为
$$\vv = \vv_1 + \vv_2 + \dots + \vv_p = \sum_{k=1}^p \vv_k,\quad \vv_k \in V_k.$$
由于向量 $\{\bb_j : j \in \Lambda_k\}$ 构成了 $V_k$ 的基，向量 $\vv_k$ 可以表示为 
$$\vv_k = \sum_{j \in \Lambda_k} c_j \bb_j.$$
因此，$\vv = \sum_{j=1}^n c_j \bb_j$.

\subsection{2.5. 可对角化判据}

首先，让我们回顾一个必要的条件。由于对角矩阵 $D = \text{diag}\{\lambda_1, \lambda_2, \dots, \lambda_n\}$ 的特征值（计入重数）恰好是 $\lambda_1, \lambda_2, \dots, \lambda_n$，我们发现如果一个算子 $A: V \to V$ 是可对角化的，它恰好有 $n = \dim V$ 个特征值（计入重数）。

下面的定理对实数和复向量空间都成立（甚至对任意域上的空间也成立）。

\textbf{定理 2.8}~~设一个算子 $A: V \to V$ 恰好有 $n = \dim V$ 个特征值（计入重数）。
\footnote{
由于任何复向量空间中的算子都恰好有 $n$ 个特征值（计入重数），因此在复数情况下，此假设是多余的。
}
那么 $A$ 是可对角化的当且仅当对于每个特征值 $\lambda$，特征子空间 $\text{Ker}(A - \lambda I)$ 的维数（即几何重数）等于 $\lambda$ 的代数重数。

\textbf{证明}~~首先，我们注意到，对于一个对角矩阵，特征值的代数重数和几何重数是相等的，因此对于可对角化算子也是如此。

现在我们来证明另一个蕴含关系。设 $\lambda_1, \lambda_2, \dots, \lambda_p$ 是 $A$ 的特征值，设 $E_k := \text{Ker}(A - \lambda_k I)$ 是相应的特征子空间。根据注记2.4，子空间 $E_k,k=1,2,\dots,p$ 是线性无关的。

设 $\B_k$ 是 $E_k$ 的一组基。根据引理 2.7，向量系统 $\B := \cup_k \B_k$ 是一个线性无关系统。

我们知道每个 $\B_k$ 由 $\dim E_k$（即 $\lambda_k$ 的重数）个向量组成。所以 $\B$ 中的向量数量等于特征值 $\lambda_k$ 的重数之和。但是特征值重数之和就是计入重数的特征值数量，这恰好是 $n = \dim V$.~因此，我们得到了一个 $n = \dim V$ 个线性无关的特征向量组成的系统，这意味着它是一组基。

\subsection{2.6. 实数分解}

下面的定理实际上已经证明过了（它本质上是定理 2.8 在实数空间上的情况）。我们在此陈述是为了总结实数矩阵实数对角化的情形。

\textbf{定理 2.9}~~一个实数 $n \times n$ 矩阵 $A$ 允许实数分解（即表示为 $A = SDS^{-1}$，其中 $S$ 和 $D$ 是实数矩阵，$D$ 是对角矩阵且 $S$ 可逆）当且仅当它允许复数分解并且 $A$ 的所有特征值都是实数。

\subsection{2.7. 一些例子}

\subsubsection{2.7.1. 实数特征值}

考虑矩阵 
$$A = \begin{pmatrix} 1 & 2 \\ 8 & 1 \end{pmatrix}.$$
它的特征多项式等于 
$$\begin{vmatrix} 1-\lambda & 2 \\ 8 & 1-\lambda \end{vmatrix} = (1-\lambda)^2 - 16,$$
其根（特征值）是 $\lambda = 5$ 和 $\lambda = -3$.~对于特征值 $\lambda = 5$， 
$$A - 5I = \begin{pmatrix} 1-5 & 2 \\ 8 & 1-5 \end{pmatrix} = \begin{pmatrix} -4 & 2 \\ 8 & -4 \end{pmatrix}.$$
其零空间的基由一个向量 $(1, 2)^T$ 构成，所以这是对应的特征向量。

类似地，对于 $\lambda = -3$， 
$$A - \lambda I = A + 3I = \begin{pmatrix} 1+3 & 2 \\ 8 & 1+3 \end{pmatrix} = \begin{pmatrix} 4 & 2 \\ 8 & 4 \end{pmatrix}.$$
Ker$(A + 3I)$ 的零空间由向量 $(1, -2)^T$ 张成，所以这是对应的特征向量。矩阵 $A$ 可以被对角化为
$$A =\begin{pmatrix} 1 & 2 \\ 8 & 1 \end{pmatrix}= \begin{pmatrix} 1 & 1 \\ 2 & -2 \end{pmatrix} \begin{pmatrix} 5 & 0 \\ 0 & -3 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 2 & -2 \end{pmatrix}^{-1}.$$

\subsubsection{2.7.2. 复数特征值}

考虑矩阵 
$$A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}.$$
其特征多项式是 
$$\begin{vmatrix} 1-\lambda & 2 \\ -2 & 1-\lambda \end{vmatrix} = (1-\lambda)^2 + 4,$$
特征值（特征多项式的根）是 $\lambda = 1 \pm \ii$.~对于 $\lambda = 1 + \ii$， 
$$A - \lambda I = \begin{pmatrix} 1-(1+\ii) & 2 \\ -2 & 1-(1+\ii) \end{pmatrix} = \begin{pmatrix} -\ii & 2 \\ -2 & -\ii \end{pmatrix}.$$
这个矩阵的秩是 1，所以特征子空间 $\text{Ker}(A - \lambda I)$ 由一个向量，例如 $(1, \ii)^T$ 张成。

由于矩阵 $A$ 是实数的，我们不需要计算 $\lambda = 1 - \ii$ 的特征向量：通过取上述特征向量的复共轭，我们可以自动获得它，见下面的练习 2.2。所以，对于 $\lambda = 1 - 2\ii$，一个相应的特征向量是 $(1, -\ii)^T$，因此矩阵 $A$ 可以被对角化为
$$A = \begin{pmatrix} 1 & 1 \\ \ii & -\ii \end{pmatrix} \begin{pmatrix} 1+2\ii & 0 \\ 0 & 1-2\ii \end{pmatrix} \begin{pmatrix} 1 & 1 \\ \ii & -\ii \end{pmatrix}^{-1}.$$

\subsubsection{2.7.3. 一个不可对角化的矩阵}

考虑矩阵 
$$A = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}.$$
其特征多项式是 
$$\begin{vmatrix} 1-\lambda & 1 \\ 0 & 1-\lambda \end{vmatrix} = (1-\lambda)^2,$$
所以 $A$ 有一个重数为 2 的特征值 1。然而，很容易看出 $\dim \text{Ker}(A - I) = 1$（1个主元，所以 $2-1=1$ 个自由变量）。因此，特征值 1 的几何重数与其代数重数不同，所以 $A$ 是不可对角化的。

还有一个不使用定理 2.8 的解释。即，我们得到特征子空间 $\text{Ker}(A - I)$ 是一维的（由向量 $(1, 0)^T$ 张成）。如果 $A$ 是可对角化的，那么它将在某个基下具有对角形式 $\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$，
\footnote{
注意，唯一具有某种基下矩阵为 $\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$ 的线性变换是恒等变换 $I$.~由于 $A$ 肯定不是恒等变换，我们可以立即得出 $A$ 不能被对角化，所以计算特征子空间的维数是不必要的。
}
因此特征子空间的维数将是 2。所以 $A$ 不能被对角化。

\begin{exer} \textbf{练习}

2.1. 设 $A$ 是 $n \times n$ 矩阵。判断正误：

a) $A^T$ 与 $A$ 具有相同的特征值。

b) $A^T$ 与 $A$ 具有相同的特征向量。

c) 如果 $A$ 是可对角化的，那么 $A^T$ 也是可对角化的。

证明你的结论。

2.2. 设 $A$ 是一个实数方阵，$\lambda$ 是它的一个复数特征值。假设 $\vv = (v_1, v_2, \dots, v_n)^T$ 是一个相应的特征向量，$A\vv = \lambda \vv$.~证明 $\bar{\lambda}$ 是 $A$ 的一个特征值，并且 $\bar{\vv}$ 是 $A$ 的相应特征向量。这里 $\bar{\vv}$ 是向量 $\vv$ 的复共轭，$\bar{\vv} := (\bar{v}_1, \bar{v}_2, \dots, \bar{v}_n)^T$.~

2.3. 设 
$$A = \begin{pmatrix} 4 & 3 \\ 1 & 2 \end{pmatrix}.$$
通过对 $A$ 进行对角化，求出 $A^{2004}$.~

2.4. 构建一个特征值为 1 和 3，相应特征向量为 $(1, 2)^T$ 和 $(1, 1)^T$ 的矩阵 $A$.~这样的矩阵是唯一的吗？

2.5. 对以下矩阵进行对角化，如果可能：

a) $\begin{pmatrix} 4 & -2 \\ 1 & 1 \end{pmatrix}.$

b) $\begin{pmatrix} -1 & -1 \\ 6 & 4 \end{pmatrix}.$

c) $\begin{pmatrix} -2 & 2 & 6 \\ 5 & 1 & -6 \\ -5 & 2 & 9 \end{pmatrix}$ ($\lambda = 2$ 是其中一个特征值)

2.6. 考虑矩阵 
$$A = \begin{pmatrix} 2 & 6 & -6 \\ 0 & 5 & -2 \\ 0 & 0 & 4 \end{pmatrix}.$$

a) 求它的特征值。在不计算的情况下能否求出特征值？

b) 这个矩阵可对角化吗？在不进行计算的情况下找出答案。

c) 如果矩阵可对角化，请对其进行对角化。



2.7. 对矩阵 $$\begin{pmatrix} 2 & 0 & 6 \\ 0 & 2 & 4 \\ 0 & 0 & 4 \end{pmatrix}$$
进行对角化。

2.8. 求矩阵 $$A = \begin{pmatrix} 5 & 2 \\ -3 & 0 \end{pmatrix}$$
的所有平方根，即求所有满足 $B^2 = A$ 的矩阵 $B$.~
\textbf{提示：} 求对角矩阵的平方根很容易。你可以将答案留作乘积形式。

2.9. 回顾一下著名的斐波那契数列：0, 1, 1, 2, 3, 5, 8, 13, 21, ...，它由以下方式定义：令 $\phi_0 = 0$, $\phi_1 = 1$，并定义 $$\phi_{n+2} = \phi_{n+1} + \phi_n.$$
我们想找到 $\phi_n$ 的一个公式。

a) 找到一个 $2 \times 2$ 矩阵 $A$，使得 $$\begin{pmatrix} \phi_{n+2} \\ \phi_{n+1} \end{pmatrix} = A \begin{pmatrix} \phi_{n+1} \\ \phi_n \end{pmatrix}.$$
\textbf{提示：} 结合平凡方程 $\phi_{n+1} = \phi_{n+1}$ 和斐波那契关系 $\phi_{n+2} = \phi_{n+1} + \phi_n$.~

b) 对 $A$ 进行对角化，并找到 $A^n$ 的一个公式。

c) 注意到 $$\begin{pmatrix} \phi_{n+1} \\ \phi_n \end{pmatrix} = A^n \begin{pmatrix} \phi_1 \\ \phi_0 \end{pmatrix} = A^n \begin{pmatrix} 1 \\ 0 \end{pmatrix},$$
找到 $\phi_n$ 的一个公式。（你需要计算一个逆矩阵并进行乘法运算。）

d) 证明向量 $(\phi_{n+1}/\phi_n, 1)^T$ 收敛到一个 $A$ 的特征向量。
\quad 你认为这是一个巧合吗？

2.10. 设 $A$ 是一个 $5 \times 5$ 矩阵，有 3 个特征值（不计重数）。假设我们知道其中一个特征子空间是三维的。
你能说 $A$ 是否可对角化吗？

2.11. 给出一个 $3 \times 3$ 矩阵的例子，它不能被对角化。在构造了矩阵之后，你能使它“通用”一些，使得矩阵的特殊结构不明显吗？

2.12. 设一个非零矩阵 $A$ 满足 $A^5 = 0$.~证明 $A$ 不能被对角化。更一般地说，任何非零幂零矩阵，即满足 $A^N = 0$ 对某个 $N$ 的矩阵，都不能被对角化。

2.13. 转置的特征值：

a) 考虑 $2 \times 2$ 矩阵空间 $M_{2 \times 2}$ 上的变换 $T(A) = A^T$.~找出它所有的特征值和特征向量。这个变换可能被对角化吗？

\textbf{提示：} 虽然可以写出这个线性变换在某个基下的矩阵，计算特征多项式等等，但直接从定义中找出特征值和特征向量会更容易。

b) 在 $n \times n$ 矩阵空间中，能否做同样的问题？

2.14. 证明两个子空间 $V_1$ 和 $V_2$ 是线性无关的当且仅当 $V_1 \cap V_2 = \{\oo\}$.~\end{exer}

